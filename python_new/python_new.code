def test group by nested expression with params self books qs Book objects annotate greatest pages Greatest "pages" Value 600 values "greatest pages" annotate min pages Min "pages" least Least "min pages" "greatest pages" values list "least" flat True self assertCountEqual books qs [300 946 1132]
def create many to many intermediary model field klass from django db import models def set managed model related through through meta managed model meta managed or related meta managed to model resolve relation klass field remote field model name "%s %s" % klass meta object name field name lazy related operation set managed klass to model name to make model tuple to model [1] to app make model tuple to model [0] from klass meta model name has conflicting m2m field False if to from and to app klass meta app label to "to %s" % to from "from %s" % from if to from and to app ! klass meta app label to "%s %s" % to app to has conflicting m2m field True meta type "Meta" { "db table" field get m2m db table klass meta "auto created" klass "app label" klass meta app label "db tablespace" klass meta db tablespace "unique together" from to "verbose name" "% from s-% to s relationship" % {"from" from "to" to} "verbose name plural" "% from s-% to s relationships" % {"from" from "to" to} "apps" field model meta apps "has conflicting m2m field" has conflicting m2m field } # Construct and return the new class return type name models Model { "Meta" meta " module " klass module from models ForeignKey klass related name "%s+" % name db tablespace field db tablespace db constraint field remote field db constraint on delete CASCADE to models ForeignKey to model related name "%s+" % name db tablespace field db tablespace db constraint field remote field db constraint on delete CASCADE }
def annotation select self """ Return the dictionary of aggregate columns that are not masked and should be used in the SELECT clause Cache this result for performance """ if self annotation select cache is not None return self annotation select cache elif not self annotations return {} elif self annotation select mask is not None self annotation select cache { k self annotations[k] for k in self annotation select mask if k in self annotations } return self annotation select cache else return self annotations
def bind template self template if self template is not None raise RuntimeError "Context is already bound to a template" self template template # Set context processors according to the template engine s settings processors template engine template context processors + self processors updates {} for processor in processors context processor self request try updates update context except TypeError as e raise TypeError f"Context processor {processor qualname } didn t return a " "dictionary " from e self dicts[self processors index] updates try yield finally self template None # Unset context processors self dicts[self processors index] {}
def language prefix self language code get language or settings LANGUAGE CODE default language get supported language variant settings LANGUAGE CODE if language code default language and not self prefix default language return "" else return "%s " % language code
def language prefix self language code get language or settings LANGUAGE CODE default language get supported language variant settings LANGUAGE CODE if language code default language and not self prefix default language return "" else return "%s " % language code
def as oracle self compiler connection **extra context # REVERSE in Oracle is undocumented and doesn t support multi-byte # strings Use a special subquery instead sql params super as sql compiler connection template " SELECT LISTAGG s WITHIN GROUP ORDER BY n DESC FROM " " SELECT LEVEL n SUBSTR % expressions s LEVEL 1 s " "FROM DUAL CONNECT BY LEVEL < LENGTH % expressions s " "GROUP BY % expressions s " **extra context return sql params * 3
def test cached property reuse different names self """Disallow this case because the decorated function wouldn t be cached """ type msg "Cannot assign the same cached property to two different names a and " " b " if PY312 error type TypeError msg type msg else error type RuntimeError msg "Error calling set name " with self assertRaisesMessage error type msg as ctx class ReusedCachedProperty @cached property def a self pass b a if not PY312 self assertEqual str ctx exception context str TypeError type msg
def check self against using DEFAULT DB ALIAS """ Do a database query to check if the expressions of the Q instance matches against the expressions """ # Avoid circular imports from django db models import Value from django db models sql import Query from django db models sql constants import SINGLE query Query None for name value in against items if not hasattr value "resolve expression" value Value value query add annotation value name select False query add annotation Value 1 " check" # This will raise a FieldError if a field is missing in "against" query add q self compiler query get compiler using using try return compiler execute sql SINGLE is not None except DatabaseError as e logger warning "Got a database error calling check on %r %s" self e return True
def force group by self """ Return a GROUP BY clause to use with a HAVING clause when no grouping is specified """ return []
def init self * assume scheme None **kwargs if assume scheme is None if settings FORMS URLFIELD ASSUME HTTPS assume scheme "https" else warnings warn "The default scheme will be changed from http to https in " "Django 6 0 Pass the forms URLField assume scheme argument to " "silence this warning or set the FORMS URLFIELD ASSUME HTTPS " "transitional setting to True to opt into using https as the new " "default scheme " RemovedInDjango60Warning stacklevel 2 assume scheme "http" # RemovedInDjango60Warning When the deprecation ends replace with # self assume scheme assume scheme or "https" self assume scheme assume scheme super init strip True **kwargs
def init self * assume scheme None **kwargs if assume scheme is None if settings FORMS URLFIELD ASSUME HTTPS assume scheme "https" else warnings warn "The default scheme will be changed from http to https in " "Django 6 0 Pass the forms URLField assume scheme argument to " "silence this warning or set the FORMS URLFIELD ASSUME HTTPS " "transitional setting to True to opt into using https as the new " "default scheme " RemovedInDjango60Warning stacklevel 2 assume scheme "http" # RemovedInDjango60Warning When the deprecation ends replace with # self assume scheme assume scheme or "https" self assume scheme assume scheme super init strip True **kwargs
def init self ds input ds driver False write False encoding "utf-8" # The write flag self write capi GDAL OF UPDATE if write else capi GDAL OF READONLY # See also https gdal org development rfc rfc23 ogr unicode html self encoding encoding Driver ensure registered if isinstance ds input str Path try # GDALOpenEx will auto-detect the data source type ds capi open ds force bytes ds input self write | capi GDAL OF VECTOR None None None except GDALException # Making the error message more clear rather than something # like "Invalid pointer returned from OGROpen" raise GDALException Could not open the datasource at "%s" % ds input elif isinstance ds input self ptr type and isinstance ds driver Driver ptr type ds ds input else raise GDALException "Invalid data source input type %s" % type ds input if ds self ptr ds driver capi get dataset driver ds self driver Driver driver else # Raise an exception if the returned pointer is NULL raise GDALException Invalid data source file "%s" % ds input
def categorical focal crossentropy target output alpha 0 25 gamma 2 0 from logits False axis -1 """Computes the alpha balanced focal crossentropy loss According to [Lin et al 2018] https arxiv org pdf 1708 02002 pdf it helps to apply a focal factor to down-weight easy examples and focus more on hard examples By default the focal tensor is computed as follows It has pt defined as pt p if y 1 else 1 - p The authors use alpha-balanced variant of focal loss in the paper FL pt −α t * 1 − pt ^gamma * log pt Extending this to multi-class case is straightforward FL pt α t * 1 − pt ^gamma * CE where minus comes from negative log-likelihood and included in CE `modulating factor` is 1 − pt ^gamma where `gamma` is a focusing parameter When `gamma` 0 there is no focal effect on the categorical crossentropy And if alpha 1 at the same time the loss is equivalent to the categorical crossentropy Args target A tensor with the same shape as `output` output A tensor alpha A weight balancing factor for all classes default is `0 25` as mentioned in the reference It can be a list of floats or a scalar In the multi-class case alpha may be set by inverse class frequency by using `compute class weight` from `sklearn utils` gamma A focusing parameter default is `2 0` as mentioned in the reference It helps to gradually reduce the importance given to simple examples in a smooth manner from logits Whether `output` is expected to be a logits tensor By default we consider that `output` encodes a probability distribution Returns A tensor """ target tf convert to tensor target output tf convert to tensor output target shape assert is compatible with output shape output from logits get logits output from logits "Softmax" "categorical focal crossentropy" output tf internal smart cond smart cond from logits lambda softmax output lambda output # scale preds so that the class probas of each sample sum to 1 output output tf reduce sum output axis axis keepdims True epsilon constant to tensor epsilon output dtype base dtype output tf clip by value output epsilon 1 0 - epsilon # Calculate cross entropy cce -target * tf math log output # Calculate factors modulating factor tf pow 1 0 - output gamma weighting factor tf multiply modulating factor alpha # Apply weighting factor focal cce tf multiply weighting factor cce focal cce tf reduce sum focal cce axis axis return focal cce
def categorical focal crossentropy target output alpha 0 25 gamma 2 0 from logits False axis -1 """Computes the alpha balanced focal crossentropy loss According to [Lin et al 2018] https arxiv org pdf 1708 02002 pdf it helps to apply a focal factor to down-weight easy examples and focus more on hard examples By default the focal tensor is computed as follows It has pt defined as pt p if y 1 else 1 - p The authors use alpha-balanced variant of focal loss in the paper FL pt −α t * 1 − pt ^gamma * log pt Extending this to multi-class case is straightforward FL pt α t * 1 − pt ^gamma * CE where minus comes from negative log-likelihood and included in CE `modulating factor` is 1 − pt ^gamma where `gamma` is a focusing parameter When `gamma` 0 there is no focal effect on the categorical crossentropy And if alpha 1 at the same time the loss is equivalent to the categorical crossentropy Args target A tensor with the same shape as `output` output A tensor alpha A weight balancing factor for all classes default is `0 25` as mentioned in the reference It can be a list of floats or a scalar In the multi-class case alpha may be set by inverse class frequency by using `compute class weight` from `sklearn utils` gamma A focusing parameter default is `2 0` as mentioned in the reference It helps to gradually reduce the importance given to simple examples in a smooth manner from logits Whether `output` is expected to be a logits tensor By default we consider that `output` encodes a probability distribution Returns A tensor """ target tf convert to tensor target output tf convert to tensor output target shape assert is compatible with output shape output from logits get logits output from logits "Softmax" "categorical focal crossentropy" output tf internal smart cond smart cond from logits lambda softmax output lambda output # scale preds so that the class probas of each sample sum to 1 output output tf reduce sum output axis axis keepdims True epsilon constant to tensor epsilon output dtype base dtype output tf clip by value output epsilon 1 0 - epsilon # Calculate cross entropy cce -target * tf math log output # Calculate factors modulating factor tf pow 1 0 - output gamma weighting factor tf multiply modulating factor alpha # Apply weighting factor focal cce tf multiply weighting factor cce focal cce tf reduce sum focal cce axis axis return focal cce
def manage venv installs whl path other backends list set BACKEND REQ keys - {backend backend } install setup [ # Installs the backend s package and common requirements "pip install " + BACKEND REQ[backend backend ] "pip install -r requirements-common txt" "pip install pytest" # Ensure other backends are uninstalled "pip uninstall -y " + BACKEND REQ[other backends[0]] + " " + BACKEND REQ[other backends[1]] # Install ` whl` package "pip install " + whl path + " --force-reinstall --no-dependencies" ] run commands venv install setup
def compile from config self config """Compiles the model with the information given in config This method uses the information in the config optimizer loss metrics etc to compile the model Args config Dict containing information for compiling the model """ has overridden compile self class compile ! Model compile if has overridden compile logging warning "`compile ` was not called as part of model loading " "because the model s `compile ` method is custom " "All subclassed Models that have `compile ` " "overridden should also override " "`get compile config ` and `compile from config config ` " "Alternatively you can " "call `compile ` manually after loading " return config saving lib deserialize keras object config self compile **config if hasattr self "optimizer" # Exempt legacy optimizers and isinstance self optimizer optimizer Optimizer and self built # Create optimizer variables self optimizer build self trainable variables
def compile from config self config """Compiles the model with the information given in config This method uses the information in the config optimizer loss metrics etc to compile the model Args config Dict containing information for compiling the model """ has overridden compile self class compile ! Model compile if has overridden compile logging warning "`compile ` was not called as part of model loading " "because the model s `compile ` method is custom " "All subclassed Models that have `compile ` " "overridden should also override " "`get compile config ` and `compile from config config ` " "Alternatively you can " "call `compile ` manually after loading " return config saving lib deserialize keras object config self compile **config if hasattr self "optimizer" # Exempt legacy optimizers and isinstance self optimizer optimizer Optimizer and self built # Create optimizer variables self optimizer build self trainable variables
def pad to bounding box image offset height offset width target height target width check dims image backend convert to tensor image is batch True image shape image shape if len image shape 3 is batch False image backend numpy expand dims image 0 elif len image shape ! 4 raise ValueError " image shape %s must have either 3 or 4 dimensions " % image shape batch height width depth image shape after padding width target width - offset width - width after padding height target height - offset height - height if check dims if not offset height > 0 raise ValueError "offset height must be > 0" if not offset width > 0 raise ValueError "offset width must be > 0" if not after padding width > 0 raise ValueError "width must be < target - offset" if not after padding height > 0 raise ValueError "height must be < target - offset" paddings backend numpy reshape backend numpy stack [ 0 0 offset height after padding height offset width after padding width 0 0 ] [4 2] padded backend numpy pad image paddings padded shape [batch target height target width depth] padded backend numpy reshape padded padded shape if not is batch padded backend numpy squeeze padded axis [0] return padded
def distribute tensor tensor tensor layout """Change the layout of a Tensor value in the jit function execution Note that this might not work outside of the jitted function for certain backend To change the layout of a value eagerly please use `backend distribution lib distribute value` Args tensor a Tensor to change the layout tensor layout TensorLayout to be applied on the value Returns a new value with the specified tensor layout """ if isinstance tensor KerasTensor # keras tensor is only used for building functional model and can t be # used to alter layout sharding return tensor return distribution lib distribute tensor tensor tensor layout
def init self super init self train function None self test function None self predict function None # Ensures maximum compatibility when jit compile True # by instructing dynamo to graph breaks and delegate to python # for functions that cannot be traced # User can set this to False after instantiating the trainer # to see trace errors and fix them for better performance dynamo config suppress errors True
def handle finite grads self grads trainable variables scale self dynamic scale # Unscale gradients unscaled grads [ g if g is None else ops divide g scale for g in grads ] self inner optimizer apply unscaled grads trainable variables trainable variables def upscale self step counter assign 0 self dynamic scale assign self dynamic scale * 2 0 def increment self step counter assign add 1 # Potentially upscale loss and reset counter ops cond ops equal self step counter self dynamic growth steps - 1 upscale increment
def handle finite grads self grads trainable variables scale self dynamic scale # Unscale gradients unscaled grads [ g if g is None else ops divide g scale for g in grads ] self inner optimizer apply unscaled grads trainable variables trainable variables def upscale self step counter assign 0 self dynamic scale assign self dynamic scale * 2 0 def increment self step counter assign add 1 # Potentially upscale loss and reset counter ops cond ops equal self step counter self dynamic growth steps - 1 upscale increment
def logaddexp x1 x2 x1 convert to tensor x1 x2 convert to tensor x2 dtype dtypes result type x1 dtype x2 dtype float x1 tf cast x1 dtype x2 tf cast x2 dtype # TODO tfnp logaddexp incorrectly promote bfloat16 to float64 return tf cast tfnp logaddexp x1 x2 dtype
def test attention with dropout self layer layers Attention dropout 0 2 query np array [[[1 0 0 0] [0 0 1 0]]] value np array [[[1 0 1 0] [1 0 1 0]]] output scores layer [query value] return attention scores True self assertAllClose output [[[1 0 1 0] [1 0 1 0]]] self assertAllClose scores [[[0 5 0 5] [0 5 0 5]]]
def divide no nan self x1 x2 original dtype x1 dtype x2 binary ops cast x2 dtype "bool" x2 mask ops cast x2 binary dtype original dtype x1 masked x1 * x2 mask x2 binary inverted ~x2 binary x2 mask inverted ops cast x2 binary inverted original dtype x2 masked x2 + x2 mask inverted result x1 masked x2 masked return result
def alpha dropout inputs rate noise shape None seed None noise shape get concrete noise shape inputs noise shape alpha 1 6732632423543772848170429916717 scale 1 0507009873554804934193349852946 alpha p -alpha * scale kept idx jax numpy greater equal uniform noise shape seed seed rate kept idx cast kept idx inputs dtype # Compute affine transformation parameters a 1 - rate * 1 + rate * alpha p**2 ** -0 5 b -a * alpha p * rate # Apply mask x inputs * kept idx + alpha p * 1 - kept idx return a * x + b
def backend apply gradients self grads trainable variables if self gradient accumulation steps is update step self iterations + 1 % self gradient accumulation steps 0 steps self gradient accumulation steps current trainable vars value [ v value for v in trainable variables ] current optimizer vars value [v value for v in self variables] new g accs jax lax cond is update step lambda [ jnp zeros x shape dtype x dtype for x in self accumulated gradients ] lambda [ grads[i] + self accumulated gradients[i] for i in range len grads ] grads jax lax cond is update step lambda [ grads[i] + self accumulated gradients[i] steps for i in range len grads ] lambda list grads self backend update step grads trainable variables self learning rate new trainable vars jax lax cond is update step lambda [v value for v in trainable variables] lambda current trainable vars value new opt vars jax lax cond is update step lambda [v value for v in self variables] lambda current optimizer vars value for value v in zip new trainable vars trainable variables v assign value for value v in zip new opt vars self variables v assign value for n g acc g acc in zip new g accs self accumulated gradients g acc assign n g acc else self backend update step grads trainable variables self learning rate if self use ema self update model variables moving average self trainable variables if self ema overwrite frequency is not None should overwrite model vars self iterations + 1 % self ema overwrite frequency 0 should overwrite model vars int should overwrite model vars astype "int32" should not overwrite model vars int jnp logical not should overwrite model vars astype "int32" current trainable vars value [ v value for v in self trainable variables ] for var average var in zip self trainable variables self model variables moving average var assign average var * should overwrite model vars int + var value * should not overwrite model vars int self iterations assign add 1
def test stateless call self class TestLayer layers Layer def init self super init self seed generator backend random SeedGenerator 1337 self ntw self add weight shape initializer "zeros" trainable False self tw self add weight shape initializer "zeros" trainable True regularizer "l1" self built True def call self x x backend convert to tensor x dtype "float32" self add loss ops sum x self ntw assign ops sum x x x + backend random normal shape seed self seed generator return x + self tw + self ntw data np random random 3 4 layer TestLayer out layer data layer1 TestLayer out1 layer1 data # Check that the layer is in fact deterministic self assertAllClose out out1 # Test stateless call correctness layer2 TestLayer trainable variables layer2 trainable variables non trainable variables layer2 non trainable variables out2 non trainable variables layer2 stateless call trainable variables non trainable variables data self assertAllClose out1 out2 self assertEqual len layer1 non trainable variables len non trainable variables for ref v v in zip layer1 non trainable variables non trainable variables self assertAllClose ref v v # Test with loss collection layer3 TestLayer trainable variables layer3 trainable variables non trainable variables layer3 non trainable variables out3 non trainable variables losses layer3 stateless call trainable variables non trainable variables data return losses True self assertAllClose out1 out3 for ref v v in zip layer1 non trainable variables non trainable variables self assertAllClose ref v v self assertLen losses 2 for ref loss loss in zip layer1 losses losses self assertAllClose ref loss loss
def prefetch to device self numpy iterator """Shard and prefetch batches on device Most of the implementation has been borrowed from `flax jax utils prefetch to device` This utility takes an iterator and returns a new iterator which fills an on device prefetch buffer Eager prefetching can improve the performance of training loops significantly by overlapping compute and data transfer """ queue collections deque # If you re training on GPUs 2 is generally the best choice because # this guarantees that you can overlap a training step on GPU with a # data prefetch step on CPU def enqueue n 2 for data in itertools islice numpy iterator n queue append distribute data data enqueue n 2 # TODO should we make `n` configurable? while queue yield queue popleft enqueue 1
def test savefig preserve layout engine tmp path fig plt figure layout compressed fig savefig tmp path foo png bbox inches tight assert fig get layout engine compress
def test MultiCursor horizOn vertOn ax1 ax3 plt figure subplots 2 sharex True ax2 plt figure subplots # useblit false to avoid having to draw the figure to cache the renderer multi widgets MultiCursor None ax1 ax2 useblit False horizOn horizOn vertOn vertOn # Only two of the axes should have a line drawn on them assert len multi vlines 2 assert len multi hlines 2 # mock a motion notify event # Can t use `do event` as that helper requires the widget # to have a single ax attribute event mock event ax1 xdata 5 ydata 25 multi onmove event # the lines in the first two ax should both move for l in multi vlines assert l get xdata 5 5 for l in multi hlines assert l get ydata 25 25 # The relevant lines get turned on after move assert len [line for line in multi vlines if line get visible ] 2 if vertOn else 0 assert len [line for line in multi hlines if line get visible ] 2 if horizOn else 0 # After toggling settings the opposite lines should be visible after move multi horizOn not multi horizOn multi vertOn not multi vertOn event mock event ax1 xdata 5 ydata 25 multi onmove event assert len [line for line in multi vlines if line get visible ] 0 if vertOn else 2 assert len [line for line in multi hlines if line get visible ] 0 if horizOn else 2 # test a move event in an Axes not part of the MultiCursor # the lines in ax1 and ax2 should not have moved event mock event ax3 xdata 75 ydata 75 multi onmove event for l in multi vlines assert l get xdata 5 5 for l in multi hlines assert l get ydata 25 25
def boxplot self x notch None sym None vert None whis None positions None widths None patch artist None bootstrap None usermedians None conf intervals None meanline None showmeans None showcaps None showbox None showfliers None boxprops None labels None flierprops None medianprops None meanprops None capprops None whiskerprops None manage ticks True autorange False zorder None capwidths None """ Draw a box and whisker plot The box extends from the first quartile Q1 to the third quartile Q3 of the data with a line at the median The whiskers extend from the box to the farthest point within 1 5x the inter-quartile range IQR distance Flier points are those past the end of the whiskers See https en wikipedia org wiki Box plot for reference code-block none Q1-1 5IQR Q1 median Q3 Q3+1 5IQR |----- -----| o |--------| |--------| o o |----- -----| flier <-----------> fliers IQR Parameters ---------- x Array or a sequence of vectors The input data If a 2D array a boxplot is drawn for each column in *x* If a sequence of 1D arrays a boxplot is drawn for each array in *x* notch bool default False Whether to draw a notched boxplot `True` or a rectangular boxplot `False` The notches represent the confidence interval CI around the median The documentation for *bootstrap* describes how the locations of the notches are computed by default but their locations may also be overridden by setting the *conf intervals* parameter note In cases where the values of the CI are less than the lower quartile or greater than the upper quartile the notches will extend beyond the box giving it a distinctive "flipped" appearance This is expected behavior and consistent with other statistical visualization packages sym str optional The default symbol for flier points An empty string hides the fliers If `None` then the fliers default to b+ More control is provided by the *flierprops* parameter vert bool default True If `True` draws vertical boxes If `False` draw horizontal boxes whis float or float float default 1 5 The position of the whiskers If a float the lower whisker is at the lowest datum above ``Q1 - whis* Q3-Q1 `` and the upper whisker at the highest datum below ``Q3 + whis* Q3-Q1 `` where Q1 and Q3 are the first and third quartiles The default value of ``whis 1 5`` corresponds to Tukey s original definition of boxplots If a pair of floats they indicate the percentiles at which to draw the whiskers e g 5 95 In particular setting this to 0 100 results in whiskers covering the whole range of the data In the edge case where ``Q1 Q3`` *whis* is automatically set to 0 100 cover the whole range of the data if *autorange* is True Beyond the whiskers data are considered outliers and are plotted as individual points bootstrap int optional Specifies whether to bootstrap the confidence intervals around the median for notched boxplots If *bootstrap* is None no bootstrapping is performed and notches are calculated using a Gaussian-based asymptotic approximation see McGill R Tukey J W and Larsen W A 1978 and Kendall and Stuart 1967 Otherwise bootstrap specifies the number of times to bootstrap the median to determine its 95% confidence intervals Values between 1000 and 10000 are recommended usermedians 1D array-like optional A 1D array-like of length ``len x `` Each entry that is not `None` forces the value of the median for the corresponding dataset For entries that are `None` the medians are computed by Matplotlib as normal conf intervals array-like optional A 2D array-like of shape `` len x 2 `` Each entry that is not None forces the location of the corresponding notch which is only drawn if *notch* is `True` For entries that are `None` the notches are computed by the method specified by the other parameters e g *bootstrap* positions array-like optional The positions of the boxes The ticks and limits are automatically set to match the positions Defaults to ``range 1 N+1 `` where N is the number of boxes to be drawn widths float or array-like The widths of the boxes The default is 0 5 or ``0 15* distance between extreme positions `` if that is smaller patch artist bool default False If `False` produces boxes with the Line2D artist Otherwise boxes are drawn with Patch artists labels sequence optional Labels for each dataset one per dataset manage ticks bool default True If True the tick locations and labels will be adjusted to match the boxplot positions autorange bool default False When `True` and the data are distributed such that the 25th and 75th percentiles are equal *whis* is set to 0 100 such that the whisker ends are at the minimum and maximum of the data meanline bool default False If `True` and *showmeans* is `True` will try to render the mean as a line spanning the full width of the box according to *meanprops* see below Not recommended if *shownotches* is also True Otherwise means will be shown as points zorder float default ``Line2D zorder 2`` The zorder of the boxplot Returns ------- dict A dictionary mapping each component of the boxplot to a list of the ` Line2D` instances created That dictionary has the following keys assuming vertical boxplots - ``boxes`` the main body of the boxplot showing the quartiles and the median s confidence intervals if enabled - ``medians`` horizontal lines at the median of each box - ``whiskers`` the vertical lines extending to the most extreme non-outlier data points - ``caps`` the horizontal lines at the ends of the whiskers - ``fliers`` points representing data that extend beyond the whiskers fliers - ``means`` points or lines representing the means Other Parameters ---------------- showcaps bool default True Show the caps on the ends of whiskers showbox bool default True Show the central box showfliers bool default True Show the outliers beyond the caps showmeans bool default False Show the arithmetic means capprops dict default None The style of the caps capwidths float or array default None The widths of the caps boxprops dict default None The style of the box whiskerprops dict default None The style of the whiskers flierprops dict default None The style of the fliers medianprops dict default None The style of the median meanprops dict default None The style of the mean data indexable object optional DATA PARAMETER PLACEHOLDER See Also -------- violinplot Draw an estimate of the probability density function """ # Missing arguments default to rcParams if whis is None whis mpl rcParams[ boxplot whiskers ] if bootstrap is None bootstrap mpl rcParams[ boxplot bootstrap ] bxpstats cbook boxplot stats x whis whis bootstrap bootstrap labels labels autorange autorange if notch is None notch mpl rcParams[ boxplot notch ] if vert is None vert mpl rcParams[ boxplot vertical ] if patch artist is None patch artist mpl rcParams[ boxplot patchartist ] if meanline is None meanline mpl rcParams[ boxplot meanline ] if showmeans is None showmeans mpl rcParams[ boxplot showmeans ] if showcaps is None showcaps mpl rcParams[ boxplot showcaps ] if showbox is None showbox mpl rcParams[ boxplot showbox ] if showfliers is None showfliers mpl rcParams[ boxplot showfliers ] if boxprops is None boxprops {} if whiskerprops is None whiskerprops {} if capprops is None capprops {} if medianprops is None medianprops {} if meanprops is None meanprops {} if flierprops is None flierprops {} if patch artist boxprops[ linestyle ] solid # Not consistent with bxp if color in boxprops boxprops[ edgecolor ] boxprops pop color # if non-default sym value put it into the flier dictionary # the logic for providing the default symbol b+ now lives # in bxp in the initial value of flierkw # handle all of the *sym* related logic here so we only have to pass # on the flierprops dict if sym is not None # no-flier case which should really be done with # showfliers False but none-the-less deal with it to keep back # compatibility if sym # blow away existing dict and make one for invisible markers flierprops dict linestyle none marker color none # turn the fliers off just to be safe showfliers False # now process the symbol string else # process the symbol string # discarded linestyle marker color process plot format sym # if we have a marker use it if marker is not None flierprops[ marker ] marker # if we have a color use it if color is not None # assume that if color is passed in the user want # filled symbol if the users want more control use # flierprops flierprops[ color ] color flierprops[ markerfacecolor ] color flierprops[ markeredgecolor ] color # replace medians if necessary if usermedians is not None if len np ravel usermedians ! len bxpstats or np shape usermedians [0] ! len bxpstats raise ValueError " usermedians and x have different lengths" else # reassign medians as necessary for stats med in zip bxpstats usermedians if med is not None stats[ med ] med if conf intervals is not None if len conf intervals ! len bxpstats raise ValueError " conf intervals and x have different lengths" else for stats ci in zip bxpstats conf intervals if ci is not None if len ci ! 2 raise ValueError each confidence interval must have two values else if ci[0] is not None stats[ cilo ] ci[0] if ci[1] is not None stats[ cihi ] ci[1] artists self bxp bxpstats positions positions widths widths vert vert patch artist patch artist shownotches notch showmeans showmeans showcaps showcaps showbox showbox boxprops boxprops flierprops flierprops medianprops medianprops meanprops meanprops meanline meanline showfliers showfliers capprops capprops whiskerprops whiskerprops manage ticks manage ticks zorder zorder capwidths capwidths return artists
def boxplot self x notch None sym None vert None whis None positions None widths None patch artist None bootstrap None usermedians None conf intervals None meanline None showmeans None showcaps None showbox None showfliers None boxprops None labels None flierprops None medianprops None meanprops None capprops None whiskerprops None manage ticks True autorange False zorder None capwidths None """ Draw a box and whisker plot The box extends from the first quartile Q1 to the third quartile Q3 of the data with a line at the median The whiskers extend from the box to the farthest point within 1 5x the inter-quartile range IQR distance Flier points are those past the end of the whiskers See https en wikipedia org wiki Box plot for reference code-block none Q1-1 5IQR Q1 median Q3 Q3+1 5IQR |----- -----| o |--------| |--------| o o |----- -----| flier <-----------> fliers IQR Parameters ---------- x Array or a sequence of vectors The input data If a 2D array a boxplot is drawn for each column in *x* If a sequence of 1D arrays a boxplot is drawn for each array in *x* notch bool default False Whether to draw a notched boxplot `True` or a rectangular boxplot `False` The notches represent the confidence interval CI around the median The documentation for *bootstrap* describes how the locations of the notches are computed by default but their locations may also be overridden by setting the *conf intervals* parameter note In cases where the values of the CI are less than the lower quartile or greater than the upper quartile the notches will extend beyond the box giving it a distinctive "flipped" appearance This is expected behavior and consistent with other statistical visualization packages sym str optional The default symbol for flier points An empty string hides the fliers If `None` then the fliers default to b+ More control is provided by the *flierprops* parameter vert bool default True If `True` draws vertical boxes If `False` draw horizontal boxes whis float or float float default 1 5 The position of the whiskers If a float the lower whisker is at the lowest datum above ``Q1 - whis* Q3-Q1 `` and the upper whisker at the highest datum below ``Q3 + whis* Q3-Q1 `` where Q1 and Q3 are the first and third quartiles The default value of ``whis 1 5`` corresponds to Tukey s original definition of boxplots If a pair of floats they indicate the percentiles at which to draw the whiskers e g 5 95 In particular setting this to 0 100 results in whiskers covering the whole range of the data In the edge case where ``Q1 Q3`` *whis* is automatically set to 0 100 cover the whole range of the data if *autorange* is True Beyond the whiskers data are considered outliers and are plotted as individual points bootstrap int optional Specifies whether to bootstrap the confidence intervals around the median for notched boxplots If *bootstrap* is None no bootstrapping is performed and notches are calculated using a Gaussian-based asymptotic approximation see McGill R Tukey J W and Larsen W A 1978 and Kendall and Stuart 1967 Otherwise bootstrap specifies the number of times to bootstrap the median to determine its 95% confidence intervals Values between 1000 and 10000 are recommended usermedians 1D array-like optional A 1D array-like of length ``len x `` Each entry that is not `None` forces the value of the median for the corresponding dataset For entries that are `None` the medians are computed by Matplotlib as normal conf intervals array-like optional A 2D array-like of shape `` len x 2 `` Each entry that is not None forces the location of the corresponding notch which is only drawn if *notch* is `True` For entries that are `None` the notches are computed by the method specified by the other parameters e g *bootstrap* positions array-like optional The positions of the boxes The ticks and limits are automatically set to match the positions Defaults to ``range 1 N+1 `` where N is the number of boxes to be drawn widths float or array-like The widths of the boxes The default is 0 5 or ``0 15* distance between extreme positions `` if that is smaller patch artist bool default False If `False` produces boxes with the Line2D artist Otherwise boxes are drawn with Patch artists labels sequence optional Labels for each dataset one per dataset manage ticks bool default True If True the tick locations and labels will be adjusted to match the boxplot positions autorange bool default False When `True` and the data are distributed such that the 25th and 75th percentiles are equal *whis* is set to 0 100 such that the whisker ends are at the minimum and maximum of the data meanline bool default False If `True` and *showmeans* is `True` will try to render the mean as a line spanning the full width of the box according to *meanprops* see below Not recommended if *shownotches* is also True Otherwise means will be shown as points zorder float default ``Line2D zorder 2`` The zorder of the boxplot Returns ------- dict A dictionary mapping each component of the boxplot to a list of the ` Line2D` instances created That dictionary has the following keys assuming vertical boxplots - ``boxes`` the main body of the boxplot showing the quartiles and the median s confidence intervals if enabled - ``medians`` horizontal lines at the median of each box - ``whiskers`` the vertical lines extending to the most extreme non-outlier data points - ``caps`` the horizontal lines at the ends of the whiskers - ``fliers`` points representing data that extend beyond the whiskers fliers - ``means`` points or lines representing the means Other Parameters ---------------- showcaps bool default True Show the caps on the ends of whiskers showbox bool default True Show the central box showfliers bool default True Show the outliers beyond the caps showmeans bool default False Show the arithmetic means capprops dict default None The style of the caps capwidths float or array default None The widths of the caps boxprops dict default None The style of the box whiskerprops dict default None The style of the whiskers flierprops dict default None The style of the fliers medianprops dict default None The style of the median meanprops dict default None The style of the mean data indexable object optional DATA PARAMETER PLACEHOLDER See Also -------- violinplot Draw an estimate of the probability density function """ # Missing arguments default to rcParams if whis is None whis mpl rcParams[ boxplot whiskers ] if bootstrap is None bootstrap mpl rcParams[ boxplot bootstrap ] bxpstats cbook boxplot stats x whis whis bootstrap bootstrap labels labels autorange autorange if notch is None notch mpl rcParams[ boxplot notch ] if vert is None vert mpl rcParams[ boxplot vertical ] if patch artist is None patch artist mpl rcParams[ boxplot patchartist ] if meanline is None meanline mpl rcParams[ boxplot meanline ] if showmeans is None showmeans mpl rcParams[ boxplot showmeans ] if showcaps is None showcaps mpl rcParams[ boxplot showcaps ] if showbox is None showbox mpl rcParams[ boxplot showbox ] if showfliers is None showfliers mpl rcParams[ boxplot showfliers ] if boxprops is None boxprops {} if whiskerprops is None whiskerprops {} if capprops is None capprops {} if medianprops is None medianprops {} if meanprops is None meanprops {} if flierprops is None flierprops {} if patch artist boxprops[ linestyle ] solid # Not consistent with bxp if color in boxprops boxprops[ edgecolor ] boxprops pop color # if non-default sym value put it into the flier dictionary # the logic for providing the default symbol b+ now lives # in bxp in the initial value of flierkw # handle all of the *sym* related logic here so we only have to pass # on the flierprops dict if sym is not None # no-flier case which should really be done with # showfliers False but none-the-less deal with it to keep back # compatibility if sym # blow away existing dict and make one for invisible markers flierprops dict linestyle none marker color none # turn the fliers off just to be safe showfliers False # now process the symbol string else # process the symbol string # discarded linestyle marker color process plot format sym # if we have a marker use it if marker is not None flierprops[ marker ] marker # if we have a color use it if color is not None # assume that if color is passed in the user want # filled symbol if the users want more control use # flierprops flierprops[ color ] color flierprops[ markerfacecolor ] color flierprops[ markeredgecolor ] color # replace medians if necessary if usermedians is not None if len np ravel usermedians ! len bxpstats or np shape usermedians [0] ! len bxpstats raise ValueError " usermedians and x have different lengths" else # reassign medians as necessary for stats med in zip bxpstats usermedians if med is not None stats[ med ] med if conf intervals is not None if len conf intervals ! len bxpstats raise ValueError " conf intervals and x have different lengths" else for stats ci in zip bxpstats conf intervals if ci is not None if len ci ! 2 raise ValueError each confidence interval must have two values else if ci[0] is not None stats[ cilo ] ci[0] if ci[1] is not None stats[ cihi ] ci[1] artists self bxp bxpstats positions positions widths widths vert vert patch artist patch artist shownotches notch showmeans showmeans showcaps showcaps showbox showbox boxprops boxprops flierprops flierprops medianprops medianprops meanprops meanprops meanline meanline showfliers showfliers capprops capprops whiskerprops whiskerprops manage ticks manage ticks zorder zorder capwidths capwidths return artists
def get axis name self """Return the axis name """ return [name for name axis in self axes axis map items if axis is self][0]
def test toolbar button la mode icon # test that icon in LA mode can be used for buttons # see GH#25164 import tempfile import warnings from PIL import Image import matplotlib import matplotlib pyplot as plt from matplotlib backend tools import ToolToggleBase # tweaking toolbar raises an UserWarning with warnings catch warnings warnings simplefilter "ignore" UserWarning matplotlib rcParams["toolbar"] "toolmanager" # create an icon in LA mode with tempfile TemporaryDirectory as tempdir img Image new "LA" 26 26 tmp img path os path join tempdir "test la icon png" img save tmp img path class CustomTool ToolToggleBase image tmp img path fig plt figure toolmanager fig canvas manager toolmanager toolbar fig canvas manager toolbar toolmanager add tool "test" CustomTool toolbar add tool "test" "group" print "success"
def test interactive backend env toolbar if env["MPLBACKEND"] "macosx" if toolbar "toolmanager" pytest skip "toolmanager is not implemented for macosx " if env["MPLBACKEND"] "wx" pytest skip "wx backend is deprecated; tests failed on appveyor" try proc run helper test interactive impl json dumps {"toolbar" toolbar} timeout test timeout extra env env except subprocess CalledProcessError as err pytest fail "Subprocess failed to test intended behavior\n" + str err stderr assert proc stdout count "CloseEvent" 1
inline double mpl round double v { return double mpl round to int v ; }
def nonsingular self vmin vmax if vmin > vmax vmin vmax vmax vmin if not np isfinite vmin or not np isfinite vmax vmin vmax 1 10 # Initial range no data plotted yet elif vmax < 0 api warn external "Data has no positive values and therefore cannot be " "log-scaled " vmin vmax 1 10 else ax self axis axes shared axises [ other axis for other ax in [*ax shared axes["x"] get siblings ax *ax shared axes["y"] get siblings ax *ax shared axes["z"] get siblings ax ] for other axis in other ax axis map values if other axis get major locator is self] minpos min self axis get minpos * other axis get minpos for other axis in shared axises if not np isfinite minpos minpos 1e-300 # This should never take effect if vmin < 0 vmin minpos if vmin vmax vmin decade less vmin self base vmax decade greater vmax self base return vmin vmax
def finalize offset self if self update "loc" self update loc self get loc in canvas elif self update "bbox" self update bbox to anchor self get loc in canvas
def init self ref artist use blit False self ref artist ref artist if not ref artist pickable ref artist set picker True self got artist False self use blit use blit and self canvas supports blit callbacks ref artist figure canvas callbacks self disconnectors [ functools partial callbacks disconnect callbacks connect picklable name func for name func in [ "pick event" self on pick "button release event" self on release "motion notify event" self on motion ] ]
def visit figmpl html self node imagedir srcset rel copy images figmpl self node # doc examples subd plot 1 rst docsource PurePosixPath self document[ source ] # doc # make sure to add the trailing slash srctop PurePosixPath self builder srcdir # examples subd plot 1 rst relsource relpath docsource srctop # doc build html desttop PurePosixPath self builder outdir # doc build html examples subd dest desttop relsource # images for dirhtml and images for html imagerel relpath imagedir os path dirname dest if self builder name "dirhtml" imagerel + imagerel + else # html imagerel imagerel + # make uri also be relative nm PurePosixPath node[ uri ][1 ] name uri imagerel + + rel + nm # make srcset str Need to change all the prefixes! if srcset srcsetst maxmult -1 for mult in srcset nm PurePosixPath srcset[mult][1 ] name # images plot 1 2 0x png path imagerel + + rel + nm srcsetst + f {path} if mult 0 srcsetst + else srcsetst + f {mult 1 2f}x if mult > maxmult maxmult mult maxsrc path # trim trailing comma and space srcsetst srcsetst[ -2] else srcsetst maxsrc uri alt node[ alt ] if node[ class ] is not None classst for cl in node[ class ] classst + cl + classst f class "{classst}" else classst # <figure class "align-default" id "id1"> # <a class "reference internal image-reference" href " images index-1 2x png"> # <img alt " images index-1 2x png" src " images index-1 2x png" style "width 53%;" > # < a> # <figcaption> # <p><span class "caption-text">Figure caption is here < span> # <a class "headerlink" href "#id1" title "Permalink to this image">#< a>< p> # < figcaption> # < figure> stylers [ width height scale ] stylest for style in stylers if node[style] stylest + f {style} {node[style]}; figalign node[ align ] if node[ align ] else center img block f <img src "{uri}" style "{stylest}" srcset "{srcsetst}" + f alt "{alt}" {classst} > html block f <figure class "align-{figalign}">\n html block + f <a class "reference internal image-reference" href "{maxsrc}">\n html block + + img block + \n < a>\n if node[ caption ] html block + <figcaption>\n html block + f <p><span class "caption-text">{node["caption"]}< span>< p>\n html block + < figcaption>\n html block + < figure>\n self body append html block
def safe first finite obj * skip nonfinite True """ Return the first non-None and optionally finite element in *obj* This is a method for internal use This is a type-independent way of obtaining the first non-None element supporting both index access and the iterator protocol The first non-None element will be obtained when skip none is True """ def safe isfinite val if val is None return False try return np isfinite val if np isscalar val else True except TypeError # This is something that NumPy cannot make heads or tails of # assume "finite" return True if skip nonfinite is False if isinstance obj collections abc Iterator # needed to accept `array flat` as input # np flatiter reports as an instance of collections Iterator # but can still be indexed via [] # This has the side effect of re-setting the iterator but # that is acceptable try return obj[0] except TypeError pass raise RuntimeError "matplotlib does not support generators " "as input" return next iter obj elif isinstance obj np flatiter # TODO do the finite filtering on this return obj[0] elif isinstance obj collections abc Iterator raise RuntimeError "matplotlib does not " "support generators as input" else try return next val for val in obj if safe isfinite val except TypeError # not an iterable return obj
def getMinorMajor ellipse Ellipse -> Tuple[list list] """ Calculates the end points of minor and major axis of an ellipse Parameters ---------- ellipse ~matplotlib patches Ellipse Ellipse patch Returns ------- ~typing Tuple[list list] """ # Calculate the endpoints of the minor axis x0 minor ellipse center[0] - ellipse height 2 * np sin np deg2rad ellipse angle y0 minor ellipse center[1] + ellipse height 2 * np cos np deg2rad ellipse angle x1 minor ellipse center[0] + ellipse height 2 * np sin np deg2rad ellipse angle y1 minor ellipse center[1] - ellipse height 2 * np cos np deg2rad ellipse angle # Calculate the endpoints of the major axis x0 major ellipse center[0] - ellipse width 2 * np cos np deg2rad ellipse angle y0 major ellipse center[1] - ellipse width 2 * np sin np deg2rad ellipse angle x1 major ellipse center[0] + ellipse width 2 * np cos np deg2rad ellipse angle y1 major ellipse center[1] + ellipse width 2 * np sin np deg2rad ellipse angle return [ x0 minor y0 minor x1 minor y1 minor ] [ x0 major y0 major x1 major y1 major ]
def init self patch ox oy * shade 0 3 **kwargs """ Create a shadow of the given *patch* By default the shadow will have the same face color as the *patch* but darkened The darkness can be controlled by *shade* Parameters ---------- patch ` Patch` The patch to create the shadow for ox oy float The shift of the shadow in data coordinates scaled by a factor of dpi 72 shade float default 0 3 How the darkness of the shadow relates to the original color If 0 the shadow is black if 1 the shadow has the same color as the *patch* versionadded 3 8 **kwargs Properties of the shadow patch Supported keys are % Patch kwdoc s """ super init self patch patch self ox self oy ox oy self shadow transform transforms Affine2D self update from self patch if not 0 < shade < 1 raise ValueError "shade must be between 0 and 1 " color shade * np asarray colors to rgb self patch get facecolor self update { facecolor color edgecolor color alpha 0 5 # Place shadow patch directly behind the inherited patch zorder np nextafter self patch zorder -np inf **kwargs}
def switch backend newbackend """ Set the pyplot backend Switching to an interactive backend is possible only if no event loop for another interactive backend has started Switching to and from non-interactive backends is always possible If the new backend is different than the current backend then all open Figures will be closed via ``plt close all `` Parameters ---------- newbackend str The case-insensitive name of the backend to use """ global backend mod # make sure the init is pulled up so we can assign to it later import matplotlib backends if newbackend is rcsetup auto backend sentinel current framework cbook get running interactive framework mapping { qt qtagg gtk3 gtk3agg gtk4 gtk4agg wx wxagg tk tkagg macosx macosx headless agg } best guess mapping get current framework None if best guess is not None candidates [best guess] else candidates [] candidates + [ "macosx" "qtagg" "gtk4agg" "gtk3agg" "tkagg" "wxagg"] # Don t try to fallback on the cairo-based backends as they each have # an additional dependency pycairo over the agg-based backend and # are of worse quality for candidate in candidates try switch backend candidate except ImportError continue else rcParamsOrig[ backend ] candidate return else # Switching to Agg should always succeed; if it doesn t let the # exception propagate out switch backend "agg" rcParamsOrig["backend"] "agg" return # have to escape the switch on access logic old backend dict getitem rcParams backend backend mod importlib import module cbook backend module name newbackend required framework backend mod FigureCanvas required interactive framework if required framework is not None current framework cbook get running interactive framework if current framework and required framework and current framework ! required framework raise ImportError "Cannot load backend {!r} which requires the {!r} interactive " "framework as {!r} is currently running" format newbackend required framework current framework # Load the new figure manager and show functions from the backend # Classically backends can directly export these functions This should # keep working for backcompat new figure manager getattr backend mod "new figure manager" None show getattr backend mod "show" None # In that classical approach backends are implemented as modules but # "inherit" default method implementations from backend bases Backend # This is achieved by creating a "class" that inherits from # backend bases Backend and whose body is filled with the module globals class backend mod matplotlib backend bases Backend locals update vars backend mod # However the newer approach for defining new figure manager and # show is to derive them from canvas methods In that case also # update backend mod accordingly; also per-backend customization of # draw if interactive is disabled if new figure manager is None # Only try to get the canvas class if have opted into the new scheme canvas class backend mod FigureCanvas def new figure manager given figure num figure return canvas class new manager figure num def new figure manager num *args FigureClass Figure **kwargs fig FigureClass *args **kwargs return new figure manager given figure num fig def draw if interactive if matplotlib is interactive manager pylab helpers Gcf get active if manager manager canvas draw idle backend mod new figure manager given figure \ new figure manager given figure backend mod new figure manager new figure manager backend mod draw if interactive draw if interactive # If the manager explicitly overrides pyplot show use it even if a global # show is already present as the latter may be here for backcompat manager class getattr getattr backend mod "FigureCanvas" None "manager class" None # We can t compare directly manager class pyplot show and FMB pyplot show because # pyplot show is a classmethod so the above constructs are bound classmethods and # thus always different being bound to different classes We also have to use # getattr static instead of vars as manager class could have no dict manager pyplot show inspect getattr static manager class "pyplot show" None base pyplot show inspect getattr static FigureManagerBase "pyplot show" None if show is None or manager pyplot show is not None and manager pyplot show ! base pyplot show backend mod show manager class pyplot show log debug "Loaded backend %s version %s " newbackend backend mod backend version rcParams[ backend ] rcParamsDefault[ backend ] newbackend backend mod backend mod for func name in ["new figure manager" "draw if interactive" "show"] globals [func name] signature inspect signature getattr backend mod func name # Need to keep a global reference to the backend for compatibility reasons # See https github com matplotlib matplotlib issues 6092 matplotlib backends backend newbackend if not cbook str equal old backend newbackend close "all" # make sure the repl display hook is installed in case we become # interactive try install repl displayhook except ImportError as err log warning str err raise ImportError
def init self text xy xytext None xycoords data textcoords None arrowprops None annotation clip None **kwargs """ Annotate the point *xy* with text *text* In the simplest form the text is placed at *xy* Optionally the text can be displayed in another position *xytext* An arrow pointing from the text to the annotated point *xy* can then be added by defining *arrowprops* Parameters ---------- text str The text of the annotation xy float float The point * x y * to annotate The coordinate system is determined by *xycoords* xytext float float default *xy* The position * x y * to place the text at The coordinate system is determined by *textcoords* xycoords single or two-tuple of str or ` Artist` or ` Transform` or \ callable default data The coordinate system that *xy* is given in The following types of values are supported - One of the following strings Value Description figure points Points from the lower left of the figure figure pixels Pixels from the lower left of the figure figure fraction Fraction of figure from lower left subfigure points Points from the lower left of the subfigure subfigure pixels Pixels from the lower left of the subfigure subfigure fraction Fraction of subfigure from lower left axes points Points from lower left corner of axes axes pixels Pixels from lower left corner of axes axes fraction Fraction of axes from lower left data Use the coordinate system of the object being annotated default polar * theta r * if not native data coordinates Note that subfigure pixels and figure pixels are the same for the parent figure so users who want code that is usable in a subfigure can use subfigure pixels - An ` Artist` *xy* is interpreted as a fraction of the artist s `~matplotlib transforms Bbox` E g * 0 0 * would be the lower left corner of the bounding box and * 0 5 1 * would be the center top of the bounding box - A ` Transform` to transform *xy* to screen coordinates - A function with one of the following signatures def transform renderer -> Bbox def transform renderer -> Transform where *renderer* is a ` RendererBase` subclass The result of the function is interpreted like the ` Artist` and ` Transform` cases above - A tuple * xcoords ycoords * specifying separate coordinate systems for *x* and *y* *xcoords* and *ycoords* must each be of one of the above described types See ref `plotting-guide-annotation` for more details textcoords single or two-tuple of str or ` Artist` or ` Transform` \ or callable default value of *xycoords* The coordinate system that *xytext* is given in All *xycoords* values are valid as well as the following strings Value Description offset points Offset in points from the *xy* value offset pixels Offset in pixels from the *xy* value offset fontsize Offset relative to fontsize from the *xy* value arrowprops dict optional The properties used to draw a ` FancyArrowPatch` arrow between the positions *xy* and *xytext* Defaults to None i e no arrow is drawn For historical reasons there are two different ways to specify arrows "simple" and "fancy" **Simple arrow ** If *arrowprops* does not contain the key arrowstyle the allowed keys are Key Description width The width of the arrow in points headwidth The width of the base of the arrow head in points headlength The length of the arrow head in points shrink Fraction of total length to shrink from both ends ? Any key to class `matplotlib patches FancyArrowPatch` The arrow is attached to the edge of the text box the exact position corners or centers depending on where it s pointing to **Fancy arrow ** This is used if arrowstyle is provided in the *arrowprops* Valid keys are the following `~matplotlib patches FancyArrowPatch` parameters Key Description arrowstyle the arrow style connectionstyle the connection style relpos see below; default is 0 5 0 5 patchA default is bounding box of the text patchB default is None shrinkA default is 2 points shrinkB default is 2 points mutation scale default is text size in points mutation aspect default is 1 ? any key for class `matplotlib patches PathPatch` The exact starting point position of the arrow is defined by *relpos* It s a tuple of relative coordinates of the text box where 0 0 is the lower left corner and 1 1 is the upper right corner Values <0 and >1 are supported and specify points outside the text box By default 0 5 0 5 so the starting point is centered in the text box annotation clip bool or None default None Whether to clip i e not draw the annotation when the annotation point *xy* is outside the axes area - If *True* the annotation will be clipped when *xy* is outside the axes - If *False* the annotation will always be drawn - If *None* the annotation will be clipped when *xy* is outside the axes and *xycoords* is data **kwargs Additional kwargs are passed to `~matplotlib text Text` Returns ------- ` Annotation` See Also -------- ref `plotting-guide-annotation` """ AnnotationBase init self xy xycoords xycoords annotation clip annotation clip # warn about wonky input data if xytext is None and textcoords is not None and textcoords ! xycoords api warn external "You have used the `textcoords` kwarg but " "not the `xytext` kwarg This can lead to " "surprising results " # clean up textcoords and assign default if textcoords is None textcoords self xycoords self textcoords textcoords # cleanup xytext defaults if xytext is None xytext self xy x y xytext self arrowprops arrowprops if arrowprops is not None arrowprops arrowprops copy if "arrowstyle" in arrowprops self arrow relpos arrowprops pop "relpos" 0 5 0 5 else # modified YAArrow API to be used with FancyArrowPatch for key in [ width headwidth headlength shrink frac ] arrowprops pop key None self arrow patch FancyArrowPatch 0 0 1 1 **arrowprops else self arrow patch None # Must come last as some kwargs may be propagated to arrow patch Text init self x y text **kwargs
def subfigures self nrows 1 ncols 1 squeeze True wspace None hspace None width ratios None height ratios None **kwargs """ Add a set of subfigures to this figure or subfigure A subfigure has the same artist methods as a figure and is logically the same as a figure but cannot print itself See doc ` gallery subplots axes and figures subfigures` note ``subfigure`` is new in v3 4 and the API is still provisional Parameters ---------- nrows ncols int default 1 Number of rows columns of the subfigure grid squeeze bool default True If True extra dimensions are squeezed out from the returned array of subfigures wspace hspace float default None The amount of width height reserved for space between subfigures expressed as a fraction of the average subfigure width height If not given the values will be inferred from a figure or rcParams when necessary width ratios array-like of length *ncols* optional Defines the relative widths of the columns Each column gets a relative width of ``width ratios[i] sum width ratios `` If not given all columns will have the same width height ratios array-like of length *nrows* optional Defines the relative heights of the rows Each row gets a relative height of ``height ratios[i] sum height ratios `` If not given all rows will have the same height """ gs GridSpec nrows nrows ncols ncols figure self wspace wspace hspace hspace width ratios width ratios height ratios height ratios sfarr np empty nrows ncols dtype object for i in range ncols for j in range nrows sfarr[j i] self add subfigure gs[j i] **kwargs if squeeze # Discarding unneeded dimensions that equal 1 If we only have one # subfigure just return it instead of a 1-element array return sfarr item if sfarr size 1 else sfarr squeeze else # Returned axis array will be always 2-d even if nrows ncols 1 return sfarr
def hist self x bins None range None density False weights None cumulative False bottom None histtype bar align mid orientation vertical rwidth None log False color None label None stacked False **kwargs """ Compute and plot a histogram This method uses `numpy histogram` to bin the data in *x* and count the number of values in each bin then draws the distribution either as a ` BarContainer` or ` Polygon` The *bins* *range* *density* and *weights* parameters are forwarded to `numpy histogram` If the data has already been binned and counted use `~ bar` or `~ stairs` to plot the distribution counts bins np histogram x plt stairs counts bins Alternatively plot pre-computed bins and counts using ``hist `` by treating each bin as a single point with a weight equal to its count plt hist bins[ -1] bins weights counts The data input *x* can be a singular array a list of datasets of potentially different lengths [*x0* *x1* ] or a 2D ndarray in which each column is a dataset Note that the ndarray form is transposed relative to the list form If the input is an array then the return value is a tuple *n* *bins* *patches* ; if the input is a sequence of arrays then the return value is a tuple [*n0* *n1* ] *bins* [*patches0* *patches1* ] Masked arrays are not supported Parameters ---------- x n array or sequence of n arrays Input values this takes either a single array or a sequence of arrays which are not required to be of the same length bins int or sequence or str default rc `hist bins` If *bins* is an integer it defines the number of equal-width bins in the range If *bins* is a sequence it defines the bin edges including the left edge of the first bin and the right edge of the last bin; in this case bins may be unequally spaced All but the last righthand-most bin is half-open In other words if *bins* is [1 2 3 4] then the first bin is ``[1 2 `` including 1 but excluding 2 and the second ``[2 3 `` The last bin however is ``[3 4]`` which *includes* 4 If *bins* is a string it is one of the binning strategies supported by `numpy histogram bin edges` auto fd doane scott stone rice sturges or sqrt range tuple or None default None The lower and upper range of the bins Lower and upper outliers are ignored If not provided *range* is `` x min x max `` Range has no effect if *bins* is a sequence If *bins* is a sequence or *range* is specified autoscaling is based on the specified bin range instead of the range of x density bool default False If ``True`` draw and return a probability density each bin will display the bin s raw count divided by the total number of counts *and the bin width* ``density counts sum counts * np diff bins `` so that the area under the histogram integrates to 1 ``np sum density * np diff bins 1`` If *stacked* is also ``True`` the sum of the histograms is normalized to 1 weights n array-like or None default None An array of weights of the same shape as *x* Each value in *x* only contributes its associated weight towards the bin count instead of 1 If *density* is ``True`` the weights are normalized so that the integral of the density over the range remains 1 cumulative bool or -1 default False If ``True`` then a histogram is computed where each bin gives the counts in that bin plus all bins for smaller values The last bin gives the total number of datapoints If *density* is also ``True`` then the histogram is normalized such that the last bin equals 1 If *cumulative* is a number less than 0 e g -1 the direction of accumulation is reversed In this case if *density* is also ``True`` then the histogram is normalized such that the first bin equals 1 bottom array-like scalar or None default None Location of the bottom of each bin i e bins are drawn from ``bottom`` to ``bottom + hist x bins `` If a scalar the bottom of each bin is shifted by the same amount If an array each bin is shifted independently and the length of bottom must match the number of bins If None defaults to 0 histtype { bar barstacked step stepfilled } default bar The type of histogram to draw - bar is a traditional bar-type histogram If multiple data are given the bars are arranged side by side - barstacked is a bar-type histogram where multiple data are stacked on top of each other - step generates a lineplot that is by default unfilled - stepfilled generates a lineplot that is by default filled align { left mid right } default mid The horizontal alignment of the histogram bars - left bars are centered on the left bin edges - mid bars are centered between the bin edges - right bars are centered on the right bin edges orientation { vertical horizontal } default vertical If horizontal `~ Axes barh` will be used for bar-type histograms and the *bottom* kwarg will be the left edges rwidth float or None default None The relative width of the bars as a fraction of the bin width If ``None`` automatically compute the width Ignored if *histtype* is step or stepfilled log bool default False If ``True`` the histogram axis will be set to a log scale color color or array-like of colors or None default None Color or sequence of colors one per dataset Default ``None`` uses the standard line color sequence label str or None default None String or sequence of strings to match multiple datasets Bar charts yield multiple patches per dataset but only the first gets the label so that `~ Axes legend` will work as expected stacked bool default False If ``True`` multiple data are stacked on top of each other If ``False`` multiple data are arranged side by side if histtype is bar or on top of each other if histtype is step Returns ------- n array or list of arrays The values of the histogram bins See *density* and *weights* for a description of the possible semantics If input *x* is an array then this is an array of length *nbins* If input is a sequence of arrays ``[data1 data2 ]`` then this is a list of arrays with the values of the histograms for each of the arrays in the same order The dtype of the array *n* or of its element arrays will always be float even if no weighting or normalization is used bins array The edges of the bins Length nbins + 1 nbins left edges and right edge of last bin Always a single array even when multiple data sets are passed in patches ` BarContainer` or list of a single ` Polygon` or list of \ such objects Container of individual artists used to create the histogram or list of such containers if there are multiple input datasets Other Parameters ---------------- data indexable object optional DATA PARAMETER PLACEHOLDER **kwargs `~matplotlib patches Patch` properties See Also -------- hist2d 2D histogram with rectangular bins hexbin 2D histogram with hexagonal bins stairs Plot a pre-computed histogram Notes ----- For large numbers of bins >1000 plotting can be significantly accelerated by setting *histtype* to step or stepfilled rather than bar or barstacked or by using `~ Axes stairs` to plot a pre-computed histogram ``plt stairs *np histogram data `` """ # Avoid shadowing the builtin bin range range from builtins import range if np isscalar x x [x] if bins is None bins mpl rcParams[ hist bins ] # Validate string inputs here to avoid cluttering subsequent code api check in list [ bar barstacked step stepfilled ] histtype histtype api check in list [ left mid right ] align align api check in list [ horizontal vertical ] orientation orientation if histtype barstacked and not stacked stacked True # Massage x for processing x cbook reshape 2D x x nx len x # number of datasets # Process unit information process unit info sets the unit and # converts the first dataset; then we convert each following dataset # one at a time if orientation "vertical" convert units self convert xunits x [*self process unit info [ "x" x[0] ] kwargs *map convert units x[1 ] ] else # horizontal convert units self convert yunits x [*self process unit info [ "y" x[0] ] kwargs *map convert units x[1 ] ] if bin range is not None bin range convert units bin range if not cbook is scalar or string bins bins convert units bins # We need to do to weights what was done to x if weights is not None w cbook reshape 2D weights weights else w [None] * nx if len w ! nx raise ValueError weights should have the same shape as x input empty True for xi wi in zip x w len xi len xi if wi is not None and len wi ! len xi raise ValueError weights should have the same shape as x if len xi input empty False if color is None colors [self get lines get next color for i in range nx ] else colors mcolors to rgba array color if len colors ! nx raise ValueError f"The color keyword argument must have one " f"color per dataset but {nx} datasets and " f"{len colors } colors were provided" hist kwargs dict # if the bin range is not given compute without nan numpy # does not do this for us when guessing the range but will # happily ignore nans when computing the histogram if bin range is None xmin np inf xmax -np inf for xi in x if len xi # python s min max ignore nan # np minnan returns nan for all nan input xmin min xmin np nanmin xi xmax max xmax np nanmax xi if xmin < xmax # Only happens if we have seen a finite value bin range xmin xmax # If bins are not specified either explicitly or via range # we need to figure out the range required for all datasets # and supply that to np histogram if not input empty and len x > 1 if weights is not None w np concatenate w else w None bins np histogram bin edges np concatenate x bins bin range w else hist kwargs[ range ] bin range density bool density if density and not stacked hist kwargs[ density ] density # List to store all the top coordinates of the histograms tops [] # Will have shape n datasets n bins # Loop through datasets for i in range nx # this will automatically overwrite bins # so that each histogram uses the same bins m bins np histogram x[i] bins weights w[i] **hist kwargs tops append m tops np array tops float # causes problems later if it s an int bins np array bins float # causes problems if float16 if stacked tops tops cumsum axis 0 # If a stacked density plot normalize so the area of all the # stacked histograms together is 1 if density tops tops np diff bins tops[-1] sum if cumulative slc slice None if isinstance cumulative Number and cumulative < 0 slc slice None None -1 if density tops tops * np diff bins [ slc] cumsum axis 1 [ slc] else tops tops[ slc] cumsum axis 1 [ slc] patches [] if histtype startswith bar totwidth np diff bins if rwidth is not None dr np clip rwidth 0 1 elif len tops > 1 and not stacked or mpl rcParams[ internal classic mode ] dr 0 8 else dr 1 0 if histtype bar and not stacked width dr * totwidth nx dw width boffset -0 5 * dr * totwidth * 1 - 1 nx elif histtype barstacked or stacked width dr * totwidth boffset dw 0 0 0 0 if align mid boffset + 0 5 * totwidth elif align right boffset + totwidth if orientation horizontal barfunc self barh bottom kwarg left else # orientation vertical barfunc self bar bottom kwarg bottom for top color in zip tops colors if bottom is None bottom np zeros len top if stacked height top - bottom else height top bars barfunc bins[ -1]+boffset height width align center log log color color **{bottom kwarg bottom} patches append bars if stacked bottom top boffset + dw # Remove stickies from all bars but the lowest ones as otherwise # margin expansion would be unable to cross the stickies in the # middle of the bars for bars in patches[1 ] for patch in bars patch sticky edges x[ ] patch sticky edges y[ ] [] elif histtype startswith step # these define the perimeter of the polygon x np zeros 4 * len bins - 3 y np zeros 4 * len bins - 3 x[0 2*len bins -1 2] x[1 2*len bins -1 2] bins bins[ -1] x[2*len bins -1 ] x[1 2*len bins -1][ -1] if bottom is None bottom 0 y[1 2*len bins -1 2] y[2 2*len bins 2] bottom y[2*len bins -1 ] y[1 2*len bins -1][ -1] if log if orientation horizontal self set xscale log nonpositive clip else # orientation vertical self set yscale log nonpositive clip if align left x - 0 5* bins[1]-bins[0] elif align right x + 0 5* bins[1]-bins[0] # If fill kwarg is set it will be passed to the patch collection # overriding this fill histtype stepfilled xvals yvals [] [] for top in tops if stacked # top of the previous polygon becomes the bottom y[2*len bins -1 ] y[1 2*len bins -1][ -1] # set the top of this polygon y[1 2*len bins -1 2] y[2 2*len bins 2] top + bottom # The starting point of the polygon has not yet been # updated So far only the endpoint was adjusted This # assignment closes the polygon The redundant endpoint is # later discarded for step and stepfilled y[0] y[-1] if orientation horizontal xvals append y copy yvals append x copy else xvals append x copy yvals append y copy # stepfill is closed step is not split -1 if fill else 2 * len bins # add patches in reverse order so that when stacking # items lower in the stack are plotted on top of # items higher in the stack for x y color in reversed list zip xvals yvals colors patches append self fill x[ split] y[ split] closed True if fill else None facecolor color edgecolor None if fill else color fill fill if fill else None zorder None if fill else mlines Line2D zorder for patch list in patches for patch in patch list if orientation vertical patch sticky edges y append 0 elif orientation horizontal patch sticky edges x append 0 # we return patches so put it back in the expected order patches reverse # If None make all labels None via zip longest below ; otherwise # cast each element to str but keep a single str as it labels [] if label is None else np atleast 1d np asarray label str for patch lbl in itertools zip longest patches labels if patch p patch[0] p internal update kwargs if lbl is not None p set label lbl for p in patch[1 ] p internal update kwargs p set label nolegend if nx 1 return tops[0] bins patches[0] else patch type "BarContainer" if histtype startswith "bar" else "list[Polygon]" return tops bins cbook silent list patch type patches
def cmd expr args r""" Helper to define TeX commands ``cmd "\cmd" args `` is equivalent to ``"\cmd" - args | Error "Expected \cmd{arg}{ }" `` where the names in the error message are taken from element names in *args* If *expr* already includes arguments e g "\cmd{arg}{ }" then they are stripped when constructing the parse element but kept and *expr* is used as is in the error message """ def names elt if isinstance elt ParseExpression for expr in elt exprs yield from names expr elif elt resultsName yield elt resultsName csname expr split "{" 1 [0] err csname + "" join "{%s}" % name for name in names args if expr csname else expr print csname args err return csname - args | Error f"Expected {err}"
def init self shorthand name """ Parameters ---------- shorthand name str A string representing the "name" of the transform The name carries no significance other than to improve the readability of ``str transform `` when DEBUG True """ self parents {} # Initially invalid until first computation self invalid self INVALID FULL self shorthand name shorthand name
def set ticks position self position """ Set the ticks position Parameters ---------- str { lower upper both default none } The position of the bolded axis lines ticks and tick labels """ api check in list [ lower upper both default none ] position position self tick position position
def test pil kwargs webp Image init if "WEBP" not in Image SAVE pytest skip "No WEBP support" plt plot [0 1 2] [0 1 0] buf small io BytesIO pil kwargs low {"quality" 1} plt savefig buf small format "webp" pil kwargs pil kwargs low assert len pil kwargs low 1 buf large io BytesIO pil kwargs high {"quality" 100} plt savefig buf large format "webp" pil kwargs pil kwargs high assert len pil kwargs high 1 assert buf large getbuffer nbytes > buf small getbuffer nbytes
def add gridspec self nrows 1 ncols 1 **kwargs """ Create a low-level ` GridSpec` that has this figure as a parent Parameters ---------- nrows int default 1 Number of rows in grid ncols int default 1 Number of columns in grid Returns ------- ` GridSpec` Other Parameters ---------------- **kwargs Keyword arguments are passed to ` GridSpec` See Also -------- matplotlib pyplot subplots Examples -------- Adding a subplot that spans two rows fig plt figure gs fig add gridspec 2 2 ax1 fig add subplot gs[0 0] ax2 fig add subplot gs[1 0] # spans two rows ax3 fig add subplot gs[ 1] """ kwargs pop figure None # pop in case user has added this gs GridSpec nrows nrows ncols ncols figure self **kwargs return gs
def add gridspec self nrows 1 ncols 1 **kwargs """ Create a low-level ` GridSpec` that has this figure as a parent Parameters ---------- nrows int default 1 Number of rows in grid ncols int default 1 Number of columns in grid Returns ------- ` GridSpec` Other Parameters ---------------- **kwargs Keyword arguments are passed to ` GridSpec` See Also -------- matplotlib pyplot subplots Examples -------- Adding a subplot that spans two rows fig plt figure gs fig add gridspec 2 2 ax1 fig add subplot gs[0 0] ax2 fig add subplot gs[1 0] # spans two rows ax3 fig add subplot gs[ 1] """ kwargs pop figure None # pop in case user has added this gs GridSpec nrows nrows ncols ncols figure self **kwargs return gs
def test locale comma proc mpl testing subprocess run helper impl locale comma timeout 60 extra env { MPLBACKEND Agg } skip msg next line[len SKIP ] strip for line in proc stdout splitlines if line startswith SKIP if skip msg pytest skip skip msg
def get pos and bbox ax renderer """ Get the position and the bbox for the axes Parameters ---------- ax Axes renderer Renderer Returns ------- pos Bbox Position in figure coordinates bbox Bbox Tight bounding box in figure coordinates """ fig ax figure pos ax get position original True # pos is in panel co-ords but we need in figure for the layout pos pos transformed fig transSubfigure - fig transFigure tightbbox martist get tightbbox for layout only ax renderer if tightbbox is None bbox pos else bbox tightbbox transformed fig transFigure inverted return pos bbox
def set array self A # docstring inherited prev unmask self get unmasked polys # MPL <3 8 compressed the mask so we need to handle flattened 1d input # until the deprecation expires also only warning when there are masked # elements and thus compression occurring if self deprecated compression and np ndim A 1 api warn deprecated "3 8" message "Setting a PolyQuadMesh array using " "the compressed values is deprecated " "Pass the full 2D shape of the original array " f"{prev unmask shape} including the masked elements " Afull np empty self original mask shape Afull[~self original mask] A # We also want to update the mask with any potential # new masked elements that came in But we don t want # to update any of the compression from the original mask self original mask copy mask[~self original mask] | np ma getmask A A np ma array Afull mask mask return super set array A self deprecated compression False super set array A # If the mask has changed at all we need to update # the set of Polys that we are drawing if not np array equal prev unmask self get unmasked polys self set unmasked verts
def set ticklabels self labels * minor False fontdict None **kwargs r""" [*Discouraged*] Set this Axis tick labels with list of string labels admonition Discouraged The use of this method is discouraged because of the dependency on tick positions In most cases you ll want to use ``Axes set [x y z]ticks positions labels `` or ``Axis set ticks`` instead If you are using this method you should always fix the tick positions before e g by using ` Axis set ticks` or by explicitly setting a `~ ticker FixedLocator` Otherwise ticks are free to move and the labels may end up in unexpected positions Parameters ---------- labels sequence of str or of ` Text`\s Texts for labeling each tick location in the sequence set by ` Axis set ticks`; the number of labels must match the number of locations The labels are used as is without further formatting minor bool If True set minor ticks instead of major ticks fontdict dict optional admonition Discouraged The use of *fontdict* is discouraged Parameters should be passed as individual keyword arguments or using dictionary-unpacking ``set ticklabels **fontdict `` A dictionary controlling the appearance of the ticklabels The default *fontdict* is { fontsize rcParams[ axes titlesize ] fontweight rcParams[ axes titleweight ] verticalalignment baseline horizontalalignment loc} **kwargs Text properties warning This only sets the properties of the current ticks Ticks are not guaranteed to be persistent Various operations can create delete and modify the Tick instances There is an imminent risk that these settings can get lost if you work on the figure further including also panning zooming on a displayed figure Use ` set tick params` instead if possible Returns ------- list of ` Text`\s For each tick includes ``tick label1`` if it is visible then ``tick label2`` if it is visible in that order """ try labels [t get text if hasattr t get text else t for t in labels] except TypeError raise TypeError f"{labels } must be a sequence" from None locator self get minor locator if minor else self get major locator if not labels # eg labels [] formatter mticker NullFormatter elif isinstance locator mticker FixedLocator # Passing [] as a list of labels is often used as a way to # remove all tick labels so only error for > 0 labels if len locator locs ! len labels and len labels ! 0 raise ValueError "The number of FixedLocator locations" f" {len locator locs } usually from a call to" " set ticks does not match" f" the number of labels {len labels } " tickd {loc lab for loc lab in zip locator locs labels } func functools partial self format with dict tickd formatter mticker FuncFormatter func else api warn external "set ticklabels should only be used with a fixed number of " "ticks i e after set ticks or using a FixedLocator " formatter mticker FixedFormatter labels with warnings catch warnings warnings filterwarnings "ignore" message "FixedFormatter should only be used together with FixedLocator" if minor self set minor formatter formatter locs self get minorticklocs ticks self get minor ticks len locs else self set major formatter formatter locs self get majorticklocs ticks self get major ticks len locs ret [] if fontdict is not None kwargs update fontdict for pos loc tick in enumerate zip locs ticks tick update position loc tick label formatter loc pos # deal with label1 tick label1 set text tick label tick label1 internal update kwargs # deal with label2 tick label2 set text tick label tick label2 internal update kwargs # only return visible tick labels if tick label1 get visible ret append tick label1 if tick label2 get visible ret append tick label2 self stale True return ret
void FT2Font load char long charcode FT Int32 flags FT2Font *&ft object bool fallback false { if this is parent FT2Font cache will be filled in 2 ways 1 set text was previously called 2 set text was not called and fallback was enabled if fallback && char to font find charcode ! char to font end { ft object char to font[charcode]; since it will be assigned to ft object anyway FT2Font *throwaway NULL; ft object->load char charcode flags throwaway false ; } else if fallback { FT UInt final glyph index; FT Error charcode error glyph error; FT2Font *ft object with glyph this; bool was found load char with fallback ft object with glyph final glyph index glyphs char to font glyph to font charcode flags charcode error glyph error true ; if !was found { if charcode error { throw ft error "Could not load charcode" charcode error ; } else if glyph error { throw ft error "Could not load charcode" glyph error ; } } ft object ft object with glyph; } else { ft object this; FT UInt glyph index FT Get Char Index face FT ULong charcode ; if FT Error error FT Load Glyph face glyph index flags { throw ft error "Could not load charcode" error ; } FT Glyph thisGlyph; if FT Error error FT Get Glyph face->glyph &thisGlyph { throw ft error "Could not get glyph" error ; } glyphs push back thisGlyph ; } }
def test find nearest contour no filled xy np indices 15 15 img np exp -np pi * np sum xy - 5 **2 0 5 **2 cs plt contourf img 10 with pytest warns mpl api MatplotlibDeprecationWarning \ pytest raises ValueError match "Method does not support filled contours" cs find nearest contour 1 1 pixel False with pytest warns mpl api MatplotlibDeprecationWarning \ pytest raises ValueError match "Method does not support filled contours" cs find nearest contour 1 10 indices 5 7 pixel False with pytest warns mpl api MatplotlibDeprecationWarning \ pytest raises ValueError match "Method does not support filled contours" cs find nearest contour 2 5 indices 2 7 pixel True
def get macos fonts """Cache and list the font paths known to ``system profiler SPFontsDataType`` """ d plistlib loads subprocess check output ["system profiler" "-xml" "SPFontsDataType"] return [Path entry["path"] for entry in d[" items"]]
def remove self """ Remove this colorbar from the figure If the colorbar was created with ``use gridspec True`` the previous gridspec is restored """ if hasattr self ax colorbar info parents self ax colorbar info[ parents ] for a in parents if self ax in a colorbars a colorbars remove self ax self ax remove self mappable callbacks disconnect self mappable colorbar cid self mappable colorbar None self mappable colorbar cid None # Remove the extension callbacks self ax callbacks disconnect self extend cid1 self ax callbacks disconnect self extend cid2 try ax self mappable axes except AttributeError return try subplotspec self ax get subplotspec get gridspec subplot spec except AttributeError # use gridspec was False pos ax get position original True ax set position pos else # use gridspec was True ax set subplotspec subplotspec
def direction self """Direction of the span selector vertical or horizontal Writable """ return self direction
def direction self """Direction of the span selector vertical or horizontal Writable """ return self direction
def kwarg doc text """ Decorator for defining the documentation used when artist properties are passed through as keyword arguments See e g the ``**kwargs`` section in ` Axes text` The given text is stored in a privat variable `` kwarg doc`` on the method It is used for generating the kwdoc list for artists which we auto-generate through docstring interpolation e g via ``% Line2D kwdoc s`` The text should contain the supported types as well as the default value if applicable e g @ docstring kwarg doc "bool default rc `text usetex`" def set usetex self usetex """ def decorator func func kwarg doc text return func return decorator
def test subplotspec args fig axs plt subplots 1 2 # should work gs gridspec GridSpecFromSubplotSpec 2 1 subplot spec axs[0] get subplotspec assert gs get topmost subplotspec axs[0] get subplotspec # this is a mistake and not what the type hints give but we allow with pytest raises TypeError match "subplot spec must be type SubplotSpec" gs gridspec GridSpecFromSubplotSpec 2 1 subplot spec axs[0] with pytest raises TypeError match "subplot spec must be type SubplotSpec" gs gridspec GridSpecFromSubplotSpec 2 1 subplot spec axs
void initONNXBindings PyObject* module { auto m py handle module cast<py module> ; ONNX specific passes m def " jit pass onnx remove print" RemovePrintOps def " jit pass onnx preprocess caffe2" PreprocessCaffe2Ops def " jit pass onnx" ToONNX def " jit pass onnx assign output shape" torch wrap pybind function [] std shared ptr<Graph>& graph const std vector<at Tensor>& tensors const python IODescriptor& desc bool onnx shape inference bool is script { ONNXAssignOutputShape graph tensors desc onnx shape inference is script ; } def " jit pass onnx function substitution" wrap pybind function ONNXFunctionCallSubstitution def " jit pass onnx autograd function process" wrap pybind function ONNXAutogradFunctionProcess def " jit pass onnx peephole" torch wrap pybind function [] std shared ptr<Graph>& graph int opset version bool fixed batch size { return PeepholeOptimizeONNX graph opset version fixed batch size ; } def " jit pass onnx preprocess" torch wrap pybind function PreprocessForONNX def " jit pass onnx eval peephole" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& paramsDict { EvalPeepholeONNX graph paramsDict ; return paramsDict; } pybind11 return value policy move def " jit pass onnx cast all constant to floating" torch wrap pybind function CastAllConstantToFloating def " jit pass onnx constant fold" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& paramsDict int opset version { ConstantFoldONNX graph paramsDict opset version ; overload resolution return paramsDict; } pybind11 return value policy move def " jit pass onnx eliminate unused items" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& paramsDict { EliminateUnusedItemsONNX graph->block paramsDict ; overload resolution return paramsDict; } pybind11 return value policy move def " jit pass onnx scalar type analysis" torch wrap pybind function [] std shared ptr<Graph>& graph bool lowprecision cast int opset version { return ScalarTypeAnalysisForONNX graph lowprecision cast opset version ; } py arg "graph" py arg "lowprecision cast" true py arg "opset version" def " jit pass onnx remove inplace ops for onnx" torch wrap pybind function RemoveInplaceOpsForONNX def " jit pass onnx node shape type inference" torch wrap pybind function [] Node* n std map<std string IValue>& params dict int opset version { ONNXShapeTypeInference n params dict opset version ; } def " jit pass onnx graph shape type inference" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& params dict int opset version { ONNXShapeTypeInference graph params dict opset version ; } py arg "graph" py arg "params dict" true py arg "opset version" def " jit pass onnx set dynamic input shape" torch wrap pybind function ONNXSetDynamicInputShape def " jit pass onnx lint" torch wrap pybind function ONNXLintGraph def " jit pass onnx function extraction" torch wrap pybind function torch jit onnx ONNXFunctionExtraction def " jit pass onnx block" torch wrap pybind function BlockToONNX def " jit pass onnx unpack quantized weights" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& paramsDict bool caffe2 { UnpackQuantizedWeights graph paramsDict caffe2 ; return paramsDict; } pybind11 return value policy move def " jit pass onnx quantization insert permutes" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue>& paramsDict { insertPermutes graph paramsDict ; return paramsDict; } pybind11 return value policy move def " jit onnx list model parameters" torch wrap pybind function [] Module& module { return list module parameters module ; } def " jit pass prepare division for onnx" torch wrap pybind function PrepareDivisionForONNX def " jit onnx convert pattern from subblock" torch wrap pybind function ConvertPatternFromSubblock def " jit pass fixup onnx controlflow node" torch wrap pybind function FixupONNXControlflowNode def " jit pass onnx deduplicate initializers" torch wrap pybind function [] std shared ptr<Graph>& graph std map<std string IValue> params dict bool is train { DeduplicateInitializers graph params dict is train ; return params dict; } pybind11 return value policy move def " jit pass onnx clear scope records" &torch jit onnx ONNXClearScopeRecords def " jit pass onnx track scope attributes" &torch jit onnx ONNXTrackScopeAttributes def " jit is onnx log enabled" torch jit onnx is log enabled "Returns whether ONNX logging is enabled or disabled " def " jit set onnx log enabled" torch jit onnx set log enabled "Enables or disables ONNX logging " def " jit set onnx log output stream" [] std string stream name "stdout" -> void { std shared ptr<std ostream> out; if stream name "stdout" { out std shared ptr<std ostream> &std cout [] std ostream* {} ; } else if stream name "stderr" { out std shared ptr<std ostream> &std cerr [] std ostream* {} ; } else { std cerr << "ERROR only `stdout` and `stderr`" << "are supported as `stream name`" << std endl; } torch jit onnx set log output stream out ; } "Set specific file stream for ONNX logging " def " jit onnx log" [] py args args -> void { if torch jit onnx is log enabled { auto& out torch jit onnx get log output stream ; for auto arg args { out << c10 str arg ; } out << std endl; } } "Write `args` to the previously specified ONNX log stream " def " jit pass onnx assign scoped names for node and value" torch wrap pybind function torch jit onnx AssignScopedNamesForNodeAndValue "Assign informative scoped names for nodes and values " def " jit onnx create full scope name" torch wrap pybind function torch jit onnx ONNXScopeName createFullScopeName "Create a full scope name from class name and variable name " ; m def " check onnx proto" [] const std string& proto string bool full check { check onnx proto proto string full check ; } py arg "proto string" py arg "full check" false ; auto onnx m def submodule " onnx" ; py enum < ONNX NAMESPACE TensorProto DataType> onnx "TensorProtoDataType" value "UNDEFINED" ONNX NAMESPACE TensorProto DataType UNDEFINED value "FLOAT" ONNX NAMESPACE TensorProto DataType FLOAT value "UINT8" ONNX NAMESPACE TensorProto DataType UINT8 value "INT8" ONNX NAMESPACE TensorProto DataType INT8 value "UINT16" ONNX NAMESPACE TensorProto DataType UINT16 value "INT16" ONNX NAMESPACE TensorProto DataType INT16 value "INT32" ONNX NAMESPACE TensorProto DataType INT32 value "INT64" ONNX NAMESPACE TensorProto DataType INT64 value "STRING" ONNX NAMESPACE TensorProto DataType STRING value "BOOL" ONNX NAMESPACE TensorProto DataType BOOL value "FLOAT16" ONNX NAMESPACE TensorProto DataType FLOAT16 value "DOUBLE" ONNX NAMESPACE TensorProto DataType DOUBLE value "UINT32" ONNX NAMESPACE TensorProto DataType UINT32 value "UINT64" ONNX NAMESPACE TensorProto DataType UINT64 value "COMPLEX64" ONNX NAMESPACE TensorProto DataType COMPLEX64 value "COMPLEX128" ONNX NAMESPACE TensorProto DataType COMPLEX128 value "BFLOAT16" ONNX NAMESPACE TensorProto DataType BFLOAT16 ; py enum <OperatorExportTypes> onnx "OperatorExportTypes" value "ONNX" OperatorExportTypes ONNX value "ONNX ATEN" OperatorExportTypes ONNX ATEN value "ONNX ATEN FALLBACK" OperatorExportTypes ONNX ATEN FALLBACK value "ONNX FALLTHROUGH" OperatorExportTypes ONNX FALLTHROUGH ; py enum <TrainingMode> onnx "TrainingMode" value "EVAL" TrainingMode EVAL value "PRESERVE" TrainingMode PRESERVE value "TRAINING" TrainingMode TRAINING ; onnx attr "PRODUCER VERSION" py str TORCH VERSION ; #ifdef BUILD CAFFE2 onnx attr " CAFFE2 ATEN FALLBACK" true; #else onnx attr " CAFFE2 ATEN FALLBACK" false; #endif }
def export without kwargs fn Union[torch nn Module Callable] opset version *args use binary format bool True **kwargs if isinstance fn torch nn Module signature inspect signature fn forward else signature inspect signature fn # We hope the input kwargs will be mapped to bound args after binding # If not we will raise an error bound signature bind *args **kwargs bound apply defaults # kwargs are not handled assert not bound kwargs class Wrapper torch nn Module def init self fn super init self fn fn def forward self *args result pytree tree flatten self fn *args return result # args will be converted to symbolic tensor Let s copy to avoid side effects bound args copy deepcopy bound args # Translate callable to FX graph # # TODO wechi There are several symbolic tracing mechanisms to convert # nn Module to FX graph We should choose the right one after they are # matured class GraphCaptureCompiler def init self self captured graph Optional[torch fx GraphModule] None self captured graph count 0 def compile self gm torch fx GraphModule assert self captured graph count 0 self captured graph gm self captured graph count + 1 return gm compiler GraphCaptureCompiler torch dynamo optimize compiler compile nopython True Wrapper fn *bound args torch dynamo reset assert compiler captured graph # Export FX graph to ONNX ModelProto return export compiler captured graph opset version # Function optimized by dynamo doesn t have None in args *tuple arg for arg in bound args if arg is not None decomposition table ONNX FRIENDLY DECOMPOSITION TABLE use binary format use binary format
def save model with external data basepath str model location str initializer location str torch load paths Tuple[str ] onnx model "onnx ModelProto" -> None """Load PyTorch tensors from files and add to "onnx model" as external initializers Output files ONNX model file path ONNX initializer folder os path join basepath initializer location After running this function you can do ort sess onnxruntime InferenceSession os path join basepath model location to execute the model Arguments basepath Base path of the external data file e g " tmp large-onnx-model" model location Relative location of the ONNX model file E g "model onnx" so that the model file is saved to " tmp large-onnx-model model onnx" initializer location Relative location of the ONNX initializer folder E g "initializers" so that the initializers are saved to " tmp large-onnx-model initializers" torch load paths Files which containing serialized PyTorch tensors to be saved as ONNX initializers They are loaded by torch load onnx model ONNX model to be saved with external initializers If an input name matches a tensor loaded from "torch load paths" the tensor will be saved as that input s external initializer """ onnx model with initializers onnx ModelProto onnx model with initializers CopyFrom onnx model onnx input names [input name for input in onnx model graph input] for path in torch load paths state ditc torch load path for name tensor in state ditc items # Basically "transformer attention self query weight" is mapped # to "transformer attention self query weight" for mimicking the # name-modifying code in FX-to-ONNX exporter # See function replace get attr with placeholder for details refined name name replace " " " " # For each refined PyTorch tensor name loaded by torch load # 1 Search its best match in ONNX model E g the match of # "transformer attention weight" could be "attention weight" # 2 Set "tensor" as the initializer of the matched ONNX input # E g "tensor" is stored as the initializer of "attention weight" # Step 1 is required because sometimes tensor names are stored with prefix the dictionary # loaded by torch load for onnx input name in onnx input names if onnx input name endswith refined name or refined name endswith onnx input name # Find a match Change refined name to the matched ONNX input name so that we # create initializer with the right ONNX name refined name onnx input name break relative tensor file path os path join initializer location refined name # Create one file per tensor # tensor proto raw data is stored to external file at # os path join basepath relative tensor file path tensor proto create tensor proto with external data tensor refined name relative tensor file path basepath # Add the tensor proto to the ONNX model as an initializer with external data onnx model with initializers graph initializer append tensor proto # model location should be a pure file name such as "file name onnx" not "folder file name onnx" onnx save onnx model with initializers os path join basepath model location
def make buffer allocation self buffer from cpp import DTYPE TO ATEN output idx None for idx output in enumerate V graph graph outputs if isinstance output ir NoneAsConstantBuffer ir ShapeAsConstantBuffer continue if buffer output data output idx idx break if output idx is not None and V graph aot mode # In aot mode output buffers are managed by the AOT runtime return f"at Tensor {buffer get name } outputs[{output idx}]{self ending}" else # TODO map layout here device buffer get device dtype buffer get dtype shape tuple buffer get size stride tuple buffer get stride return f"{self declare}{buffer get name } {self namespace}empty strided " f"{self codegen shape tuple shape } " f"{self codegen shape tuple stride } " f"{self codegen device device }" f" dtype {DTYPE TO ATEN[dtype]} {self ending}"
def call method self tx name args "List[VariableTracker]" kwargs "Dict[str VariableTracker]" -> "VariableTracker" if tx strict checks enabled if name in self strict mode banned ops unimplemented f"Illegal method invocation {name} in strict mode" from import ConstantVariable TorchVariable TupleVariable from builder import wrap fx proxy kwargs dict kwargs options VariableTracker propagate self args kwargs values if name in "stride" "size" dim var None if len args 1 dim var args[0] elif "dim" in kwargs dim var kwargs["dim"] else assert not args and not kwargs f"Tensor {name} unhandled args kwargs" dim guard if dyn dim var def make const size variable x **options return SizeVariable [ConstantVariable y **options for y in x] **options RetVariable make const size variable if name "size" else ConstantVariable # Technically this should not be necessary but I m including it # for enhanced BC in case example value is sometimes not set # it really should always be set though! if r getattr self name is not None if dim is None return RetVariable r **options else return ConstantVariable r[dim] **options # It might still be constant! Consult the fake tensor and see if fake self proxy node meta get "example value" is not None if dim is None fake r getattr fake name if not free symbols fake r # int conversion for safety in case a SymInt refined # to constant return RetVariable tuple int r for r in fake r **options else fake r getattr fake name dim if not free symbols fake r return ConstantVariable int fake r **options # Oops it s not constant Do the dynamic shapes path return wrap fx proxy tx tx output create proxy "call method" name *proxy args kwargs [self] + list args kwargs **options elif name in "numel" "nelement" if self size is not None return ConstantVariable product self size **options # It might still be constant! Consult the fake tensor and see if fake self proxy node meta get "example value" is not None fake r fake numel if not free symbols fake r return ConstantVariable int fake r **options assert not kwargs f"Tensor {name} unhandled kwargs" # Oops it s not constant Do the dynamic shapes path return wrap fx proxy tx tx output create proxy "call method" "numel" *proxy args kwargs [self] + list args kwargs **options elif name in "ndimension" "dim" and self ndim is not None constant result ConstantVariable self ndim **options elif name "is floating point" and self dtype is not None constant result ConstantVariable self dtype is floating point **options elif name "is contiguous" and self is contiguous is not None if "memory format" in kwargs memory format kwargs pop "memory format" as python constant else memory format torch contiguous format constant result ConstantVariable memory format in self is contiguous **options elif name "type" and self dtype is not None and len args 0 and isinstance self device torch device tensortype [k for k v in tensortype to dtype items if self dtype in v][ 0 ] if self device type "cuda" constant result ConstantVariable f"torch cuda {tensortype name }" **options else constant result ConstantVariable f"torch {tensortype name }" **options elif name "type" and len args 1 and fqn type args[0] as python constant "torch tensortype" # torch FloatTensor etc are all of type "torch tensortype" # torch fx s tracer fails on these types because it doesn t support arguments of torch tensortype type # So we pass it in as a string which is also supported see above implementation for type with 0 args tensor type args[0] as python constant tensor type const ConstantVariable fqn tensor type **options return wrap fx proxy tx tx output create proxy "call method" name *proxy args kwargs [self tensor type const] kwargs **options elif name "get device" and isinstance self device torch device index self device index if self device type ! "cpu" else -1 constant result ConstantVariable index **options else constant result None if constant result assert not kwargs f"Tensor {name} unhandled kwargs" # TODO I think this branch is dead if len args 1 return constant result getitem const args[0] elif args return TupleVariable [constant result getitem const a for a in args] **options return constant result elif name "numpy" assert not args "Tensor numpy doesn t take args " # TODO support force if kwargs and "force" in kwargs unimplemented f"Tensor numpy force {kwargs[ force ]} " proxy tx output create proxy "call function" torch detach *proxy args kwargs [self] {} return NumpyNdarrayVariable create tx proxy **options elif name "tolist" from builder import SourcelessBuilder def tolist tensor sub proxy def wrap i sub proxy return SymNodeVariable create tx sub proxy item sym num tx output shape env create unbacked symint if tensor dtype not in [ torch int8 torch int16 torch int32 torch int64 ] unimplemented "Input tensor for tolist must be an integer tensor" if tensor dim 0 return wrap tensor sub proxy if tensor dim 1 return [wrap val sub proxy[i] for i val in enumerate tensor ] return [ tolist sub tensor sub proxy sub proxy[i] for i sub tensor in enumerate tensor ] tensor self as proxy node meta["example value"] out tolist tensor self as proxy return SourcelessBuilder tx out add options options elif name in "backward" "data ptr" unimplemented f"Tensor {name}" elif name "item" and not config capture scalar outputs unimplemented f"Tensor {name}" elif name " len " return self call method tx "size" [ConstantVariable 0 **options ] {} elif name " setitem " key value args def has bool key v if isinstance v TensorVariable return v dtype in torch bool torch int8 elif isinstance v TupleVariable return any has bool key item for item in v items else return False if not config capture dynamic output shape ops and has bool key key and isinstance value TensorVariable and value requires grad unimplemented "boolean masking setitem backwards requires dynamic shapes" tx output guards update options["guards"] tx output create proxy "call function" operator setitem *proxy args kwargs [self] + list args kwargs return ConstantVariable None **options elif name in "resize " "resize as " if "memory format" in kwargs memory format kwargs["memory format"] as python constant else memory format torch contiguous format if name "resize " self size args[0] as python constant self is contiguous memory format else assert isinstance args[0] TensorVariable if self size and args[0] size if self size args[0] size or memory format is torch preserve format self is contiguous args[0] is contiguous else self size args[0] size self stride args[0] stride self ndim args[0] ndim self is contiguous memory format return wrap fx proxy tx tx output create proxy "call method" name *proxy args kwargs [self] + list args kwargs **options elif name "add " and len args 1 and len kwargs 1 and "alpha" in kwargs result TorchVariable torch mul **options call function tx args + [kwargs["alpha"]] {} return self call method tx "add " [result] {} elif name "addcdiv " and len args 2 and len kwargs 1 and "value" in kwargs result TorchVariable torch div **options call function tx args {} result TorchVariable torch mul **options call function tx [result kwargs["value"]] {} return self call method tx "add " [result] {} elif name " contains " # Rewrite contains here so that downstream passes can trace through # without dealing with unbacked symbool Roughly the code we translate is # def contains self x # return x self any item result TorchVariable torch eq **options call function tx [self args[0]] {} result TorchVariable torch any **options call function tx [result] {} return result call method tx "item" [] {} elif name "redistribute" # rewrite non-primitive args kwargs to be included in the on-the-fly prim function # and rewrite args to have only proxyable args then insert call function args as value [x as python constant for x in args] def redistribute fn with prim types x return x redistribute *args as value # attach the same function name for better debugging redistribute fn with prim types name f"prim {name}" return wrap fx proxy tx tx proxy tx output create proxy "call function" redistribute fn with prim types *proxy args kwargs [self] {} **options else # Convert x new torch Size into x new empty torch Size # as Tensor new acts differently with a Size input versus a tuple input if name "new" and len args 1 and isinstance args[0] SizeVariable ShapeVariable name "new empty" return wrap fx proxy tx tx output create proxy "call method" name *proxy args kwargs [self] + list args kwargs **options
def numpy to tensor value """Convert tnp ndarray to tensor leave other types intact If a list tuple loop through it to convert """ if isinstance value np ndarray return numpy to tensor tnp array value dtype str value dtype if isinstance value tnp ndarray return value tensor elif isinstance value tuple list return type value numpy to tensor obj for obj in value else return value
def add graph output self value graph outputs key id value as proxy if graph outputs key not in self graph outputs self graph outputs[graph outputs key] GraphOutputEntry len self graph outputs value else self graph outputs[graph outputs key] merge value return graph outputs key
def numpy to tensor value """Convert tnp ndarray to tensor leave other types intact If a list tuple loop through it to convert """ if isinstance value np ndarray return torch as tensor value if isinstance value tnp ndarray return value tensor elif isinstance value tuple list return type value numpy to tensor obj for obj in value else return value
def call function self tx args "List[VariableTracker]" kwargs "Dict[str VariableTracker]" -> "VariableTracker" from utils import numpy to tensor wrapper from tensor import NumpyNdarrayVariable options VariableTracker propagate [[self]] [args] [list kwargs values ] # lookup method name in tnp Things like np dtype float are not supported yet if self value name "dtype" unimplemented f"numpy dtype function is not supported yet Got type {type self value } " else # We are dealing with a callable func get np to tnp map get self value if func is None unimplemented f"Can t find numpy function {self value} in torch numpy " " Please file an issue to request support for this function " # TODO larryliu0820 currently assuming all numpy * functions are returning a ndarray that can be # wrapped by NumpyNdarrayVariable which is wrong! proxy tx output create proxy "call function" numpy to tensor wrapper func *proxy args kwargs args kwargs return NumpyNdarrayVariable create tx proxy **options
BuiltinOpResolver BuiltinOpResolver { AddBuiltin BuiltinOperator ABS Register ABS * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator HARD SWISH Register HARD SWISH ; AddBuiltin BuiltinOperator RELU Register RELU * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator RELU N1 TO 1 Register RELU N1 TO 1 ; AddBuiltin BuiltinOperator RELU 0 TO 1 Register RELU 0 TO 1 ; AddBuiltin BuiltinOperator RELU6 Register RELU6 * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator TANH Register TANH * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator LOGISTIC Register LOGISTIC * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator AVERAGE POOL 2D Register AVERAGE POOL 2D * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator MAX POOL 2D Register MAX POOL 2D * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator L2 POOL 2D Register L2 POOL 2D ; AddBuiltin BuiltinOperator CONV 2D Register CONV 2D * min version * 1 * max version * 8 ; AddBuiltin BuiltinOperator DEPTHWISE CONV 2D Register DEPTHWISE CONV 2D * min version * 1 * max version * 7 ; AddBuiltin BuiltinOperator SVDF Register SVDF * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RNN Register RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator BIDIRECTIONAL SEQUENCE RNN Register BIDIRECTIONAL SEQUENCE RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator UNIDIRECTIONAL SEQUENCE RNN Register UNIDIRECTIONAL SEQUENCE RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EMBEDDING LOOKUP Register EMBEDDING LOOKUP * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EMBEDDING LOOKUP SPARSE Register EMBEDDING LOOKUP SPARSE ; AddBuiltin BuiltinOperator FULLY CONNECTED Register FULLY CONNECTED * min version * 1 * max version * 11 ; AddBuiltin BuiltinOperator LSH PROJECTION Register LSH PROJECTION ; AddBuiltin BuiltinOperator HASHTABLE LOOKUP Register HASHTABLE LOOKUP ; AddBuiltin BuiltinOperator SOFTMAX Register SOFTMAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator CONCATENATION Register CONCATENATION * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator ADD Register ADD * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator SPACE TO BATCH ND Register SPACE TO BATCH ND * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator BATCH TO SPACE ND Register BATCH TO SPACE ND * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator MUL Register MUL * min version * 1 * max version * 7 ; AddBuiltin BuiltinOperator L2 NORMALIZATION Register L2 NORMALIZATION * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LOCAL RESPONSE NORMALIZATION Register LOCAL RESPONSE NORMALIZATION ; AddBuiltin BuiltinOperator LSTM Register LSTM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator BIDIRECTIONAL SEQUENCE LSTM Register BIDIRECTIONAL SEQUENCE LSTM * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator UNIDIRECTIONAL SEQUENCE LSTM Register UNIDIRECTIONAL SEQUENCE LSTM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator PAD Register PAD * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator PADV2 Register PADV2 * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RESHAPE Register RESHAPE ; AddBuiltin BuiltinOperator RESIZE BILINEAR Register RESIZE BILINEAR * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RESIZE NEAREST NEIGHBOR Register RESIZE NEAREST NEIGHBOR * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SKIP GRAM Register SKIP GRAM ; AddBuiltin BuiltinOperator SPACE TO DEPTH Register SPACE TO DEPTH * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator DEPTH TO SPACE Register DEPTH TO SPACE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator GATHER Register GATHER * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator TRANSPOSE Register TRANSPOSE * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator MEAN Register MEAN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator DIV Register DIV * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SUB Register SUB * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator SPLIT Register SPLIT * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SPLIT V Register SPLIT V * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SQUEEZE Register SQUEEZE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator STRIDED SLICE Register STRIDED SLICE * min version * 1 * max version * 8 ; AddBuiltin BuiltinOperator EXP Register EXP * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator TOPK V2 Register TOPK V2 * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator LOG Register LOG * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LOG SOFTMAX Register LOG SOFTMAX * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator CAST Register CAST * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator DEQUANTIZE Register DEQUANTIZE * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator PRELU Register PRELU ; AddBuiltin BuiltinOperator MAXIMUM Register MAXIMUM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator MINIMUM Register MINIMUM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator ARG MAX Register ARG MAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ARG MIN Register ARG MIN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator GREATER Register GREATER * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator GREATER EQUAL Register GREATER EQUAL * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator LESS Register LESS * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator LESS EQUAL Register LESS EQUAL * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator FLOOR Register FLOOR ; AddBuiltin BuiltinOperator CEIL Register CEIL ; AddBuiltin BuiltinOperator ROUND Register ROUND ; AddBuiltin BuiltinOperator NEG Register NEG ; AddBuiltin BuiltinOperator SELECT Register SELECT * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SELECT V2 Register SELECT V2 * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SLICE Register SLICE * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator SIN Register SIN ; AddBuiltin BuiltinOperator COS Register COS ; AddBuiltin BuiltinOperator TRANSPOSE CONV Register TRANSPOSE CONV * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator TILE Register TILE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SUM Register SUM * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator REDUCE PROD Register REDUCE PROD * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator REDUCE MAX Register REDUCE MAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator REDUCE MIN Register REDUCE MIN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator REDUCE ANY Register REDUCE ANY ; AddBuiltin BuiltinOperator REDUCE ALL Register REDUCE ALL ; AddBuiltin BuiltinOperator EXPAND DIMS Register EXPAND DIMS ; AddBuiltin BuiltinOperator SPARSE TO DENSE Register SPARSE TO DENSE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EQUAL Register EQUAL * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator NOT EQUAL Register NOT EQUAL * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SQRT Register SQRT ; AddBuiltin BuiltinOperator RSQRT Register RSQRT * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SHAPE Register SHAPE ; AddBuiltin BuiltinOperator RANK Register RANK ; AddBuiltin BuiltinOperator POW Register POW ; AddBuiltin BuiltinOperator FAKE QUANT Register FAKE QUANT 1 2 ; AddBuiltin BuiltinOperator PACK Register PACK * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator ONE HOT Register ONE HOT ; AddBuiltin BuiltinOperator LOGICAL OR Register LOGICAL OR ; AddBuiltin BuiltinOperator LOGICAL AND Register LOGICAL AND ; AddBuiltin BuiltinOperator LOGICAL NOT Register LOGICAL NOT ; AddBuiltin BuiltinOperator UNPACK Register UNPACK * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator FLOOR DIV Register FLOOR DIV * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SQUARE Register SQUARE ; AddBuiltin BuiltinOperator ZEROS LIKE Register ZEROS LIKE ; AddBuiltin BuiltinOperator FLOOR MOD Register FLOOR MOD * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator RANGE Register RANGE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LEAKY RELU Register LEAKY RELU * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SQUARED DIFFERENCE Register SQUARED DIFFERENCE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator FILL Register FILL * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator MIRROR PAD Register MIRROR PAD * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator UNIQUE Register UNIQUE ; AddBuiltin BuiltinOperator REVERSE V2 Register REVERSE V2 * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ADD N Register ADD N ; AddBuiltin BuiltinOperator GATHER ND Register GATHER ND * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator WHERE Register WHERE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator ELU Register ELU ; AddBuiltin BuiltinOperator REVERSE SEQUENCE Register REVERSE SEQUENCE ; AddBuiltin BuiltinOperator MATRIX DIAG Register MATRIX DIAG ; AddBuiltin BuiltinOperator QUANTIZE Register QUANTIZE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator MATRIX SET DIAG Register MATRIX SET DIAG ; AddBuiltin BuiltinOperator IF tflite ops builtin Register IF ; AddBuiltin BuiltinOperator WHILE tflite ops builtin Register WHILE ; AddBuiltin BuiltinOperator NON MAX SUPPRESSION V4 Register NON MAX SUPPRESSION V4 ; AddBuiltin BuiltinOperator NON MAX SUPPRESSION V5 Register NON MAX SUPPRESSION V5 ; AddBuiltin BuiltinOperator SCATTER ND Register SCATTER ND ; AddBuiltin BuiltinOperator DENSIFY Register DENSIFY ; AddBuiltin BuiltinOperator SEGMENT SUM Register SEGMENT SUM ; AddBuiltin BuiltinOperator BATCH MATMUL Register BATCH MATMUL * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator CUMSUM Register CUMSUM ; The version one of broadcast to op won t be not supported since the version one was rollbacked and the builtin op code number has been changed because of builtin op code shortage problem AddBuiltin BuiltinOperator BROADCAST TO Register BROADCAST TO * min version * 2 * max version * 3 ; AddBuiltin BuiltinOperator CALL ONCE tflite ops builtin Register CALL ONCE ; AddBuiltin BuiltinOperator RFFT2D Register RFFT2D ; AddBuiltin BuiltinOperator CONV 3D Register CONV 3D ; AddBuiltin BuiltinOperator IMAG Register IMAG ; AddBuiltin BuiltinOperator REAL Register REAL ; AddBuiltin BuiltinOperator COMPLEX ABS Register COMPLEX ABS ; AddBuiltin BuiltinOperator BROADCAST ARGS Register BROADCAST ARGS ; AddBuiltin BuiltinOperator HASHTABLE Register HASHTABLE ; AddBuiltin BuiltinOperator HASHTABLE FIND Register HASHTABLE FIND ; AddBuiltin BuiltinOperator HASHTABLE IMPORT Register HASHTABLE IMPORT ; AddBuiltin BuiltinOperator HASHTABLE SIZE Register HASHTABLE SIZE ; AddBuiltin BuiltinOperator CONV 3D TRANSPOSE Register CONV 3D TRANSPOSE ; AddBuiltin BuiltinOperator VAR HANDLE Register VAR HANDLE ; AddBuiltin BuiltinOperator READ VARIABLE Register READ VARIABLE ; AddBuiltin BuiltinOperator ASSIGN VARIABLE Register ASSIGN VARIABLE ; AddBuiltin BuiltinOperator MULTINOMIAL Register MULTINOMIAL ; AddBuiltin BuiltinOperator RANDOM STANDARD NORMAL Register RANDOM STANDARD NORMAL ; AddBuiltin BuiltinOperator BUCKETIZE Register BUCKETIZE ; AddBuiltin BuiltinOperator RANDOM UNIFORM Register RANDOM UNIFORM ; AddBuiltin BuiltinOperator GELU Register GELU * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator DYNAMIC UPDATE SLICE Register DYNAMIC UPDATE SLICE ; AddBuiltin BuiltinOperator UNSORTED SEGMENT PROD Register UNSORTED SEGMENT PROD ; AddBuiltin BuiltinOperator UNSORTED SEGMENT MAX Register UNSORTED SEGMENT MAX ; AddBuiltin BuiltinOperator UNSORTED SEGMENT MIN Register UNSORTED SEGMENT MIN ; AddBuiltin BuiltinOperator UNSORTED SEGMENT SUM Register UNSORTED SEGMENT SUM ; AddBuiltin BuiltinOperator ATAN2 Register ATAN2 ; AddBuiltin BuiltinOperator SIGN Register SIGN * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator BITCAST Register BITCAST ; AddBuiltin BuiltinOperator BITWISE XOR Register BITWISE XOR ; AddBuiltin BuiltinOperator RIGHT SHIFT Register RIGHT SHIFT ; AddBuiltin BuiltinOperator STABLEHLO SCATTER Register STABLEHLO SCATTER ; AddCustom "NumericVerify" tflite ops custom Register NUMERIC VERIFY ; TODO andrewharp ahentz Move these somewhere more appropriate so that custom ops aren t always included by default AddCustom "Mfcc" tflite ops custom Register MFCC ; AddCustom "AudioSpectrogram" tflite ops custom Register AUDIO SPECTROGRAM ; AddCustom "TFLite Detection PostProcess" tflite ops custom Register DETECTION POSTPROCESS ; By definition all of the ops added above are not user-defined ops since they are supported by BuiltinOpResolver may directly contain user defined ops false; Populate the list of TF Lite delegate creators The created delegates could be applied to the model graph by default at runtime delegate creators push back [] TfLiteContext* context { return tflite MaybeCreateXNNPACKDelegate context XNNPackQS8Options default value ; } ; }
BuiltinOpResolver BuiltinOpResolver { AddBuiltin BuiltinOperator ABS Register ABS * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator HARD SWISH Register HARD SWISH ; AddBuiltin BuiltinOperator RELU Register RELU * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator RELU N1 TO 1 Register RELU N1 TO 1 ; AddBuiltin BuiltinOperator RELU 0 TO 1 Register RELU 0 TO 1 ; AddBuiltin BuiltinOperator RELU6 Register RELU6 * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator TANH Register TANH * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator LOGISTIC Register LOGISTIC * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator AVERAGE POOL 2D Register AVERAGE POOL 2D * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator MAX POOL 2D Register MAX POOL 2D * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator L2 POOL 2D Register L2 POOL 2D ; AddBuiltin BuiltinOperator CONV 2D Register CONV 2D * min version * 1 * max version * 8 ; AddBuiltin BuiltinOperator DEPTHWISE CONV 2D Register DEPTHWISE CONV 2D * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator SVDF Register SVDF * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RNN Register RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator BIDIRECTIONAL SEQUENCE RNN Register BIDIRECTIONAL SEQUENCE RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator UNIDIRECTIONAL SEQUENCE RNN Register UNIDIRECTIONAL SEQUENCE RNN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EMBEDDING LOOKUP Register EMBEDDING LOOKUP * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EMBEDDING LOOKUP SPARSE Register EMBEDDING LOOKUP SPARSE ; AddBuiltin BuiltinOperator FULLY CONNECTED Register FULLY CONNECTED * min version * 1 * max version * 11 ; AddBuiltin BuiltinOperator LSH PROJECTION Register LSH PROJECTION ; AddBuiltin BuiltinOperator HASHTABLE LOOKUP Register HASHTABLE LOOKUP ; AddBuiltin BuiltinOperator SOFTMAX Register SOFTMAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator CONCATENATION Register CONCATENATION * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ADD Register ADD * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SPACE TO BATCH ND Register SPACE TO BATCH ND * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator BATCH TO SPACE ND Register BATCH TO SPACE ND * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator MUL Register MUL * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator L2 NORMALIZATION Register L2 NORMALIZATION * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LOCAL RESPONSE NORMALIZATION Register LOCAL RESPONSE NORMALIZATION ; AddBuiltin BuiltinOperator LSTM Register LSTM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator BIDIRECTIONAL SEQUENCE LSTM Register BIDIRECTIONAL SEQUENCE LSTM * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator UNIDIRECTIONAL SEQUENCE LSTM Register UNIDIRECTIONAL SEQUENCE LSTM * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator PAD Register PAD * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator PADV2 Register PADV2 * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RESHAPE Register RESHAPE ; AddBuiltin BuiltinOperator RESIZE BILINEAR Register RESIZE BILINEAR * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator RESIZE NEAREST NEIGHBOR Register RESIZE NEAREST NEIGHBOR * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SKIP GRAM Register SKIP GRAM ; AddBuiltin BuiltinOperator SPACE TO DEPTH Register SPACE TO DEPTH * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator DEPTH TO SPACE Register DEPTH TO SPACE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator GATHER Register GATHER * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator TRANSPOSE Register TRANSPOSE * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator MEAN Register MEAN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator DIV Register DIV * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SUB Register SUB * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator SPLIT Register SPLIT * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator SPLIT V Register SPLIT V * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SQUEEZE Register SQUEEZE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator STRIDED SLICE Register STRIDED SLICE * min version * 1 * max version * 6 ; AddBuiltin BuiltinOperator EXP Register EXP ; AddBuiltin BuiltinOperator TOPK V2 Register TOPK V2 * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LOG Register LOG ; AddBuiltin BuiltinOperator LOG SOFTMAX Register LOG SOFTMAX * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator CAST Register CAST * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator DEQUANTIZE Register DEQUANTIZE * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator PRELU Register PRELU ; AddBuiltin BuiltinOperator MAXIMUM Register MAXIMUM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator MINIMUM Register MINIMUM * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator ARG MAX Register ARG MAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ARG MIN Register ARG MIN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator GREATER Register GREATER * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator GREATER EQUAL Register GREATER EQUAL * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LESS Register LESS * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator LESS EQUAL Register LESS EQUAL * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator FLOOR Register FLOOR ; AddBuiltin BuiltinOperator CEIL Register CEIL ; AddBuiltin BuiltinOperator ROUND Register ROUND ; AddBuiltin BuiltinOperator NEG Register NEG ; AddBuiltin BuiltinOperator SELECT Register SELECT * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SELECT V2 Register SELECT V2 ; AddBuiltin BuiltinOperator SLICE Register SLICE * min version * 1 * max version * 5 ; AddBuiltin BuiltinOperator SIN Register SIN ; AddBuiltin BuiltinOperator COS Register COS ; AddBuiltin BuiltinOperator TRANSPOSE CONV Register TRANSPOSE CONV * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator TILE Register TILE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SUM Register SUM * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator REDUCE PROD Register REDUCE PROD * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator REDUCE MAX Register REDUCE MAX * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator REDUCE MIN Register REDUCE MIN * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator REDUCE ANY Register REDUCE ANY ; AddBuiltin BuiltinOperator REDUCE ALL Register REDUCE ALL ; AddBuiltin BuiltinOperator EXPAND DIMS Register EXPAND DIMS ; AddBuiltin BuiltinOperator SPARSE TO DENSE Register SPARSE TO DENSE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator EQUAL Register EQUAL * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator NOT EQUAL Register NOT EQUAL * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator SQRT Register SQRT ; AddBuiltin BuiltinOperator RSQRT Register RSQRT * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SHAPE Register SHAPE ; AddBuiltin BuiltinOperator RANK Register RANK ; AddBuiltin BuiltinOperator POW Register POW ; AddBuiltin BuiltinOperator FAKE QUANT Register FAKE QUANT 1 2 ; AddBuiltin BuiltinOperator PACK Register PACK * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ONE HOT Register ONE HOT ; AddBuiltin BuiltinOperator LOGICAL OR Register LOGICAL OR ; AddBuiltin BuiltinOperator LOGICAL AND Register LOGICAL AND ; AddBuiltin BuiltinOperator LOGICAL NOT Register LOGICAL NOT ; AddBuiltin BuiltinOperator UNPACK Register UNPACK * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator FLOOR DIV Register FLOOR DIV * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SQUARE Register SQUARE ; AddBuiltin BuiltinOperator ZEROS LIKE Register ZEROS LIKE ; AddBuiltin BuiltinOperator FLOOR MOD Register FLOOR MOD ; AddBuiltin BuiltinOperator RANGE Register RANGE ; AddBuiltin BuiltinOperator LEAKY RELU Register LEAKY RELU * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator SQUARED DIFFERENCE Register SQUARED DIFFERENCE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator FILL Register FILL * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator MIRROR PAD Register MIRROR PAD * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator UNIQUE Register UNIQUE ; AddBuiltin BuiltinOperator REVERSE V2 Register REVERSE V2 * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator ADD N Register ADD N ; AddBuiltin BuiltinOperator GATHER ND Register GATHER ND * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator WHERE Register WHERE * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator ELU Register ELU ; AddBuiltin BuiltinOperator REVERSE SEQUENCE Register REVERSE SEQUENCE ; AddBuiltin BuiltinOperator MATRIX DIAG Register MATRIX DIAG ; AddBuiltin BuiltinOperator QUANTIZE Register QUANTIZE * min version * 1 * max version * 3 ; AddBuiltin BuiltinOperator MATRIX SET DIAG Register MATRIX SET DIAG ; AddBuiltin BuiltinOperator IF tflite ops builtin Register IF ; AddBuiltin BuiltinOperator WHILE tflite ops builtin Register WHILE ; AddBuiltin BuiltinOperator NON MAX SUPPRESSION V4 Register NON MAX SUPPRESSION V4 ; AddBuiltin BuiltinOperator NON MAX SUPPRESSION V5 Register NON MAX SUPPRESSION V5 ; AddBuiltin BuiltinOperator SCATTER ND Register SCATTER ND ; AddBuiltin BuiltinOperator DENSIFY Register DENSIFY ; AddBuiltin BuiltinOperator SEGMENT SUM Register SEGMENT SUM ; AddBuiltin BuiltinOperator BATCH MATMUL Register BATCH MATMUL * min version * 1 * max version * 4 ; AddBuiltin BuiltinOperator CUMSUM Register CUMSUM ; The version one of broadcast to op won t be not supported since the version one was rollbacked and the builtin op code number has been changed because of builtin op code shortage problem AddBuiltin BuiltinOperator BROADCAST TO Register BROADCAST TO * min version * 2 * max version * 3 ; AddBuiltin BuiltinOperator CALL ONCE tflite ops builtin Register CALL ONCE ; AddBuiltin BuiltinOperator RFFT2D Register RFFT2D ; AddBuiltin BuiltinOperator CONV 3D Register CONV 3D ; AddBuiltin BuiltinOperator IMAG Register IMAG ; AddBuiltin BuiltinOperator REAL Register REAL ; AddBuiltin BuiltinOperator COMPLEX ABS Register COMPLEX ABS ; AddBuiltin BuiltinOperator BROADCAST ARGS Register BROADCAST ARGS ; AddBuiltin BuiltinOperator HASHTABLE Register HASHTABLE ; AddBuiltin BuiltinOperator HASHTABLE FIND Register HASHTABLE FIND ; AddBuiltin BuiltinOperator HASHTABLE IMPORT Register HASHTABLE IMPORT ; AddBuiltin BuiltinOperator HASHTABLE SIZE Register HASHTABLE SIZE ; AddBuiltin BuiltinOperator CONV 3D TRANSPOSE Register CONV 3D TRANSPOSE ; AddBuiltin BuiltinOperator VAR HANDLE Register VAR HANDLE ; AddBuiltin BuiltinOperator READ VARIABLE Register READ VARIABLE ; AddBuiltin BuiltinOperator ASSIGN VARIABLE Register ASSIGN VARIABLE ; AddBuiltin BuiltinOperator MULTINOMIAL Register MULTINOMIAL ; AddBuiltin BuiltinOperator RANDOM STANDARD NORMAL Register RANDOM STANDARD NORMAL ; AddBuiltin BuiltinOperator BUCKETIZE Register BUCKETIZE ; AddBuiltin BuiltinOperator RANDOM UNIFORM Register RANDOM UNIFORM ; AddBuiltin BuiltinOperator GELU Register GELU * min version * 1 * max version * 2 ; AddBuiltin BuiltinOperator DYNAMIC UPDATE SLICE Register DYNAMIC UPDATE SLICE ; AddBuiltin BuiltinOperator UNSORTED SEGMENT PROD Register UNSORTED SEGMENT PROD ; AddBuiltin BuiltinOperator UNSORTED SEGMENT MAX Register UNSORTED SEGMENT MAX ; AddBuiltin BuiltinOperator UNSORTED SEGMENT MIN Register UNSORTED SEGMENT MIN ; AddBuiltin BuiltinOperator UNSORTED SEGMENT SUM Register UNSORTED SEGMENT SUM ; AddBuiltin BuiltinOperator ATAN2 Register ATAN2 ; AddBuiltin BuiltinOperator SIGN Register SIGN * min version * 1 * max version * 2 ; AddCustom "NumericVerify" tflite ops custom Register NUMERIC VERIFY ; TODO andrewharp ahentz Move these somewhere more appropriate so that custom ops aren t always included by default AddCustom "Mfcc" tflite ops custom Register MFCC ; AddCustom "AudioSpectrogram" tflite ops custom Register AUDIO SPECTROGRAM ; AddCustom "TFLite Detection PostProcess" tflite ops custom Register DETECTION POSTPROCESS ; By definition all of the ops added above are not user-defined ops since they are supported by BuiltinOpResolver may directly contain user defined ops false; Populate the list of TF Lite delegate creators The created delegates could be applied to the model graph by default at runtime delegate creators push back [] TfLiteContext* context { return tflite MaybeCreateXNNPACKDelegate context *enable xnnpack unsigned quantized * false ; } ; }
TEST F RangeSamplerTest FixedUnigramNoMatchingRangeWeights { Env* env Env Default ; string fname io JoinPath testing TmpDir "vocab file" ; TF CHECK OK WriteStringToFile env fname kVocabContent ; FixedUnigramSampler* test sampler new FixedUnigramSampler 8 0 8 0 1 0 ; Status s test sampler->SetDistributionSampler env fname ; EXPECT TRUE errors IsInvalidArgument s << s; }
def testStatelessDtypes self # Test case for GitHub issue 59160 shape 0 constant op constant 1 dtype dtypes int64 #125091515651 shape [shape 0 ] seed 0 12 seed 1 34 seed [seed 0 seed 1 ] counts 10 0 probs 0 4 output dtype dtypes float16 out stateless random ops stateless random binomial shape shape seed seed counts counts probs probs output dtype output dtype self evaluate out
Value getTosaConst16bitTable PatternRewriter& rewriter Operation* op std function<double double > func double min double max { SmallVector<int16 t 513> table; double step max - min 512 0f; double half step step 2 0f; for int32 t i 0; i < 512; i++ { int32 t sample val std llround func min + i * step * 32768 0 ; double midpoint interp val std round func min + i + 1 * step * 32768 0 + std round func min + i * step * 32768 0 2 0 ; double midpoint val std round func min + i * step + half step * 32768 0 ; double midpoint err midpoint interp val - midpoint val; int32 t bias std llround midpoint err 2 0 ; table push back static cast<int16 t> std min std max sample val - bias -32768 32767 ; } int32 t max val std llround func max * 32768 0 ; table push back static cast<int16 t> std min std max max val -32768 32767 ; auto const type tensorflow GetTypeFromTFTensorShape {513} rewriter getIntegerType 16 ; auto const attr DenseElementsAttr get const type llvm ArrayRef table ; auto const op rewriter create<tosa ConstOp> op->getLoc const type const attr ; return const op getResult ; }
bool ROCMBlas DoBlasInternalImpl FuncT rocblas func Stream *stream bool pointer mode host bool err on failure Args args { absl MutexLock lock{&mu }; CHECK blas ! nullptr ; if !SetStream stream { return false; } gpu ScopedActivateExecutorContext sac{parent }; set the atomics mode leaving default to library bool allow atomics !OpDeterminismRequired ; rocblas status ret; if !allow atomics { ret rocblas set atomics mode blas rocblas atomics not allowed ; if err on failure && ret ! rocblas status success { LOG ERROR << "failed to to set atomics mode before " << rocblas func kName << " " << ToString ret ; } } ret rocblas func blas args ; if err on failure && ret ! rocblas status success { LOG ERROR << "failed to run ROCBLAS routine " << rocblas func kName << " " << ToString ret ; } return ret rocblas status success; }
bool IsGoogleTensorRTEnabled { #if GOOGLE CUDA && GOOGLE TENSORRT #if TF USE TENSORRT STATIC LOG INFO << "TensorRT libraries are statically linked skip dlopen check"; return true; #else TF USE TENSORRT STATIC auto handle or se internal DsoLoader TryDlopenTensorRTLibraries ; if !handle or ok { LOG WARNING WITH PREFIX << "TensorRT is not installed "; } return handle or ok ; #endif TF USE TENSORRT STATIC #else GOOGLE CUDA && GOOGLE TENSORRT return false; #endif GOOGLE CUDA && GOOGLE TENSORRT }
const ZenLayoutRewritePass ZenOpRewriteRecord * ZenLayoutRewritePass CheckNodeForZenOpRewrite const Node *n const { CHECK NOTNULL n ; DataType data type; for auto rewrite record zen rewrite db cbegin ; rewrite record ! zen rewrite db cend ; ++rewrite record { if n->type string compare rewrite record->tf op name 0 && rewrite record->check validity n { TF CHECK OK GetNodeAttr n->def "T" &data type ; if !zen op registry IsZenOpKernelRegistered rewrite record->zen op name data type { No Zen kernel is registered for op return nullptr; } return &*rewrite record; } } return nullptr; }
Value getTosaConst16bitTable PatternRewriter& rewriter Operation* op std function<double double > func double min double max { SmallVector<int16 t 513> table; double step max - min 512 0f; double half step step 2 0f; for int32 t i 0; i < 512; i++ { int32 t sample val std llround func min + i * step * 32768 0 ; double midpoint interp val std round func min + i + 1 * step * 32768 0 + std round func min + i * step * 32768 0 2 0 ; double midpoint val std round func min + i * step + half step * 32768 0 ; double midpoint err midpoint interp val - midpoint val; int32 t bias std llround midpoint err 2 0 ; table push back static cast<int16 t> std min std max sample val - bias -32768 32767 ; } int32 t max val std llround func max * 32768 0 ; table push back static cast<int16 t> std min std max max val -32768 32767 ; auto const type tensorflow GetTypeFromTFTensorShape {513} rewriter getIntegerType 16 ; auto const attr DenseElementsAttr get const type llvm ArrayRef table ; auto const op rewriter create<tosa ConstOp> op->getLoc const type const attr ; return const op getResult ; }
StatusOr<bool> FuseLeakyRelu HloComputation* comp se CudaComputeCapability cc { bool changed false; for HloInstruction* instr comp->MakeInstructionPostOrder { const DebugOptions& debug options instr->GetModule ->config debug options ; if !debug options xla gpu use runtime fusion || !cc IsAtLeast se CudaComputeCapability AMPERE { return false; } HloInstruction* gte; HloInstruction* conv; HloInstruction* alpha; We don t want to upgrade depthwise convolutions to ConvBiasActivation because the fused CUDNN functions are slower for some of those auto gte pattern m GetTupleElement &gte m Op &conv WithPredicate IsNonDepthwiseConvCustomCall WithOneUse WithElementType F16 WithPredicate HasThreeUsers ; if !Match instr m Select m Compare gte pattern m Broadcast m ConstantEffectiveScalar 0 WithComparisonDirection ComparisonDirection kGt WithOneUse gte pattern m Multiply gte pattern m Broadcast m ConstantEffectiveScalar &alpha { continue; } TF ASSIGN OR RETURN CudnnConvBackendConfig config conv->backend config<CudnnConvBackendConfig> ; if config activation mode ! se dnn kNone { continue; } if !ConsumeFuel "cudnn-fused-convolution-rewriter" [&] { return absl StrCat "FuseLeakyRelu " conv->ToString ; } { continue; } TF ASSIGN OR RETURN conv EnsureIsConvBiasActivation conv ; config set activation mode se dnn kLeakyRelu ; TF RETURN IF ERROR conv->set backend config config ; TF RETURN IF ERROR comp->ReplaceInstruction instr gte ; changed true; } return changed; }
Value getTosaConst8bitTable PatternRewriter& rewriter Operation* op double input scale int32 t input zp double output scale int32 t output zp std function<double double > func { SmallVector<int8 t 256> table; for int32 t i -128; i < 128; i++ { double dequantized input scale * i - input zp ; double transformed func dequantized ; std feclearexcept FE ALL EXCEPT ; int32 t rescaled std llround transformed output scale ; When transformed output scale is generates an exception replace it with INT8 MAX if std fetestexcept FE INVALID { table push back INT8 MAX ; continue; } int32 t quantized static cast<int32 t> rescaled + output zp ; table push back static cast<int8 t> std min std max quantized -128 127 ; } auto element qtype UniformQuantizedType get true rewriter getIntegerType 8 rewriter getF32Type 1 0f 0 -128 127 ; auto const type tensorflow GetTypeFromTFTensorShape {256} element qtype ; auto storage type tensorflow GetTypeFromTFTensorShape {256} element qtype getStorageType ; auto const attr DenseElementsAttr get storage type llvm ArrayRef table ; auto const op rewriter create<tosa ConstOp> op->getLoc const type const attr ; return const op getResult ; }
def testBFloat16 self with ops device " cpu 0" for rank in range 1 MAX RANK + 1 np arr self makeIncremental 2 * rank dtypes bfloat16 self compareAllAxes np arr
def testFloat32BFloat16 self for dtype in [dtypes float32 dtypes bfloat16] dtype np np float32 if dtype dtypes float32 else dtypes bfloat16 as numpy dtype for rank in range 1 MAX RANK + 1 np arr self makeIncremental 2 * rank dtype self compareAllAxes np arr for in range 10 size x int 2**np random uniform 0 15 size y int 2**np random uniform 0 15 if size x * size y > 1e7 size y int 1e7 size x arr np ones [size x size y] dtype dtype np col sum np sum arr axis 0 dtype np float32 row sum np sum arr axis 1 dtype np float32 with self session graph ops Graph use gpu True as sess tf row sum self tf reduce arr 1 False tf col sum self tf reduce arr 0 False tf out row tf out col self evaluate [tf row sum tf col sum] if dtype dtypes bfloat16 col sum dtype np col sum row sum dtype np row sum self assertAllCloseAccordingToType col sum tf out col self assertAllCloseAccordingToType row sum tf out row for size x in [1 3 16 33] for size y in [1 3 16 33] for size z in [1 3 16 33] arr np ones [size x size y size z] dtype dtype np sum y np sum arr axis 1 dtype np float32 sum xz np sum arr axis 0 2 dtype np float32 with self session graph ops Graph use gpu True as sess tf sum xz self tf reduce arr [0 2] False tf sum y self tf reduce arr 1 False tf out sum xz tf out sum y self evaluate [tf sum xz tf sum y] if dtype dtypes bfloat16 sum y dtype np sum y sum xz dtype np sum xz self assertAllCloseAccordingToType sum y tf out sum y self assertAllCloseAccordingToType sum xz tf out sum xz
def real input name None r"""Returns the real part of a complex or real tensor Given a tensor `input` this operation returns a tensor of type `float` that is the real part of each element in `input` considered as a complex number For example ```python x tf constant [-2 25 + 4 75j 3 25 + 5 75j] tf math real x # [-2 25 3 25] ``` If `input` is already real it is returned unchanged Args input A `Tensor` Must have numeric type name A name for the operation optional Returns A `Tensor` of type `float32` or `float64` """ with ops name scope name "Real" [input] as name input ops convert to tensor input name "input" if input dtype is complex real dtype input dtype real dtype return gen math ops real input Tout real dtype name name elif tf debugging is numeric tensor input return input else raise TypeError "input must be a numeric tensor but got tensor with dtype {}" format input dtype
def real input name None r"""Returns the real part of a complex or real tensor Given a tensor `input` this operation returns a tensor of type `float` that is the real part of each element in `input` considered as a complex number For example ```python x tf constant [-2 25 + 4 75j 3 25 + 5 75j] tf math real x # [-2 25 3 25] ``` If `input` is already real it is returned unchanged Args input A `Tensor` Must have numeric type name A name for the operation optional Returns A `Tensor` of type `float32` or `float64` """ with ops name scope name "Real" [input] as name input ops convert to tensor input name "input" if input dtype is complex real dtype input dtype real dtype return gen math ops real input Tout real dtype name name elif tf debugging is numeric tensor input return input else raise TypeError "input must be a numeric tensor but got tensor with dtype {}" format input dtype
std optional<Value> convertStridedSliceOp PatternRewriter& rewriter Operation* op Value result value Value input value Value begin value Value end value Value strides value int32 t begin mask int32 t end mask int32 t ellipsis mask int32 t new axis mask int32 t shrink axis mask { The mask arguments are bitmasks where bit [i] applies to dimension [i] of the input tensor The rough algorithm for lowering strided slice is as follows 0 Process begin end masks since they are basically syntactic sugar on top of the begin value end value arrays 1 Slice1 Ignoring stride slice the interesting range from the input tensor 2 Reshape2 Reshape the tensor from 1 such that each dimension with abs stride ! 1 is split into two dimensions of size i stride i stride i 3 Slice3 Slice the tensor from 2 such that we select index [0] from each of the stride i dimensions in 2 4 Reshape4 Reshape the tensor to eliminate the stride i dimensions add any dimensions in new axis mask and remove any dimensions in the shrink axis mask Limitations * This implementation only supports ellipsis mask 0 for now auto input type input value getType dyn cast<RankedTensorType> ; ShapedType result type result value getType cast<ShapedType> ; if ellipsis mask ! 0 { void rewriter notifyMatchFailure op "ellipses mask not supported yet" ; return std nullopt; } if !input type { void rewriter notifyMatchFailure op "input type has unknown rank" ; return std nullopt; } int32 t input rank input type getRank ; Type element type input type getElementType ; Conditionally extract begin end values if requied SmallVector<int32 t> begin end strides; if failed getVectorFromValue32 strides value strides { void rewriter notifyMatchFailure op "strides isn t a constant" ; return std nullopt; } Current configuration does not support negative strides greater than 1 Bail out for now fix if this proves to be legal for auto stride strides if stride < -1 return std nullopt; bool all strides one true; int32 t strides size strides size ; for auto stride strides all strides one & abs stride 1; If all of the masks are set we can just bypass the entire thing const int32 t all masks one 1 << strides size - 1; if failed getVectorFromValue32 begin value begin && begin mask ! all masks one { void rewriter notifyMatchFailure op "begin isn t a constant" ; return std nullopt; } if end mask ! all masks one && failed getVectorFromValue32 end value end { void rewriter notifyMatchFailure op "end isn t a constant" ; return std nullopt; } if llvm any of strides [] auto i { return i < -1; } { void rewriter notifyMatchFailure op "stride < -1 unsupported" ; return std nullopt; } Set begin mask values if possible for const auto& val llvm enumerate begin begin mask | val value 0 << val index ; If all begin end masks are set and striding is one we can just return the matrix with reversed dims for negative strides if all strides one && begin mask all masks one && end mask all masks one { return reverseNegativeStride rewriter op input value strides ; } Set the bits true for the remaining dimensions int32 t new mask bits 1 << input rank - all masks one - 1; begin mask | new mask bits; end mask | new mask bits; Negative values are exclusive from the opposite end while TOSA is inclusive we offset to adjust for this for auto& b begin if b < 0 b b - 1; for auto& e end if e < 0 e e - 1; Fill the remaining stride and begin end with default values strides resize input rank 1 ; begin resize input rank 0 ; end resize input rank -1 ; Set masking-bit overrides for int i 0; i < input rank; ++i { if begin mask & 1 << i begin[i] 0; if end mask & 1 << i end[i] -1; } If we know the static end we can adjust by it for int i 0; i < input rank; ++i { if input type isDynamicDim i continue; if begin[i] < 0 begin[i] + input type getDimSize i + 1; if end[i] < 0 end[i] + input type getDimSize i + 1; } Perform some final validation on the begin end values for int i 0; i < input rank; ++i { if begin[i] < 0 && input type isDynamicDim i { void rewriter notifyMatchFailure op "begin offset is negative on dynamic size" ; return std nullopt; } if end[i] < -1 && input type isDynamicDim i { void rewriter notifyMatchFailure op "end is exclusive of final entry on dynamic size" ; return std nullopt; } } Step 0 Process the begin end masks and build the begin sizes for the first slice SmallVector<int64 t> a1 begin input rank a1 size input rank ; for int i 0; i < input rank; ++i { Wrap around index if begin and end is negative a1 begin[i] begin[i]; if end[i] -1 && input type isDynamicDim i { Slice using -1 as TOSA s sentinal value a1 size[i] -1; } else if end[i] < 0 && input type isDynamicDim i { Other dynamic cases cannot be handled void rewriter notifyMatchFailure op "input dim is dynamic and slice end depends on the length" ; return std nullopt; } else { a1 size[i] end[i] - a1 begin[i]; } Shrink axis mask means we know the size and stride are 1 if shrink axis mask & 1 << i { a1 size[i] 1; strides[i] 1; } } Step 1 Slice the input array auto a1 slice op CreateOpAndInfer<tosa SliceOp> rewriter op->getLoc tensorflow GetTypeFromTFTensorShape a1 size element type input value rewriter getDenseI64ArrayAttr a1 begin rewriter getDenseI64ArrayAttr tensorflow ConvertMlirShapeToTF a1 size ; If unary striding is used we can reverse reshape and return the result if all strides one { auto reversed reverseNegativeStride rewriter op a1 slice op getResult strides ; auto shape reversed getType cast<RankedTensorType> getShape ; SmallVector<int64 t> new shape; for int i 0; i < input rank; ++i { if ! shrink axis mask & 1 << i { if new axis mask & 1 << i new shape push back 1 ; new shape push back shape[i] ; } } return CreateOpAndInfer<tosa ReshapeOp> rewriter op->getLoc result type reversed rewriter getDenseI64ArrayAttr tensorflow ConvertMlirShapeToTF new shape getResult ; } Step 2 reshape the sliced array SmallVector<int64 t> a2 shape; for int i 0; i < input rank; ++i { int64 t abs stride i abs strides[i] ; a2 shape push back a1 size[i] -1 ? -1 a1 size[i] abs stride i ; if abs stride i ! 1 { only add a stride dimension if strides[i] ! 1 a2 shape push back abs stride i ; } } auto a2 reshape op CreateOpAndInfer<tosa ReshapeOp> rewriter op->getLoc tensorflow GetTypeFromTFTensorShape a2 shape element type a1 slice op getResult rewriter getDenseI64ArrayAttr tensorflow ConvertMlirShapeToTF a2 shape ; Step 3 take a slice along the strides SmallVector<int64 t> a3 begin a3 size; for int i 0; i < input rank; ++i { int64 t abs stride i abs strides[i] ; a3 begin push back 0 ; if shrink axis mask & 1 << i { a3 size push back 1 ; } else { a3 size push back a1 size[i] -1 ? -1 a1 size[i] abs stride i ; } if abs stride i ! 1 { previous reshape only adds a stride dimension if strides[i] ! 1 a3 begin push back 0 ; a3 size push back 1 ; } } assert a2 shape size a3 begin size ; assert a2 shape size a3 size size ; auto a3 slice op CreateOpAndInfer<tosa SliceOp> rewriter op->getLoc tensorflow GetTypeFromTFTensorShape a3 size element type a2 reshape op getResult rewriter getDenseI64ArrayAttr a3 begin rewriter getDenseI64ArrayAttr tensorflow ConvertMlirShapeToTF a3 size ; Step 4 reshape the now-strided tensor SmallVector<int64 t> a4 shape; for int i 0; i < input rank; ++i { if ! shrink axis mask & 1 << i { if new axis mask & 1 << i a4 shape push back 1 ; a4 shape push back a1 size[i] -1 ? -1 a1 size[i] abs strides[i] ; } } auto a4 reshape op CreateOpAndInfer<tosa ReshapeOp> rewriter op->getLoc result type a3 slice op getResult rewriter getDenseI64ArrayAttr tensorflow ConvertMlirShapeToTF a4 shape getResult ; return reverseNegativeStride rewriter op a4 reshape op strides ; }
void Setup const MklConcatFwdParams& concat fwd dims const std vector<memory desc>& srcs md { Create memory descriptors for concat with specified srcs format for size t i 0; i < concat fwd dims num inputs; i++ { dnnl memory desc source md memory desc GET MEMORY DESC srcs md[i] ; context src md push back source md ; std shared ptr<dnnl memory> src mem new dnnl memory source md cpu engine DummyData ; context data mem shdptr push back src mem ; context data mem push back *context data mem shdptr[i] ; } Store the expected memory format context dst md reset new memory desc {concat fwd dims dst dims} MklDnnType<T> concat fwd dims mkl common format ; Create a concat primitive descriptor context fwd pd reset new CONCAT PRIM DESC cpu engine concat fwd dims concat dims context src md context dst md ; Create memory primitive based on dummy data context dst mem reset new memory *context dst md cpu engine DummyData ; context concat fwd reset new concat *context fwd pd ; std unordered map<int memory> net args { {DNNL ARG DST *context dst mem}}; for int i 0; i < concat fwd dims num inputs; ++i { net args insert {DNNL ARG MULTIPLE SRC + i context data mem[i]} ; } context fwd primitives args push back net args ; context fwd primitives push back *context concat fwd ; }
static void Initialize { Initialize cpuid struct if cpuid ! nullptr { return; } cpuid new CPUIDInfo; if ! getauxval AT HWCAP & HWCAP CPUID { return; } std ifstream CPUspresent; CPUspresent open " sys devices system cpu present" std ios in ; int present cpu -1; if CPUspresent is open { std string line; if bool getline CPUspresent line { We just need to find one CPU that is active from which we can read MIDR register to find implement variant and revision information auto ending line end ; for auto i line begin ; i < line end ; ++i { if *i - || *i { ending i; break; } } line erase ending line end ; That should be the fist number present cpu std stoi line ; } } if present cpu -1 { return; } std stringstream str; str << " sys devices system cpu cpu" << present cpu << " regs identification midr el1"; std ifstream midr el1 file str str std ios in ; if midr el1 file is open { std string line; if bool getline midr el1 file line { uint32 midr el1 std stoul line nullptr 16 ; Unpack variant and CPU ID cpuid->implementer midr el1 >> 24 & 0xFF; cpuid->variant midr el1 >> 20 & 0xF; cpuid->cpunum midr el1 >> 4 & 0xFFF; } } }
absl Status InterpreterInvokeWithOpResolver const tflite Model* model TfLiteDelegate* delegate const OpResolver& op resolver const std vector<TensorFloat32>& inputs std vector<TensorFloat32>* outputs { auto interpreter std make unique<Interpreter> ; if InterpreterBuilder model op resolver &interpreter ! kTfLiteOk { return absl InternalError "Unable to create TfLite InterpreterBuilder" ; } if delegate && interpreter->ModifyGraphWithDelegate delegate ! kTfLiteOk { return absl InternalError "Unable to modify TfLite graph with the delegate" ; } interpreter->SetNumThreads 1 ; if interpreter->AllocateTensors ! kTfLiteOk { return absl InternalError "Unable to allocate TfLite tensors" ; } for int i 0; i < inputs size ; ++i { if interpreter->tensor interpreter->inputs [i] ->type ! kTfLiteFloat32 { return absl InternalError "input data type is not float32" ; } float* tflite data interpreter->typed tensor<float> interpreter->inputs [i] ; if inputs[i] data size * sizeof float interpreter->tensor interpreter->inputs [i] ->bytes { return absl InternalError "input data size doesn t match for this input" ; } std memcpy tflite data inputs[i] data data inputs[i] data size * sizeof float ; } if interpreter->Invoke ! kTfLiteOk { return absl InternalError "Unable to invoke TfLite interpreter" ; } if !outputs || !outputs->empty { return absl InternalError "Invalid outputs pointer" ; } outputs->reserve interpreter->outputs size ; for auto t interpreter->outputs { const TfLiteTensor* out tensor interpreter->tensor t ; TensorFloat32 bhwc; bhwc id t; TODO impjdi Relax this condition to arbitrary batch size if out tensor->dims->data[0] ! 1 { return absl InternalError "Batch dimension is expected to be 1" ; } bhwc shape b out tensor->dims->data[0]; switch out tensor->dims->size { case 2 bhwc shape h 1; bhwc shape w 1; bhwc shape c out tensor->dims->data[1]; break; case 3 bhwc shape h 1; bhwc shape w out tensor->dims->data[1]; bhwc shape c out tensor->dims->data[2]; break; case 4 bhwc shape h out tensor->dims->data[1]; bhwc shape w out tensor->dims->data[2]; bhwc shape c out tensor->dims->data[3]; break; default return absl InternalError "Unsupported dimensions size " + std to string out tensor->dims->size ; } bhwc data std vector<float> out tensor->data f out tensor->data f + out tensor->bytes sizeof float ; outputs->push back bhwc ; } return absl OkStatus ; }
MatmulParameters MatmulParameters se StreamExecutor* stream exec DataType ab dtype DataType c dtype bool trans a bool trans b uint64 t m uint64 t n uint64 t k int64 t lda int64 t ldb int64 t ldc stream executor dnn ActivationMode activation mode int version device id stream exec->device ordinal { proto set ab dtype ab dtype ; proto set c dtype c dtype ; proto set trans a trans a ; proto set trans b trans b ; proto set m m ; proto set n n ; proto set k k ; proto set lda lda ; proto set ldb ldb ; proto set ldc ldc ; proto set activation mode activation mode ; Have to convert to std string because apparently our open-source protobuf does not speak absl string view proto set device identifier std string stream exec->GetDeviceDescription model str ; proto set version version ; hash code ComputeHash device id proto ; }
void Compute OpKernelContext* context override { const Tensor& logits in context->input 0 ; const Tensor& labels in context->input 1 ; TensorShape shape in logits in shape ; BCast bcast BCast FromShape logits in shape BCast FromShape labels in shape *fewer dims optimization * false ; if !logits in IsSameSize labels in { OP REQUIRES context bcast IsValid errors InvalidArgument "logits and labels must be broadcastable logits size " logits in shape DebugString " labels size " labels in shape DebugString ; shape in BCast ToShape bcast output shape ; } OP REQUIRES context TensorShapeUtils IsMatrix shape in errors InvalidArgument "logits and labels must be either " "2-dimensional or broadcasted to be " "2-dimensional" ; if std is same<Device GPUDevice> value { OP REQUIRES context !OpDeterminismRequired errors Unimplemented "The GPU implementation of SoftmaxCrossEntropyWithLogits" " that would have been executed is not deterministic " " Note that the Python API uses an alternative " " deterministic GPU-accelerated path when determinism is" " enabled " ; } loss is 1-D one per example and size is batch size Tensor scratch; if std is same<Device CPUDevice> value { OP REQUIRES OK context context->allocate temp DataTypeToEnum<T> value TensorShape {shape in dim size 0 shape in dim size 1 } &scratch ; } else { OP REQUIRES OK context context->allocate temp DataTypeToEnum<T> value TensorShape {shape in dim size 0 1} &scratch ; } Tensor* loss out nullptr; OP REQUIRES OK context context->allocate output 0 TensorShape {shape in dim size 0 } &loss out ; Tensor* back out nullptr; Try to reuse the logits in buffer for the backprop output OP REQUIRES OK context context->forward input or allocate output {0} 1 shape in &back out ; if shape in dim size 0 < 0 return; functor XentFunctor<Device T> functor; functor context->eigen device<Device> shape in AsEigenDSizes<2> BCast ToIndexArray<2> bcast x bcast BCast ToIndexArray<2> bcast y bcast logits in template shaped<T 2> bcast x reshape labels in template shaped<T 2> bcast y reshape scratch matrix<T> loss out->vec<T> back out->matrix<T> ; }
def testInputGradientKernelSizeMatchesInputSize self for data format use gpu in GetTestConfigs self ConstructAndTestGradient batch 2 input rows 4 input cols 3 filter rows 4 filter cols 3 in depth 2 out depth 3 stride rows 1 stride cols 1 padding "VALID" test input True data format data format use gpu use gpu max err 0 005 if test is built with rocm else 0 003
ThreadPool ThreadPool Env* env const ThreadOptions& thread options const string& name int num threads bool low latency hint Eigen Allocator* allocator { CHECK GE num threads 1 ; #ifdef DNNL AARCH64 USE ACL if num threads std thread hardware concurrency && num threads > 16 { num threads num threads - 1; } #endif DNNL AARCH64 USE ACL #ifdef TENSORFLOW THREADSCALING EXPERIMENTAL CHECK GT absl GetFlag FLAGS tensorflow num threads scale factor 0 ; num threads * absl GetFlag FLAGS tensorflow num threads scale factor ; if num threads < 1 num threads 1; #endif TENSORFLOW THREADSCALING EXPERIMENTAL eigen threadpool reset new Eigen ThreadPoolTempl<EigenEnvironment> num threads low latency hint EigenEnvironment env thread options "tf " + name ; underlying threadpool eigen threadpool get ; threadpool device reset new Eigen ThreadPoolDevice underlying threadpool num threads allocator ; }
ThreadPool ThreadPool Env* env const ThreadOptions& thread options const string& name int num threads bool low latency hint Eigen Allocator* allocator { CHECK GE num threads 1 ; #ifdef DNNL AARCH64 USE ACL if num threads std thread hardware concurrency && num threads > 16 { num threads num threads - 1; } #endif DNNL AARCH64 USE ACL #ifdef TENSORFLOW THREADSCALING EXPERIMENTAL CHECK GT absl GetFlag FLAGS tensorflow num threads scale factor 0 ; num threads * absl GetFlag FLAGS tensorflow num threads scale factor ; if num threads < 1 num threads 1; #endif TENSORFLOW THREADSCALING EXPERIMENTAL eigen threadpool reset new Eigen ThreadPoolTempl<EigenEnvironment> num threads low latency hint EigenEnvironment env thread options "tf " + name ; underlying threadpool eigen threadpool get ; threadpool device reset new Eigen ThreadPoolDevice underlying threadpool num threads allocator ; }
void RunTest { using tensorflow ops Placeholder; std vector<string> activations {"Relu" "Relu6" "Elu" "LeakyRelu"}; if IsMKLEnabled activations push back "Tanh" ; for const string& activation activations { if DTYPE DT HALF && activation ! "Relu" continue; tensorflow Scope s tensorflow Scope NewRootScope ; auto lhs shape ops Placeholder Shape {8 32} ; auto rhs shape ops Placeholder Shape {32 64} ; auto bias shape ops Placeholder Shape {64} ; auto lhs Placeholder s WithOpName "lhs" DTYPE lhs shape ; auto rhs Placeholder s WithOpName "rhs" DTYPE rhs shape ; auto bias Placeholder s WithOpName "bias" DTYPE bias shape ; auto matmul ops MatMul s WithOpName "matmul" lhs rhs ; auto bias add ops BiasAdd s WithOpName "bias add" matmul bias ; float leakyrelu alpha 0 5; ops Identity fetch [&] -> ops Identity { auto activate s WithOpName "activation" ; auto fetch s WithOpName "fetch" ; if activation "Relu" { return ops Identity fetch ops Relu activate bias add ; } else if activation "Relu6" { return ops Identity fetch ops Relu6 activate bias add ; } else if activation "Elu" { return ops Identity fetch ops Elu activate bias add ; } else if IsMKLEnabled && activation "Tanh" { return ops Identity fetch ops Tanh activate bias add ; } else if activation "LeakyRelu" { auto attr ops internal LeakyRelu Alpha leakyrelu alpha ; return ops Identity fetch ops internal LeakyRelu activate bias add attr ; } return ops Identity fetch bias ; } ; auto lhs t GenerateTensorWithSetRandom<DTYPE> {8 32} ; auto rhs t GenerateTensorWithSetRandom<DTYPE> {32 64} ; auto bias t GenerateTensorWithSetRandom<DTYPE> {64} ; GrapplerItem item; item fetch {"fetch"}; item feed {{"lhs" lhs t} {"rhs" rhs t} {"bias" bias t}}
def get facet counts self pk attname filtered qs original value self used parameters get self parameter name counts {} for i choice in enumerate self lookup choices self used parameters[self parameter name] choice[0] lookup qs self queryset self request filtered qs if lookup qs is not None counts[f"{i} c"] models Count pk attname filter models Q pk in lookup qs self used parameters[self parameter name] original value return counts
def check pattern startswith slash self """ Check that the pattern does not begin with a forward slash """ if not settings APPEND SLASH # Skip check as it can be useful to start a URL pattern with a slash # when APPEND SLASH False return [] if self regex startswith " " "^ " "^\\ " and not self regex endswith " " warning Warning "Your URL pattern {} has a route beginning with a Remove this " "slash as it is unnecessary If this pattern is targeted in an " "include ensure the include pattern has a trailing " format self describe id "urls W002" return [warning] else return []
def get facet counts self pk attname filtered qs original value self used parameters get self parameter name counts {} for i choice in enumerate self lookup choices self used parameters[self parameter name] choice[0] lookup qs self queryset self request filtered qs if lookup qs is not None counts[f"{i} c"] models Count pk attname filter models Q pk in lookup qs self used parameters[self parameter name] original value return counts
def setUpTestData cls cls user pass models User objects create username "joe" password "qwerty" cls user deny models User objects create username "jim" password "qwerty" models Group objects create name "Joe group" # Add permissions auth add customuser and auth change customuser perms models Permission objects filter codename in "add customuser" "change customuser" cls user pass user permissions add *perms
async def aclear expired cls pass
async def aclear expired cls pass
def test custom bad request template self response self client get " raises400 " self assertIs response wsgi request response context request
def is collapsible self if any self formset errors return False return "collapse" in self classes
def create options self options self import options self browser if self headless match self browser case "chrome" | "edge" options add argument "--headless new" case "firefox" options add argument "-headless"
def create options self options self import options self browser if self headless match self browser case "chrome" | "edge" options add argument "--headless new" case "firefox" options add argument "-headless"
def exists self limit True q self clone if not q distinct and q is sliced if q group by is True q add fields f attname for f in self model meta concrete fields False # Disable GROUP BY aliases to avoid orphaning references to the # SELECT clause which is about to be cleared q set group by allow aliases False q clear select clause if q combined queries and q combinator "union" q combined queries tuple combined query exists limit False for combined query in q combined queries q clear ordering force True if limit is True q set limits high 1 q add annotation Value 1 "a" return q
def test migrate test setting false self mocked sync apps mocked migrate *mocked objects test connection get connection copy test connection settings dict[ TEST ][ MIGRATE ] False creation test connection creation class test connection if connection vendor oracle # Don t close connection on Oracle creation connection close mock Mock old database name test connection settings dict[ NAME ] try with mock patch object creation create test db creation create test db verbosity 0 autoclobber True serialize False # Migrations don t run mocked migrate assert called args kwargs mocked migrate call args self assertEqual args [] self assertEqual kwargs[ plan ] [] # App is synced mocked sync apps assert called mocked args mocked sync apps call args self assertEqual mocked args[1] { app unmigrated } finally with mock patch object creation destroy test db creation destroy test db old database name verbosity 0
def test sparse inputs self from keras utils module utils import tensorflow as tf x tf sparse from dense np array [[-1 0 0 2 0 7 1 2]] layer layers Discretization bin boundaries [0 0 0 5 1 0] output layer x self assertTrue backend is tensor output self assertAllClose output np array [[0 1 2 3]]
def standardize dtype dtype if dtype is None return config floatx dtype PYTHON DTYPES MAP get dtype dtype if hasattr dtype "name" dtype dtype name elif hasattr dtype " str " and "torch" in str dtype or "jax numpy" in str dtype dtype str dtype split " " [-1] elif hasattr dtype " str " and " DimExpr" in str dtype return config floatx elif hasattr dtype " name " dtype dtype name if dtype not in ALLOWED DTYPES raise ValueError f"Invalid dtype {dtype}" return dtype
def build keras model keras module num classes input shape 28 28 1 model keras module Sequential [ keras module Input shape input shape keras module layers Conv2D 32 kernel size 3 3 activation "relu" keras module layers BatchNormalization keras module layers MaxPooling2D pool size 2 2 keras module layers Conv2D 64 kernel size 3 3 activation "relu" # TODO Renable the following line # keras module layers BatchNormalization scale False center True keras module layers MaxPooling2D pool size 2 2 keras module layers Flatten keras module layers Dense num classes activation "softmax" ] model summary return model
def stateless call self trainable variables non trainable variables *args return losses False **kwargs """Call the layer without any side effects Args trainable variables List of trainable variables of the model non trainable variables List of non-trainable variables of the model *args Positional arguments to be passed to `call ` return losses If `True` `stateless call ` will return the list of losses created during `call ` as part of its return values **kwargs Keyword arguments to be passed to `call ` Returns A tuple By default returns ` outputs non trainable variables ` If `return losses True` then returns ` outputs non trainable variables losses ` Note `non trainable variables` include not only non-trainable weights such as `BatchNormalization` statistics but also RNG seed state if there are any random operations part of the layer such as dropout and `Metric` state if there are any metrics attached to the layer These are all elements of state of the layer Example ```python model data trainable variables model trainable variables non trainable variables model non trainable variables # Call the model with zero side effects outputs non trainable variables model stateless call trainable variables non trainable variables data # Attach the updated state to the model # until you do this the model is still in its pre-call state for ref var value in zip model non trainable variables non trainable variables ref var assign value ``` """ self check super called if not self built raise ValueError f"To call stateless call {self class name } must be " "built i e its variables must have been already created " "You can build it by calling it on some data " if len trainable variables ! len self trainable variables raise ValueError "Argument `trainable variables` must be a list of tensors " "corresponding 1 1 to " f"{self class name } trainable variables " f"Received list with length {len trainable variables } " f"but expected {len self trainable variables } variables " if len non trainable variables ! len self non trainable variables raise ValueError "Argument `non trainable variables` must be a list of tensors " "corresponding 1 1 to " f"{self class name } non trainable variables " f"Received list with length {len non trainable variables } " f"but expected {len self non trainable variables } variables " # Gather variable mapping all variables map lambda v v value if isinstance v KerasVariable else v itertools chain trainable variables non trainable variables mapping zip self trainable variables + self non trainable variables all variables # Call in stateless scope losses None with backend StatelessScope state mapping mapping collect losses return losses as scope if isinstance self dtype policy dtype policies QuantizedDTypePolicy outputs self quantized call *args **kwargs else outputs self call *args **kwargs if return losses losses self losses # Gather updated non-trainable variables non trainable variables [] for v in self non trainable variables new v scope get current value v if new v is not None non trainable variables append new v else non trainable variables append v if return losses return outputs non trainable variables losses return outputs non trainable variables
def distribute tensor tensor layout """Distribute the tensor based on the layout Note that this function can be used both in eager context or within a jitted function Args tensor `jax Array` that need to be distributed layout `TensorLayout` for the distribution information or a `jax sharding Sharding` instance Returns Distributed value """ from keras utils import jax utils if not isinstance layout jax sharding Sharding layout to jax layout layout # TODO scottzhu This might not be a cheap check we should consider # have some proper JAX API for doing this check if jax utils is in jax tracing scope return jax lax with sharding constraint tensor layout if layout is fully addressable return jax device put tensor layout else # Need to only distribute the value to local addressible devices and # repack them back into global format mapping layout addressable devices indices map tensor shape local values jax device put [tensor[i] for i in mapping values ] list mapping keys global value jax make array from single device arrays tensor shape layout local values return global value
def global seed generator from keras utils import jax utils if jax utils is in jax tracing scope raise ValueError "[JAX RNG] When tracing a JAX function " "you should only use seeded random ops e g " "you should create a `SeedGenerator` instance attach it " "to your layer model and pass the instance as the `seed` " "argument when calling random ops Unseeded random ops " "would get incorrectly traced by JAX and would become constant " "after tracing Example \n\n" "```\n" "# Make sure to set the seed generator as a layer attribute\n" "self seed generator keras random SeedGenerator seed 1337 \n" " \n" "out keras random normal shape 1 seed self seed generator \n" "```" gen global state get global attribute "global seed generator" if gen is None gen SeedGenerator global state set global attribute "global seed generator" gen return gen
def quantize self mode import gc # Prevent quantization of the subclasses if type self is not Dense raise NotImplementedError f"Layer {self class name } does not have a `quantize ` " "method implemented " self check quantize args mode self compute dtype if mode "int8" if backend standardize dtype self kernel dtype "int8" raise ValueError "`quantize` can only be done once per layer " # Configure `self inputs quantizer` self inputs quantizer quantizers AbsMaxQuantizer axis -1 # Quantize `self kernel` to int8 and compute corresponding scale kernel value kernel scale quantizers abs max quantize self kernel axis 0 kernel scale ops squeeze kernel scale axis 0 self tracker unlock self untrack variable self kernel kernel shape self kernel shape del self kernel self kernel self add weight name "kernel" shape kernel shape # Prevent adding a large constant to the computation graph initializer lambda shape dtype kernel value dtype "int8" trainable False self kernel scale self add weight name "kernel scale" shape self units # Prevent adding a large constant to the computation graph initializer lambda shape dtype kernel scale trainable False self tracker lock else NotImplementedError "Invalid quantization mode Expected int8 " f"Received mode {mode}" # Set new dtype policy if not isinstance self dtype policy dtype policies QuantizedDTypePolicy quantized dtype f"{mode} from {self dtype policy name}" self dtype policy dtype policies get quantized dtype # Release memory manually because sometimes the backend doesn t gc collect
def dice y true y pred """Computes the Dice loss value between `y true` and `y pred` Formula ```python loss 1 - 2 * sum y true * y pred sum y true + sum y pred ``` Args y true tensor of true targets y pred tensor of predicted targets Returns Dice loss value """ y true ops convert to tensor y true y pred ops convert to tensor y pred inputs ops reshape y true [-1] targets ops reshape y pred [-1] intersection ops sum ops dot inputs targets dice ops divide 2 0 * intersection ops sum y true + ops sum y pred + backend epsilon return 1 - dice
def test export archive errors self temp filepath os path join self get temp dir "exported model" model models Sequential [layers Dense 2 ] model tf random normal 2 3 # Endpoint name reuse export archive export lib ExportArchive export archive track model export archive add endpoint "call" model call input signature [tf TensorSpec shape None 3 dtype tf float32 ] with self assertRaisesRegex ValueError "already taken" export archive add endpoint "call" model call input signature [ tf TensorSpec shape None 3 dtype tf float32 ] # Write out with no endpoints export archive export lib ExportArchive export archive track model with self assertRaisesRegex ValueError "No endpoints have been set" export archive write out temp filepath # Invalid object type with self assertRaisesRegex ValueError "Invalid resource type" export archive export lib ExportArchive export archive track "model" # Set endpoint with no input signature export archive export lib ExportArchive export archive track model with self assertRaisesRegex ValueError "you must provide an `input signature`" export archive add endpoint "call" model call # Set endpoint that has never been called export archive export lib ExportArchive export archive track model @tf function def my endpoint x return model x export archive export lib ExportArchive export archive track model with self assertRaisesRegex ValueError "you must either provide a function" export archive add endpoint "call" my endpoint
def track variables self self torch params torch nn ParameterDict {variable path variable value for variable in self variables}
def convert to tensor x dtype None sparse None if sparse raise ValueError "`sparse True` is not supported with mlx backend" mlx dtype to mlx dtype dtype if dtype is not None else None if is tensor x if dtype is None return x return x astype mlx dtype if isinstance x Variable if dtype and standardize dtype dtype ! x dtype return x value astype mlx dtype return x value if isinstance x np ndarray x x astype standardize dtype x dtype return mx array x dtype mlx dtype if isinstance x list def to scalar list x if isinstance x list return [to scalar list xi for xi in x] elif isinstance x mx array if x ndim 0 return x item else return x tolist else return x return mx array to scalar list x dtype mlx dtype return mx array x dtype mlx dtype
def init self inputs outputs name None super init name name if backend "tensorflow" # Temporary work around for # https github com keras-team keras issues 931 # This stop tensorflow from wrapping tf function output in a # DictWrapper object self setattr tracking getattr self " self setattr tracking" True self self setattr tracking False self inputs struct tree map structure lambda x x inputs self outputs struct tree map structure lambda x x outputs self inputs tree flatten inputs self outputs tree flatten outputs if not self inputs raise ValueError "`inputs` argument cannot be empty Received \n" f"inputs {inputs}\n" f"outputs {outputs}" if not self outputs raise ValueError "`outputs` argument cannot be empty Received \n" f"inputs {inputs}\n" f"outputs {outputs}" if backend "tensorflow" self self setattr tracking self setattr tracking nodes nodes by depth operations operations by depth map graph self inputs self outputs self nodes nodes self nodes by depth nodes by depth self operations operations self operations by depth operations by depth if backend "openvino" from keras src backend openvino core import OPENVINO DTYPES from keras src backend openvino core import get device import openvino as ov import openvino runtime opset14 as ov opset from openvino import Core # prepare OpenVINO parameters ov inputs [] for input in self inputs ov type OPENVINO DTYPES[ input dtype] ov shape input shape ov shape list ov shape for i in range len ov shape if ov shape[i] is None ov shape[i] -1 param ov opset parameter shape ov shape dtype ov type ov inputs append param pass # build OpenVINO graph - ov Model ov outputs self run through graph ov inputs operation fn lambda op op ov outputs tree flatten ov outputs self ov model ov Model results ov outputs parameters ov inputs self ov core Core self ov device get device self ov compiled model self ov core compile model self ov model self ov device pass
def compute loss and updates self trainable variables non trainable variables metrics variables x y sample weight training optimizer variables None """This method is stateless and is intended for use with jax grad """ kwargs {} if self call has training arg kwargs["training"] training # Run stateless forward pass y pred non trainable variables losses self stateless call trainable variables non trainable variables x return losses True **kwargs if losses # Make forward pass losses available to compute loss self losses override clear self losses override losses loss variables self stateless compute loss trainable variables non trainable variables metrics variables x x y y y pred y pred sample weight sample weight training training if losses self losses override clear trainable variables non trainable variables metrics variables variables # Handle loss scaling unscaled loss loss if training and self optimizer is not None # Scale loss with a StatelessScope to use an update scale variable mapping list zip self optimizer variables optimizer variables with backend StatelessScope state mapping mapping loss self optimizer scale loss loss return loss unscaled loss y pred non trainable variables metrics variables
def test softmax correctness with axis tuple self if backend backend "torch" pytest skip "unsupported tuple dim by torch" input np array [[[1 0 2 0] [3 0 4 0]] [[5 0 6 0] [7 0 8 0]]] combination combinations range 3 2 for axis in list combination result knn softmax input axis axis normalized sum by axis np sum result axis axis self assertAllClose normalized sum by axis 1 0
def l2 normalize x axis None epsilon 1e-12 x convert to tensor x return tf nn l2 normalize x axis axis epsilon epsilon
def init self filepath reference model None custom objects None self filepath filepath self custom objects custom objects self metadata None self reference model None self config None if filepath endswith " keras" self init for keras custom objects filepath reference model elif filepath endswith " weights h5" pass else raise ValueError "Invalid filename " "expected a ` keras` ` weights h5` extension " f"Received filepath {filepath}" def recursive search data result {} for key in data keys value data[key] if isinstance value h5py Group and len value 0 continue if hasattr value "keys" result[key] recursive search value else result[key] value return result archive zipfile ZipFile filepath "r" weights store H5IOStore VARS FNAME + " h5" archive archive mode "r" self nested dict recursive search weights store h5 file
def get nested variables self trainable variables False non trainable variables False optimizer variables False metrics variables False """Retrieves tree-like structure of specified variables of the model This method allows selective retrieval of different variables trainable non-trainable optimizer and metrics associated with the model The variables are returned in a nested dictionary format where the keys correspond to the variable names and the values are the nested representations of the variables Args trainable variables Whether to include trainable variables non trainable variables Whether to include non-trainable variables optimizer variables Whether to include optimizer variables metrics variables Whether to include metrics variables Returns dict A dictionary containing the nested representations of the requested variables The keys are the variable names and the values are the corresponding nested dictionaries If no variable types are requested an empty dictionary is returned """ variables {} if trainable variables variables["trainable variables"] self create nested dict self trainable variables if non trainable variables variables["non trainable variables"] self create nested dict self non trainable variables if optimizer variables variables["optimizer variables"] self create nested dict self optimizer variables if metrics variables variables["metrics variables"] self create nested dict self metrics variables return variables
def get nested variables self trainable variables False non trainable variables False optimizer variables False metrics variables False """Retrieves tree-like structure of specified variables of the model This method allows selective retrieval of different variables trainable non-trainable optimizer and metrics associated with the model The variables are returned in a nested dictionary format where the keys correspond to the variable names and the values are the nested representations of the variables Args trainable variables Whether to include trainable variables non trainable variables Whether to include non-trainable variables optimizer variables Whether to include optimizer variables metrics variables Whether to include metrics variables Returns dict A dictionary containing the nested representations of the requested variables The keys are the variable names and the values are the corresponding nested dictionaries If no variable types are requested an empty dictionary is returned """ variables {} if trainable variables variables["trainable variables"] self create nested dict self trainable variables if non trainable variables variables["non trainable variables"] self create nested dict self non trainable variables if optimizer variables variables["optimizer variables"] self create nested dict self optimizer variables if metrics variables variables["metrics variables"] self create nested dict self metrics variables return variables
def init self seed None name None **kwargs if name is None name auto name self class name self name name custom backend kwargs pop "backend" None if kwargs raise ValueError f"Unrecognized keyword arguments {kwargs}" if custom backend is not None self backend custom backend else self backend backend self initial seed seed if seed is None seed make default seed if not isinstance seed int raise ValueError "Argument `seed` must be an integer " f"Received seed {seed}" def seed initializer *args **kwargs dtype kwargs get "dtype" None return self backend convert to tensor [seed 0] dtype dtype with backend name scope self name caller self self state self backend BackendVariable seed initializer shape 2 dtype self backend random seed dtype trainable False name "seed generator state"
def call self y true y pred y true y pred optree tree transpose map squeeze or expand to same rank y true y pred return self fn y true y pred **self fn kwargs
def test TensorBoard projector callback self model models Sequential [ layers Input 10 layers Embedding 10 10 name "test embedding" layers Dense 1 activation "sigmoid" ] model compile optimizer "adam" loss losses BinaryCrossentropy from logits True x y np ones 10 10 np ones 10 10 1 logdir self get log dirs tb cbk callbacks TensorBoard logdir embeddings freq 1 embeddings metadata {"test embedding" "metadata tsv"} model fit x y batch size 2 epochs 2 validation data x y callbacks [tb cbk] with open os path join logdir "projector config pbtxt" as f self assertEqual f readlines [ "embeddings {\n" " tensor name " "layer with weights-0 embeddings ATTRIBUTES VARIABLE VALUE"\n metadata path "metadata tsv"\n "}\n" ]
def call self data training True transformation self get random transformation data training training if isinstance data dict is batched self is batched data["images"] if is batched data["images"] self transform images self backend convert to tensor data["images"] transformation transformation training training else data["images"] self transform single image self backend convert to tensor data["images"] transformation transformation training training if "bounding boxes" in data if not self bounding box format raise ValueError "You passed an input with a bounding boxes key " "but you didn t specify a bounding box format " "Pass a `bounding box format` argument to your " f"{self class name } layer e g " "`bounding box format xyxy ` " bounding boxes densify bounding boxes data["bounding boxes"] is batched is batched backend self backend if "orig width" not in data raise ValueError " orig width key is missing from the input data " "Please provide the original image width " if "orig height" not in data raise ValueError " orig height key is missing from the input data " "Please provide the original image width " if is batched orig width self backend numpy expand dims data["orig width"] axis -1 orig height self backend numpy expand dims data["orig height"] axis -1 data["bounding boxes"] self transform bounding boxes bounding boxes orig height orig width transformation transformation training training else orig width self backend numpy expand dims [data["orig width"]] axis -1 orig height self backend numpy expand dims [data["orig height"]] axis -1 data["bounding boxes"] self transform single bounding box bounding boxes orig height orig width transformation transformation training training if "labels" in data if is batched data["labels"] self transform labels self backend convert to tensor data["labels"] transformation transformation training training else data["labels"] self transform single label self backend convert to tensor data["labels"] transformation transformation training training if "segmentation masks" in data if is batched data["segmentation masks"] self transform segmentation masks data["segmentation masks"] transformation transformation training training else data["segmentation masks"] self transform single segmentation mask data["segmentation masks"] transformation transformation training training return data # `data` is just images if self is batched data return self transform images self backend convert to tensor data transformation transformation training training return self transform single image self backend convert to tensor data transformation transformation training training
def call self y true y pred sample weight None try tree assert same structure y pred y true check types False except ValueError # y true is either # - flat or leaf # - has the same structure but uses different but reconcilable # container types e g `list` vs `tuple` flat y true tree flatten y true try y true tree pack sequence as y pred flat y true except y true struct tree map structure lambda "*" y true y pred struct tree map structure lambda "*" y pred raise ValueError "y true and y pred have different structures \n" f"y true {y true struct}\n" f"y pred {y pred struct}\n" if not self built self build y true y pred try tree assert same structure self y pred build structure y pred check types False except ValueError y pred tree pack sequence as self y pred build structure tree flatten y pred y true tree pack sequence as self y pred build structure tree flatten y true # We need to add a dummy `None` if the model has only a single output metrics [None] if len self metrics 0 else self metrics # Iterate all losses in flat form loss values [] def resolve path path object for path in path object object[ path] return object for path loss fn loss weight name metric in zip self flat losses metrics y t y p resolve path path y true resolve path path y pred if sample weight is not None and tree is nested sample weight sample weight resolve path path sample weight else sample weight sample weight value ops cast loss fn y t y p sample weight dtype self dtype if loss weight is not None value ops multiply value loss weight loss values append value # Record individual losses if metric metric update state value sample weight tree flatten y p [0] shape[0] if loss values total loss sum loss values return total loss return None
def format layer shape layer if not layer inbound nodes and not layer build shapes dict return "?" def format shape shape highlighted [highlight number x for x in shape] return " " + " " join highlighted + " " # There are 2 approaches to get output shapes # 1 Using `layer inbound nodes` which is possible if the model is a # Sequential or Functional # 2 Using `layer build shapes dict` which is possible if users manually # build the layer if len layer inbound nodes > 0 for i in range len layer inbound nodes outputs layer inbound nodes[i] output tensors output shapes tree map structure lambda x format shape x shape outputs else try if hasattr layer "output shape" output shapes layer output shape else outputs layer compute output shape **layer build shapes dict output shapes tree map shape structure lambda x format shape x outputs except NotImplementedError return "?" if len output shapes 1 return output shapes[0] out str output shapes out out replace " " "" return out
def custom histogram fixed width self values value range nbins values self backend cast values "float32" value min value max value range value min self backend cast value min "float32" value max self backend cast value max "float32" scaled values - value min * nbins - 1 value max - value min indices self backend cast scaled "int32" indices self backend numpy clip indices 0 nbins - 1 if backend backend "jax" # for JAX bincount is never jittable because of output shape flat indices self backend numpy reshape indices [-1] one hot self backend numpy eye nbins [flat indices] histogram self backend numpy sum one hot axis 0 else # TensorFlow PyTorch NumPy implementation using bincount flat indices self backend numpy reshape indices [-1] histogram self backend numpy bincount flat indices minlength nbins return histogram
def get random transformation self data training True seed None if isinstance data dict images data["images"] else images data images shape self backend shape images if len images shape 3 batch size 1 else batch size self backend shape images [0] if seed is None seed self get seed generator self backend backend permutation order self backend random shuffle self backend numpy arange 0 batch size seed seed lambda sample self sample from beta self alpha self alpha batch size seed seed return { "lambda sample" lambda sample "permutation order" permutation order }
def class weight to sample weights y class weight sample weight np ones shape y shape[0] dtype backend floatx if len y shape > 1 if y shape[-1] ! 1 y np argmax y axis -1 else y np squeeze y axis -1 y np round y if hasattr y "astype" y y astype "int32" else # must be a torch tensor object import torch y y to torch int32 for i in range y shape[0] sample weight[i] class weight get int y[i] 1 0 return sample weight
def fit self x None y None batch size None epochs 1 verbose "auto" callbacks None validation split 0 0 validation data None shuffle True class weight None sample weight None initial epoch 0 steps per epoch None validation steps None validation batch size None validation freq 1 raise NotImplementedError "`fit` is not supported with openvino backend"
def test PowerNorm # Check an exponent of 1 gives same results as a normal linear # normalization Also implicitly checks that vmin vmax are # automatically initialized from first array input a np array [0 0 5 1 1 5] dtype float pnorm mcolors PowerNorm 1 norm mcolors Normalize assert array almost equal norm a pnorm a a np array [-0 5 0 2 4 8] dtype float expected [-1 16 0 1 16 1 4 1] pnorm mcolors PowerNorm 2 vmin 0 vmax 8 assert array almost equal pnorm a expected assert pnorm a[0] expected[0] assert pnorm a[2] expected[2] assert array almost equal a[1 ] pnorm inverse pnorm a [1 ] # Clip True a np array [-0 5 0 1 8 16] dtype float expected [0 0 0 1 1] # Clip True when creating the norm pnorm mcolors PowerNorm 2 vmin 2 vmax 8 clip True assert array almost equal pnorm a expected assert pnorm a[0] expected[0] assert pnorm a[-1] expected[-1] # Clip True at call time pnorm mcolors PowerNorm 2 vmin 2 vmax 8 clip False assert array almost equal pnorm a clip True expected assert pnorm a[0] clip True expected[0] assert pnorm a[-1] clip True expected[-1] # Check clip True preserves masked values a np ma array [5 2] mask [True False] out pnorm a clip True assert array equal out mask [True False]
def test powernorm cbar limits fig ax plt subplots vmin vmax 300 1000 data np arange 100*100 reshape 100 100 + vmin im ax imshow data norm mcolors PowerNorm gamma 0 2 vmin vmin vmax vmax cbar fig colorbar im assert cbar ax get ylim vmin vmax
def streamplot axes x y u v density 1 linewidth None color None cmap None norm None arrowsize 1 arrowstyle -|> minlength 0 1 transform None zorder None start points None maxlength 4 0 integration direction both broken streamlines True * n arrows 1 """ Draw streamlines of a vector flow Parameters ---------- x y 1D 2D arrays Evenly spaced strictly increasing arrays to make a grid If 2D all rows of *x* must be equal and all columns of *y* must be equal; i e they must be as if generated by ``np meshgrid x 1d y 1d `` u v 2D arrays *x* and *y*-velocities The number of rows and columns must match the length of *y* and *x* respectively density float or float float Controls the closeness of streamlines When ``density 1`` the domain is divided into a 30x30 grid *density* linearly scales this grid Each cell in the grid can have at most one traversing streamline For different densities in each direction use a tuple density x density y linewidth float or 2D array The width of the streamlines With a 2D array the line width can be varied across the grid The array must have the same shape as *u* and *v* color color or 2D array The streamline color If given an array its values are converted to colors using *cmap* and *norm* The array must have the same shape as *u* and *v* cmap norm Data normalization and colormapping parameters for *color*; only used if *color* is an array of floats See `~ Axes imshow` for a detailed description arrowsize float Scaling factor for the arrow size arrowstyle str Arrow style specification See `~matplotlib patches FancyArrowPatch` minlength float Minimum length of streamline in axes coordinates start points N 2 array Coordinates of starting points for the streamlines in data coordinates the same coordinates as the *x* and *y* arrays zorder float The zorder of the streamlines and arrows Artists with lower zorder values are drawn first maxlength float Maximum length of streamline in axes coordinates integration direction { forward backward both } default both Integrate the streamline in forward backward or both directions data indexable object optional DATA PARAMETER PLACEHOLDER broken streamlines boolean default True If False forces streamlines to continue until they leave the plot domain If True they may be terminated if they come too close to another streamline n arrows int Number of arrows per streamline The arrows are spaced equally along the steps each streamline takes Note that this can be different to being spaced equally along the distance of the streamline Returns ------- StreamplotSet Container object with attributes - ``lines`` ` LineCollection` of streamlines - ``arrows`` ` PatchCollection` containing ` FancyArrowPatch` objects representing the arrows half-way along streamlines This container will probably change in the future to allow changes to the colormap alpha etc for both lines and arrows but these changes should be backward compatible """ grid Grid x y mask StreamMask density dmap DomainMap grid mask if n arrows < 0 raise ValueError f"The value of n arrows must be > 0 got {n arrows }" if zorder is None zorder mlines Line2D zorder # default to data coordinates if transform is None transform axes transData if color is None color axes get lines get next color if linewidth is None linewidth mpl rcParams[ lines linewidth ] line kw {} arrow kw dict arrowstyle arrowstyle mutation scale 10 * arrowsize api check in list [ both forward backward ] integration direction integration direction if integration direction both maxlength 2 use multicolor lines isinstance color np ndarray if use multicolor lines if color shape ! grid shape raise ValueError "If color is given it must match the shape of " "the x y grid" line colors [[]] # Empty entry allows concatenation of zero arrays color np ma masked invalid color else line kw[ color ] color arrow kw[ color ] color if isinstance linewidth np ndarray if linewidth shape ! grid shape raise ValueError "If linewidth is given it must match the " "shape of the x y grid" line kw[ linewidth ] [] else line kw[ linewidth ] linewidth arrow kw[ linewidth ] linewidth line kw[ zorder ] zorder arrow kw[ zorder ] zorder # Sanity checks if u shape ! grid shape or v shape ! grid shape raise ValueError " u and v must match the shape of the x y grid" u np ma masked invalid u v np ma masked invalid v integrate get integrator u v dmap minlength maxlength integration direction trajectories [] if start points is None for xm ym in gen starting points mask shape if mask[ym xm] 0 xg yg dmap mask2grid xm ym t integrate xg yg broken streamlines if t is not None trajectories append t else sp2 np asanyarray start points dtype float copy # Check if start points are outside the data boundaries for xs ys in sp2 if not grid x origin < xs < grid x origin + grid width and grid y origin < ys < grid y origin + grid height raise ValueError f"Starting point {xs} {ys} outside of " "data boundaries" # Convert start points from data to array coords # Shift the seed points from the bottom left of the data so that # data2grid works properly sp2[ 0] - grid x origin sp2[ 1] - grid y origin for xs ys in sp2 xg yg dmap data2grid xs ys # Floating point issues can cause xg yg to be slightly out of # bounds for xs ys on the upper boundaries Because we have # already checked that the starting points are within the original # grid clip the xg yg to the grid to work around this issue xg np clip xg 0 grid nx - 1 yg np clip yg 0 grid ny - 1 t integrate xg yg broken streamlines if t is not None trajectories append t if use multicolor lines if norm is None norm mcolors Normalize color min color max cmap cm ensure cmap cmap streamlines [] arrows [] for t in trajectories tgx tgy t T # Rescale from grid-coordinates to data-coordinates tx ty dmap grid2data tgx tgy tx + grid x origin ty + grid y origin # Create multiple tiny segments if varying width or color is given if isinstance linewidth np ndarray or use multicolor lines points np transpose [tx ty] reshape -1 1 2 streamlines extend np hstack [points[ -1] points[1 ]] else points np transpose [tx ty] streamlines append points # Distance along streamline s np cumsum np hypot np diff tx np diff ty if use multicolor lines color values interpgrid color tgx tgy [ -1] line colors append color values # Add arrows along each trajectory for x in range 1 n arrows+1 # Get index of distance along streamline to place arrow idx np searchsorted s s[-1] * x n arrows+1 arrow tail tx[idx] ty[idx] arrow head np mean tx[idx idx + 2] np mean ty[idx idx + 2] if isinstance linewidth np ndarray line widths interpgrid linewidth tgx tgy [ -1] line kw[ linewidth ] extend line widths arrow kw[ linewidth ] line widths[idx] if use multicolor lines arrow kw[ color ] cmap norm color values[idx] p patches FancyArrowPatch arrow tail arrow head transform transform **arrow kw arrows append p lc mcollections LineCollection streamlines transform transform **line kw lc sticky edges x[ ] [grid x origin grid x origin + grid width] lc sticky edges y[ ] [grid y origin grid y origin + grid height] if use multicolor lines lc set array np ma hstack line colors lc set cmap cmap lc set norm norm axes add collection lc ac mcollections PatchCollection arrows # Adding the collection itself is broken; see #2341 for p in arrows axes add patch p axes autoscale view stream container StreamplotSet lc ac return stream container
def streamplot axes x y u v density 1 linewidth None color None cmap None norm None arrowsize 1 arrowstyle -|> minlength 0 1 transform None zorder None start points None maxlength 4 0 integration direction both broken streamlines True * n arrows 1 """ Draw streamlines of a vector flow Parameters ---------- x y 1D 2D arrays Evenly spaced strictly increasing arrays to make a grid If 2D all rows of *x* must be equal and all columns of *y* must be equal; i e they must be as if generated by ``np meshgrid x 1d y 1d `` u v 2D arrays *x* and *y*-velocities The number of rows and columns must match the length of *y* and *x* respectively density float or float float Controls the closeness of streamlines When ``density 1`` the domain is divided into a 30x30 grid *density* linearly scales this grid Each cell in the grid can have at most one traversing streamline For different densities in each direction use a tuple density x density y linewidth float or 2D array The width of the streamlines With a 2D array the line width can be varied across the grid The array must have the same shape as *u* and *v* color color or 2D array The streamline color If given an array its values are converted to colors using *cmap* and *norm* The array must have the same shape as *u* and *v* cmap norm Data normalization and colormapping parameters for *color*; only used if *color* is an array of floats See `~ Axes imshow` for a detailed description arrowsize float Scaling factor for the arrow size arrowstyle str Arrow style specification See `~matplotlib patches FancyArrowPatch` minlength float Minimum length of streamline in axes coordinates start points N 2 array Coordinates of starting points for the streamlines in data coordinates the same coordinates as the *x* and *y* arrays zorder float The zorder of the streamlines and arrows Artists with lower zorder values are drawn first maxlength float Maximum length of streamline in axes coordinates integration direction { forward backward both } default both Integrate the streamline in forward backward or both directions data indexable object optional DATA PARAMETER PLACEHOLDER broken streamlines boolean default True If False forces streamlines to continue until they leave the plot domain If True they may be terminated if they come too close to another streamline n arrows int Number of arrows per streamline The arrows are spaced equally along the steps each streamline takes Note that this can be different to being spaced equally along the distance of the streamline Returns ------- StreamplotSet Container object with attributes - ``lines`` ` LineCollection` of streamlines - ``arrows`` ` PatchCollection` containing ` FancyArrowPatch` objects representing the arrows half-way along streamlines This container will probably change in the future to allow changes to the colormap alpha etc for both lines and arrows but these changes should be backward compatible """ grid Grid x y mask StreamMask density dmap DomainMap grid mask if n arrows < 0 raise ValueError f"The value of n arrows must be > 0 got {n arrows }" if zorder is None zorder mlines Line2D zorder # default to data coordinates if transform is None transform axes transData if color is None color axes get lines get next color if linewidth is None linewidth mpl rcParams[ lines linewidth ] line kw {} arrow kw dict arrowstyle arrowstyle mutation scale 10 * arrowsize api check in list [ both forward backward ] integration direction integration direction if integration direction both maxlength 2 use multicolor lines isinstance color np ndarray if use multicolor lines if color shape ! grid shape raise ValueError "If color is given it must match the shape of " "the x y grid" line colors [[]] # Empty entry allows concatenation of zero arrays color np ma masked invalid color else line kw[ color ] color arrow kw[ color ] color if isinstance linewidth np ndarray if linewidth shape ! grid shape raise ValueError "If linewidth is given it must match the " "shape of the x y grid" line kw[ linewidth ] [] else line kw[ linewidth ] linewidth arrow kw[ linewidth ] linewidth line kw[ zorder ] zorder arrow kw[ zorder ] zorder # Sanity checks if u shape ! grid shape or v shape ! grid shape raise ValueError " u and v must match the shape of the x y grid" u np ma masked invalid u v np ma masked invalid v integrate get integrator u v dmap minlength maxlength integration direction trajectories [] if start points is None for xm ym in gen starting points mask shape if mask[ym xm] 0 xg yg dmap mask2grid xm ym t integrate xg yg broken streamlines if t is not None trajectories append t else sp2 np asanyarray start points dtype float copy # Check if start points are outside the data boundaries for xs ys in sp2 if not grid x origin < xs < grid x origin + grid width and grid y origin < ys < grid y origin + grid height raise ValueError f"Starting point {xs} {ys} outside of " "data boundaries" # Convert start points from data to array coords # Shift the seed points from the bottom left of the data so that # data2grid works properly sp2[ 0] - grid x origin sp2[ 1] - grid y origin for xs ys in sp2 xg yg dmap data2grid xs ys # Floating point issues can cause xg yg to be slightly out of # bounds for xs ys on the upper boundaries Because we have # already checked that the starting points are within the original # grid clip the xg yg to the grid to work around this issue xg np clip xg 0 grid nx - 1 yg np clip yg 0 grid ny - 1 t integrate xg yg broken streamlines if t is not None trajectories append t if use multicolor lines if norm is None norm mcolors Normalize color min color max cmap cm ensure cmap cmap streamlines [] arrows [] for t in trajectories tgx tgy t T # Rescale from grid-coordinates to data-coordinates tx ty dmap grid2data tgx tgy tx + grid x origin ty + grid y origin # Create multiple tiny segments if varying width or color is given if isinstance linewidth np ndarray or use multicolor lines points np transpose [tx ty] reshape -1 1 2 streamlines extend np hstack [points[ -1] points[1 ]] else points np transpose [tx ty] streamlines append points # Distance along streamline s np cumsum np hypot np diff tx np diff ty if use multicolor lines color values interpgrid color tgx tgy [ -1] line colors append color values # Add arrows along each trajectory for x in range 1 n arrows+1 # Get index of distance along streamline to place arrow idx np searchsorted s s[-1] * x n arrows+1 arrow tail tx[idx] ty[idx] arrow head np mean tx[idx idx + 2] np mean ty[idx idx + 2] if isinstance linewidth np ndarray line widths interpgrid linewidth tgx tgy [ -1] line kw[ linewidth ] extend line widths arrow kw[ linewidth ] line widths[idx] if use multicolor lines arrow kw[ color ] cmap norm color values[idx] p patches FancyArrowPatch arrow tail arrow head transform transform **arrow kw arrows append p lc mcollections LineCollection streamlines transform transform **line kw lc sticky edges x[ ] [grid x origin grid x origin + grid width] lc sticky edges y[ ] [grid y origin grid y origin + grid height] if use multicolor lines lc set array np ma hstack line colors lc set cmap cmap lc set norm norm axes add collection lc ac mcollections PatchCollection arrows # Adding the collection itself is broken; see #2341 for p in arrows axes add patch p axes autoscale view stream container StreamplotSet lc ac return stream container
def secondary xaxis self location * functions None transform None **kwargs """ Add a second x-axis to this `~ axes Axes` For example if we want to have a second scale for the data plotted on the xaxis % secax docstring s Examples -------- The main axis shows frequency and the secondary axis shows period plot fig ax plt subplots ax loglog range 1 360 5 range 1 360 5 ax set xlabel frequency [Hz] def invert x # 1 x with special treatment of x 0 x np array x astype float near zero np isclose x 0 x[near zero] np inf x[~near zero] 1 x[~near zero] return x # the inverse of 1 x is itself secax ax secondary xaxis top functions invert invert secax set xlabel Period [s] plt show To add a secondary axis relative to your data you can pass a transform to the new axis plot fig ax plt subplots ax plot range 0 5 range -1 4 # Pass ax transData as a transform to place the axis # relative to your data at y 0 secax ax secondary xaxis 0 transform ax transData """ if not location in [ top bottom ] or isinstance location Real raise ValueError secondary xaxis location must be either a float or "top" "bottom" secondary ax SecondaryAxis self x location functions transform **kwargs self add child axes secondary ax return secondary ax
def get siblings self a """Return all of the items joined with *a* including itself """ siblings self mapping get a None [a] return sorted siblings key self mapping get
def test nonuniform logscale axs plt figure figsize 3 3 subplots 3 for i in range 3 ax axs[i] im NonUniformImage ax im set data np arange 1 4 ** 2 np arange 1 4 ** 2 np arange 9 reshape 3 3 ax set xlim 1 9 ax set ylim 1 9 ax set axis off if i 1 ax set xscale "log" ax set yscale "log" if i 2 ax set xscale "log" base 2 ax set yscale "log" base 3 ax add image im
def test rotate """Test rotating using the left mouse button """ for roll in [0 30] fig plt figure ax fig add subplot 1 1 1 projection 3d ax view init 0 0 roll ax figure canvas draw # drag mouse horizontally to change orientation ax button press mock event ax button MouseButton LEFT xdata 0 ydata 0 ax on move mock event ax button MouseButton LEFT xdata 0 5*ax pseudo w ydata 0*ax pseudo h ax figure canvas draw assert np isclose ax elev roll assert np isclose ax azim -90 assert np isclose ax roll 0
def test rotate """Test rotating using the left mouse button """ for roll in [0 30] fig plt figure ax fig add subplot 1 1 1 projection 3d ax view init 0 0 roll ax figure canvas draw # drag mouse horizontally to change orientation ax button press mock event ax button MouseButton LEFT xdata 0 ydata 0 ax on move mock event ax button MouseButton LEFT xdata 0 5*ax pseudo w ydata 0*ax pseudo h ax figure canvas draw assert np isclose ax elev roll assert np isclose ax azim -90 assert np isclose ax roll 0
def get text width height descent self s prop ismath """ Get the width height and descent offset from the bottom to the baseline in display coords of the string *s* with ` FontProperties` *prop* Whitespace at the start and the end of *s* is included in the reported width """ # Note Using the ``bbox`` property of Text objects backend # implementers can check their bounding box calculations which impact # text layout fontsize prop get size in points if ismath TeX # todo handle properties return self get texmanager get text width height descent s fontsize renderer self dpi self points to pixels 72 if ismath dims self text2path mathtext parser parse s dpi prop return dims[0 3] # return width height descent flags self text2path get hinting flag font self text2path get font prop font set size fontsize dpi # the width and height of unrotated string font set text s 0 0 flags flags w h font get width height d font get descent w 64 0 # convert from subpixels h 64 0 d 64 0 return w h d
def eq self other if not isinstance other MultivarColormap return False if not len self len other return False for c0 c1 in zip self other if not c0 c1 return False if not all self rgba bad other rgba bad return False if not self combination mode other combination mode return False return True
def test hist color semantics kwargs patch face patch edge patches plt figure subplots hist [1 2 3] **kwargs # When the expected edgecolor is blue it corresponds to the # first color of the default color cycle # When the expected edgecolor is black it corresponds to the # patch edgecolor rcParam # When the expected facecolor is blue it corresponds to the # patch facecolor rcParam or the first color of the color cycle assert all mcolors same color p get facecolor patch face if patch face ! none else p get facecolor [3] 0 for p in patches assert all mcolors same color p get edgecolor patch edge if patch edge ! none else p get edgecolor [3] 0 for p in patches
def test hist color semantics kwargs patch face patch edge patches plt figure subplots hist [1 2 3] **kwargs # When the expected edgecolor is blue it corresponds to the # first color of the default color cycle # When the expected edgecolor is black it corresponds to the # patch edgecolor rcParam # When the expected facecolor is blue it corresponds to the # patch facecolor rcParam or the first color of the color cycle assert all mcolors same color p get facecolor patch face if patch face ! none else p get facecolor [3] 0 for p in patches assert all mcolors same color p get edgecolor patch edge if patch edge ! none else p get edgecolor [3] 0 for p in patches
def get macos fonts """Cache and list the font paths known to ``system profiler SPFontsDataType`` """ try d plistlib loads subprocess check output ["system profiler" "-xml" "SPFontsDataType"] except OSError subprocess CalledProcessError plistlib InvalidFileException return [] return [Path entry["path"] for entry in d[" items"]]
def test simplify closepoly # The values of the vertices in a CLOSEPOLY should always be ignored # in favor of the most recent MOVETO s vertex values path ref Path [ 0 0 1 0 1 1 40 50 ] [Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY] path test Path [ 0 0 1 0 1 1 np nan np nan ] [Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY] path ref simplified path ref cleaned simplify True path test simplified path test cleaned simplify True assert array equal path ref simplified vertices path test simplified vertices assert array equal path ref simplified codes path test simplified codes
def test simplify closepoly # The values of the vertices in a CLOSEPOLY should always be ignored # in favor of the most recent MOVETO s vertex values paths [Path [ 1 1 2 1 2 2 np nan np nan ] [Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY] Path [ 1 1 2 1 2 2 40 50 ] [Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY] ] expected path Path [ 1 1 2 1 2 2 1 1 1 1 0 0 ] [Path MOVETO Path LINETO Path LINETO Path LINETO Path LINETO Path STOP] for path in paths simplified path path cleaned simplify True assert array equal expected path vertices simplified path vertices assert array equal expected path codes simplified path codes # test that a compound path also works path Path [ 1 1 2 1 2 2 np nan np nan -1 0 -2 0 -2 1 np nan np nan ] [Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY Path MOVETO Path LINETO Path LINETO Path CLOSEPOLY] expected path Path [ 1 1 2 1 2 2 1 1 -1 0 -2 0 -2 1 -1 0 -1 0 0 0 ] [Path MOVETO Path LINETO Path LINETO Path LINETO Path MOVETO Path LINETO Path LINETO Path LINETO Path LINETO Path STOP] simplified path path cleaned simplify True assert array equal expected path vertices simplified path vertices assert array equal expected path codes simplified path codes
def prepare points self # Helper for drawing and hit testing transform self get transform offset trf self get offset transform offsets self get offsets paths self get paths if self have units paths [] for path in self get paths vertices path vertices xs ys vertices[ 0] vertices[ 1] xs self convert xunits xs ys self convert yunits ys paths append mpath Path np column stack [xs ys] path codes xs self convert xunits offsets[ 0] ys self convert yunits offsets[ 1] offsets np ma column stack [xs ys] if not transform is affine paths [transform transform path non affine path for path in paths] transform transform get affine if not offset trf is affine offsets offset trf transform non affine offsets # This might have changed an ndarray into a masked array offset trf offset trf get affine if np ma isMaskedArray offsets offsets offsets filled np nan # Changing from a masked array to nan-filled ndarray # is probably most efficient at this point return transform offset trf offsets paths
def draw self renderer # docstring inherited if renderer is not None self renderer renderer if not self get visible return if self get text return renderer open group text self get gid with self cm set text self get wrapped text bbox info descent self get layout renderer trans self get transform # don t use self get position here which refers to text # position in Text x y self x self y if np ma is masked x x np nan if np ma is masked y y np nan posx float self convert xunits x posy float self convert yunits y posx posy trans transform posx posy if np isnan posx or np isnan posy return # don t throw a warning here if not np isfinite posx or not np isfinite posy log warning "posx and posy should be finite values" return canvasw canvash renderer get canvas width height # Update the location and size of the bbox # ` patches FancyBboxPatch` and draw it if self bbox patch self update bbox position size renderer self bbox patch draw renderer gc renderer new gc gc set foreground self get color gc set alpha self get alpha gc set url self url gc set antialiased self antialiased self set gc clip gc angle self get rotation for line wh x y in info mtext self if len info 1 else None x x + posx y y + posy if renderer flipy y canvash - y clean line ismath self preprocess math line if self get path effects from matplotlib patheffects import PathEffectRenderer textrenderer PathEffectRenderer self get path effects renderer else textrenderer renderer if self get usetex textrenderer draw tex gc x y clean line self fontproperties angle mtext mtext else textrenderer draw text gc x y clean line self fontproperties angle ismath ismath mtext mtext gc restore renderer close group text self stale False
bool load handle src bool { if src is none { return true; } py object vertices src attr "vertices" ; py object codes src attr "codes" ; auto should simplify src attr "should simplify" cast<bool> ; auto simplify threshold src attr "simplify threshold" cast<double> ; if !value set vertices inc ref ptr codes inc ref ptr should simplify simplify threshold { throw py error already set ; } return true; }
def do constrained layout fig h pad w pad hspace None wspace None rect 0 0 1 1 compress False """ Do the constrained layout Called at draw time in ``figure constrained layout `` Parameters ---------- fig `~matplotlib figure Figure` ` Figure` instance to do the layout in h pad w pad float Padding around the Axes elements in figure-normalized units hspace wspace float Fraction of the figure to dedicate to space between the Axes These are evenly spread between the gaps between the Axes A value of 0 2 for a three-column layout would have a space of 0 1 of the figure width between each column If h wspace < h w pad then the pads are used instead rect tuple of 4 floats Rectangle in figure coordinates to perform constrained layout in [left bottom width height] each from 0-1 compress bool Whether to shift Axes so that white space in between them is removed This is useful for simple grids of fixed-aspect Axes e g a grid of images Returns ------- layoutgrid private debugging structure """ renderer fig get renderer # make layoutgrid tree layoutgrids make layoutgrids fig None rect rect if not layoutgrids[ hasgrids ] api warn external There are no gridspecs with layoutgrids Possibly did not call parent GridSpec with the "figure" keyword return for in range 2 # do the algorithm twice This has to be done because decorations # change size after the first re-position i e x yticklabels get # larger smaller This second reposition tends to be much milder # so doing twice makes things work OK # make margins for all the Axes and subfigures in the # figure Add margins for colorbars make layout margins layoutgrids fig renderer h pad h pad w pad w pad hspace hspace wspace wspace make margin suptitles layoutgrids fig renderer h pad h pad w pad w pad # if a layout is such that a columns or rows margin has no # constraints we need to make all such instances in the grid # match in margin size match submerged margins layoutgrids fig # update all the variables in the layout layoutgrids[fig] update variables warn collapsed constrained layout not applied because axes sizes collapsed to zero Try making figure larger or Axes decorations smaller if check no collapsed axes layoutgrids fig reposition axes layoutgrids fig renderer h pad h pad w pad w pad hspace hspace wspace wspace if compress layoutgrids compress fixed aspect layoutgrids fig layoutgrids[fig] update variables if check no collapsed axes layoutgrids fig reposition axes layoutgrids fig renderer h pad h pad w pad w pad hspace hspace wspace wspace else api warn external warn collapsed if suptitle fig suptitle is not None and suptitle get in layout and suptitle autopos x suptitle get position suptitle set position x layoutgrids[fig] get inner bbox y1 + 0 02 suptitle set verticalalignment bottom else api warn external warn collapsed reset margins layoutgrids fig return layoutgrids
def test zoom inset connector styles fig axs plt subplots 2 for ax in axs ax plot [1 2 3] axs[1] set xlim 0 5 1 5 indicator axs[0] indicate inset zoom axs[1] linewidth 5 for conn in indicator connectors if conn get visible # Make one visible connector a different style conn set linestyle dashed conn set color blue break
def set label self s """Assigning legend labels is supported Raises RuntimeError """ raise RuntimeError "A legend label cannot be assigned to an Axis Did you mean to " "set the axis label via set label text ?"
def test engformatter offset oom oom center oom noise oom center desired oom noise desired UNIT "eV" # Doesn t really matter here but should be of order of magnitude ~ 1 r range -5 7 fig ax plt subplots # Use some random ugly number data offset 2 7149*10**oom center ydata data offset + np array r dtype float *10**oom noise ax plot ydata formatter mticker EngFormatter useOffset True unit UNIT # So that offset strings will always have the same size formatter ENG PREFIXES[0] " " ax yaxis set major formatter formatter fig canvas draw offsetGot formatter get offset ticksGot [labl get text for labl in ax get yticklabels ] # Predicting whether offset should be 0 or not is essentially testing # ScalarFormatter compute offset This function is pretty complex and it # would be nice to test it but this is out of scope for this test which # only makes sure that offset text and the ticks gets the correct unit # prefixes and the ticks if formatter offset prefix noise got offsetGot[2] prefix noise desired formatter ENG PREFIXES[oom noise desired] prefix center got offsetGot[-1-len UNIT ] prefix center desired formatter ENG PREFIXES[oom center desired] assert prefix noise desired prefix noise got assert prefix center desired prefix center got # Make sure the ticks didn t get the UNIT for tick in ticksGot assert UNIT not in tick else assert oom center desired 0 assert offsetGot "" # Make sure the ticks contain now the prefixes for tick in ticksGot # 0 is zero on all orders of magnitudes no if tick[0] "0" prefixIdx 0 else prefixIdx oom noise desired assert tick endswith formatter ENG PREFIXES[prefixIdx] + UNIT
def get offset self # docstring inherited if len self locs 0 return if self offset offsetStr if self offset offsetStr self format data self offset if self offset > 0 offsetStr + + offsetStr sciNotStr self format data 10 ** self orderOfMagnitude if self useMathText or self usetex if sciNotStr ! sciNotStr r \times%s % sciNotStr s fr ${sciNotStr}{offsetStr}$ else s join sciNotStr offsetStr return self fix minus s return
def test engformatter offset oom oom center oom noise oom center desired oom noise desired UNIT "eV" # Doesn t really matter here but should be of order of magnitude ~ 1 fig ax plt subplots # Use some random ugly number data offset 2 7149*10**oom center ydata data offset + np arange -5 7 dtype float *10**oom noise ax plot ydata formatter mticker EngFormatter useOffset True unit UNIT # So that offset strings will always have the same size formatter ENG PREFIXES[0] " " ax yaxis set major formatter formatter fig canvas draw offset got formatter get offset ticks got [labl get text for labl in ax get yticklabels ] # Predicting whether offset should be 0 or not is essentially testing # ScalarFormatter compute offset This function is pretty complex and it # would be nice to test it but this is out of scope for this test which # only makes sure that offset text and the ticks gets the correct unit # prefixes and the ticks if formatter offset # These prefix variables are used only once so we could have inlined # them all but it is more comfortable in case of tests breakages to # view their values with pytest --showlocals prefix noise got offset got[2] prefix noise desired formatter ENG PREFIXES[oom noise desired] prefix center got offset got[-1-len UNIT ] prefix center desired formatter ENG PREFIXES[oom center desired] assert prefix noise desired prefix noise got assert prefix center desired prefix center got # Make sure the ticks didn t get the UNIT for tick in ticks got assert UNIT not in tick else assert oom center desired 0 assert offset got "" # Make sure the ticks contain now the prefixes for tick in ticks got # 0 is zero on all orders of magnitudes no matter what is # oom noise desired prefix idx 0 if tick[0] "0" else oom noise desired assert tick endswith formatter ENG PREFIXES[prefix idx] + UNIT
def get backend * resolve True """ Return the name of the current backend Parameters ---------- resolve bool default True Whether to trigger backend resolution if no backend has been selected so far If True this ensures that a valid backend is returned If False this returns None if no backend has been selected so far admonition Provisional resolve flag The *resolve* flag is introduced provisionally in the anticipation of backend resolution refactoring We don t guarantee API stability for now but if all plays out well the provisional availability will prolong the range of supported releases for the migration period See Also -------- matplotlib use """ if resolve return rcParams[ backend ] else backend rcParams get backend if backend is rcsetup auto backend sentinel return None else return backend
def get backend * resolve True """ Return the name of the current backend Parameters ---------- resolve bool default True Whether to trigger backend resolution if no backend has been selected so far If True this ensures that a valid backend is returned If False this returns None if no backend has been selected so far admonition Provisional resolve flag The *resolve* flag is introduced provisionally in the anticipation of backend resolution refactoring We don t guarantee API stability for now but if all plays out well the provisional availability will prolong the range of supported releases for the migration period See Also -------- matplotlib use """ if resolve return rcParams[ backend ] else backend rcParams get backend if backend is rcsetup auto backend sentinel return None else return backend
def get backend * resolve True """ Return the name of the current backend Parameters ---------- resolve bool default True Whether to trigger backend resolution if no backend has been selected so far If True this ensures that a valid backend is returned If False this returns None if no backend has been selected so far admonition Provisional resolve flag The *resolve* flag is introduced provisionally in the anticipation of backend resolution refactoring We don t guarantee API stability for now but if all plays out well the provisional availability will prolong the range of supported releases for the migration period See Also -------- matplotlib use """ if resolve return rcParams[ backend ] else backend rcParams get backend if backend is rcsetup auto backend sentinel return None else return backend
def tripcolor ax *args alpha 1 0 norm None cmap None vmin None vmax None shading flat facecolors None **kwargs """ Create a pseudocolor plot of an unstructured triangular grid Call signatures tripcolor triangulation c * tripcolor x y c * [triangles triangles] [mask mask] The triangular grid can be specified either by passing a ` Triangulation` object as the first parameter or by passing the points *x* *y* and optionally the *triangles* and a *mask* See ` Triangulation` for an explanation of these parameters It is possible to pass the triangles positionally i e ``tripcolor x y triangles c `` However this is discouraged For more clarity pass *triangles* via keyword argument If neither of *triangulation* or *triangles* are given the triangulation is calculated on the fly In this case it does not make sense to provide colors at the triangle faces via *c* or *facecolors* because there are multiple possible triangulations for a group of points and you don t know which triangles will be constructed Parameters ---------- triangulation ` Triangulation` An already created triangular grid x y triangles mask Parameters defining the triangular grid See ` Triangulation` This is mutually exclusive with specifying *triangulation* c array-like The color values either for the points or for the triangles Which one is automatically inferred from the length of *c* i e does it match the number of points or the number of triangles If there are the same number of points and triangles in the triangulation it is assumed that color values are defined at points; to force the use of color values at triangles use the keyword argument ``facecolors c`` instead of just ``c`` This parameter is position-only facecolors array-like optional Can be used alternatively to *c* to specify colors at the triangle faces This parameter takes precedence over *c* shading { flat gouraud } default flat If flat and the color values *c* are defined at points the color values used for each triangle are from the mean c of the triangle s three points If *shading* is gouraud then color values must be defined at points % cmap doc s % norm doc s % vmin vmax doc s % colorizer doc s Returns ------- `~matplotlib collections PolyCollection` or `~matplotlib collections TriMesh` The result depends on *shading* For ``shading flat `` the result is a ` PolyCollection` for ``shading gouraud `` the result is a ` TriMesh` Other Parameters ---------------- **kwargs `~matplotlib collections Collection` properties % Collection kwdoc s """ api check in list [ flat gouraud ] shading shading tri args kwargs Triangulation get from args and kwargs *args **kwargs # Parse the color to be in one of the other variable will be None # - facecolors if specified at the triangle faces # - point colors if specified at the points if facecolors is not None if args api warn external "Positional parameter c has no effect when the keyword " "facecolors is given" point colors None if len facecolors ! len tri triangles raise ValueError "The length of facecolors must match the number " "of triangles" else # Color from positional parameter c if not args raise TypeError "tripcolor missing 1 required positional argument c ; or " "1 required keyword-only argument facecolors " elif len args > 1 raise TypeError f"Unexpected positional parameters {args[1 ]!r}" c np asarray args[0] if len c len tri x # having this before the len tri triangles comparison gives # precedence to nodes if there are as many nodes as triangles point colors c facecolors None elif len c len tri triangles point colors None facecolors c else raise ValueError The length of c must match either the number of points or the number of triangles # Handling of linewidths shading edgecolors and antialiased as # in Axes pcolor linewidths 0 25 if linewidth in kwargs kwargs[ linewidths ] kwargs pop linewidth kwargs setdefault linewidths linewidths edgecolors none if edgecolor in kwargs kwargs[ edgecolors ] kwargs pop edgecolor ec kwargs setdefault edgecolors edgecolors if antialiased in kwargs kwargs[ antialiaseds ] kwargs pop antialiased if antialiaseds not in kwargs and ec lower "none" kwargs[ antialiaseds ] False if shading gouraud if facecolors is not None raise ValueError "shading gouraud can only be used when the colors " "are specified at the points not at the faces " collection TriMesh tri alpha alpha array point colors cmap cmap norm norm **kwargs else # flat # Vertices of triangles maskedTris tri get masked triangles verts np stack tri x[maskedTris] tri y[maskedTris] axis -1 # Color values if facecolors is None # One color per triangle the mean of the 3 vertex color values colors point colors[maskedTris] mean axis 1 elif tri mask is not None # Remove color values of masked triangles colors facecolors[~tri mask] else colors facecolors collection PolyCollection verts alpha alpha array colors cmap cmap norm norm **kwargs collection scale norm norm vmin vmax ax grid False minx tri x min maxx tri x max miny tri y min maxy tri y max corners minx miny maxx maxy ax update datalim corners ax autoscale view ax add collection collection return collection
def tripcolor ax *args alpha 1 0 norm None cmap None vmin None vmax None shading flat facecolors None **kwargs """ Create a pseudocolor plot of an unstructured triangular grid Call signatures tripcolor triangulation c * tripcolor x y c * [triangles triangles] [mask mask] The triangular grid can be specified either by passing a ` Triangulation` object as the first parameter or by passing the points *x* *y* and optionally the *triangles* and a *mask* See ` Triangulation` for an explanation of these parameters It is possible to pass the triangles positionally i e ``tripcolor x y triangles c `` However this is discouraged For more clarity pass *triangles* via keyword argument If neither of *triangulation* or *triangles* are given the triangulation is calculated on the fly In this case it does not make sense to provide colors at the triangle faces via *c* or *facecolors* because there are multiple possible triangulations for a group of points and you don t know which triangles will be constructed Parameters ---------- triangulation ` Triangulation` An already created triangular grid x y triangles mask Parameters defining the triangular grid See ` Triangulation` This is mutually exclusive with specifying *triangulation* c array-like The color values either for the points or for the triangles Which one is automatically inferred from the length of *c* i e does it match the number of points or the number of triangles If there are the same number of points and triangles in the triangulation it is assumed that color values are defined at points; to force the use of color values at triangles use the keyword argument ``facecolors c`` instead of just ``c`` This parameter is position-only facecolors array-like optional Can be used alternatively to *c* to specify colors at the triangle faces This parameter takes precedence over *c* shading { flat gouraud } default flat If flat and the color values *c* are defined at points the color values used for each triangle are from the mean c of the triangle s three points If *shading* is gouraud then color values must be defined at points % cmap doc s % norm doc s % vmin vmax doc s % colorizer doc s Returns ------- `~matplotlib collections PolyCollection` or `~matplotlib collections TriMesh` The result depends on *shading* For ``shading flat `` the result is a ` PolyCollection` for ``shading gouraud `` the result is a ` TriMesh` Other Parameters ---------------- **kwargs `~matplotlib collections Collection` properties % Collection kwdoc s """ api check in list [ flat gouraud ] shading shading tri args kwargs Triangulation get from args and kwargs *args **kwargs # Parse the color to be in one of the other variable will be None # - facecolors if specified at the triangle faces # - point colors if specified at the points if facecolors is not None if args api warn external "Positional parameter c has no effect when the keyword " "facecolors is given" point colors None if len facecolors ! len tri triangles raise ValueError "The length of facecolors must match the number " "of triangles" else # Color from positional parameter c if not args raise TypeError "tripcolor missing 1 required positional argument c ; or " "1 required keyword-only argument facecolors " elif len args > 1 raise TypeError f"Unexpected positional parameters {args[1 ]!r}" c np asarray args[0] if len c len tri x # having this before the len tri triangles comparison gives # precedence to nodes if there are as many nodes as triangles point colors c facecolors None elif len c len tri triangles point colors None facecolors c else raise ValueError The length of c must match either the number of points or the number of triangles # Handling of linewidths shading edgecolors and antialiased as # in Axes pcolor linewidths 0 25 if linewidth in kwargs kwargs[ linewidths ] kwargs pop linewidth kwargs setdefault linewidths linewidths edgecolors none if edgecolor in kwargs kwargs[ edgecolors ] kwargs pop edgecolor ec kwargs setdefault edgecolors edgecolors if antialiased in kwargs kwargs[ antialiaseds ] kwargs pop antialiased if antialiaseds not in kwargs and ec lower "none" kwargs[ antialiaseds ] False if shading gouraud if facecolors is not None raise ValueError "shading gouraud can only be used when the colors " "are specified at the points not at the faces " collection TriMesh tri alpha alpha array point colors cmap cmap norm norm **kwargs else # flat # Vertices of triangles maskedTris tri get masked triangles verts np stack tri x[maskedTris] tri y[maskedTris] axis -1 # Color values if facecolors is None # One color per triangle the mean of the 3 vertex color values colors point colors[maskedTris] mean axis 1 elif tri mask is not None # Remove color values of masked triangles colors facecolors[~tri mask] else colors facecolors collection PolyCollection verts alpha alpha array colors cmap cmap norm norm **kwargs collection scale norm norm vmin vmax ax grid False minx tri x min maxx tri x max miny tri y min maxy tri y max corners minx miny maxx maxy ax update datalim corners ax autoscale view ax add collection collection return collection
def tripcolor ax *args alpha 1 0 norm None cmap None vmin None vmax None shading flat facecolors None **kwargs """ Create a pseudocolor plot of an unstructured triangular grid Call signatures tripcolor triangulation c * tripcolor x y c * [triangles triangles] [mask mask] The triangular grid can be specified either by passing a ` Triangulation` object as the first parameter or by passing the points *x* *y* and optionally the *triangles* and a *mask* See ` Triangulation` for an explanation of these parameters It is possible to pass the triangles positionally i e ``tripcolor x y triangles c `` However this is discouraged For more clarity pass *triangles* via keyword argument If neither of *triangulation* or *triangles* are given the triangulation is calculated on the fly In this case it does not make sense to provide colors at the triangle faces via *c* or *facecolors* because there are multiple possible triangulations for a group of points and you don t know which triangles will be constructed Parameters ---------- triangulation ` Triangulation` An already created triangular grid x y triangles mask Parameters defining the triangular grid See ` Triangulation` This is mutually exclusive with specifying *triangulation* c array-like The color values either for the points or for the triangles Which one is automatically inferred from the length of *c* i e does it match the number of points or the number of triangles If there are the same number of points and triangles in the triangulation it is assumed that color values are defined at points; to force the use of color values at triangles use the keyword argument ``facecolors c`` instead of just ``c`` This parameter is position-only facecolors array-like optional Can be used alternatively to *c* to specify colors at the triangle faces This parameter takes precedence over *c* shading { flat gouraud } default flat If flat and the color values *c* are defined at points the color values used for each triangle are from the mean c of the triangle s three points If *shading* is gouraud then color values must be defined at points % cmap doc s % norm doc s % vmin vmax doc s % colorizer doc s Returns ------- `~matplotlib collections PolyCollection` or `~matplotlib collections TriMesh` The result depends on *shading* For ``shading flat `` the result is a ` PolyCollection` for ``shading gouraud `` the result is a ` TriMesh` Other Parameters ---------------- **kwargs `~matplotlib collections Collection` properties % Collection kwdoc s """ api check in list [ flat gouraud ] shading shading tri args kwargs Triangulation get from args and kwargs *args **kwargs # Parse the color to be in one of the other variable will be None # - facecolors if specified at the triangle faces # - point colors if specified at the points if facecolors is not None if args api warn external "Positional parameter c has no effect when the keyword " "facecolors is given" point colors None if len facecolors ! len tri triangles raise ValueError "The length of facecolors must match the number " "of triangles" else # Color from positional parameter c if not args raise TypeError "tripcolor missing 1 required positional argument c ; or " "1 required keyword-only argument facecolors " elif len args > 1 raise TypeError f"Unexpected positional parameters {args[1 ]!r}" c np asarray args[0] if len c len tri x # having this before the len tri triangles comparison gives # precedence to nodes if there are as many nodes as triangles point colors c facecolors None elif len c len tri triangles point colors None facecolors c else raise ValueError The length of c must match either the number of points or the number of triangles # Handling of linewidths shading edgecolors and antialiased as # in Axes pcolor linewidths 0 25 if linewidth in kwargs kwargs[ linewidths ] kwargs pop linewidth kwargs setdefault linewidths linewidths edgecolors none if edgecolor in kwargs kwargs[ edgecolors ] kwargs pop edgecolor ec kwargs setdefault edgecolors edgecolors if antialiased in kwargs kwargs[ antialiaseds ] kwargs pop antialiased if antialiaseds not in kwargs and ec lower "none" kwargs[ antialiaseds ] False if shading gouraud if facecolors is not None raise ValueError "shading gouraud can only be used when the colors " "are specified at the points not at the faces " collection TriMesh tri alpha alpha array point colors cmap cmap norm norm **kwargs else # flat # Vertices of triangles maskedTris tri get masked triangles verts np stack tri x[maskedTris] tri y[maskedTris] axis -1 # Color values if facecolors is None # One color per triangle the mean of the 3 vertex color values colors point colors[maskedTris] mean axis 1 elif tri mask is not None # Remove color values of masked triangles colors facecolors[~tri mask] else colors facecolors collection PolyCollection verts alpha alpha array colors cmap cmap norm norm **kwargs collection scale norm norm vmin vmax ax grid False minx tri x min maxx tri x max miny tri y min maxy tri y max corners minx miny maxx maxy ax update datalim corners ax autoscale view ax add collection collection return collection
def test table fontsize # Data for plotting tableData [[ a 1] [ b 1]] # Create the figure and axis objects fig ax plt subplots # Plot the data although we are focusing on the table ax plot np linspace 0 10 100 np linspace 0 10 100 + 1 # Add a table with fontsize 30 t ax table cellText tableData loc top cellLoc center fontsize 30 # Retrieve the font size from a specific cell e g cell 0 0 cell fontsize t[ 0 0 ] get fontsize # Assert that the fontsize is correctly set to 30 assert cell fontsize 30 f"Expected fontsize 30 but got {cell fontsize}"
def init self * edgecolors None facecolors None linewidths None linestyles solid capstyle None joinstyle None antialiaseds None offsets None offset transform None norm None # optional for ScalarMappable cmap None # ditto colorizer None pickradius 5 0 hatch None urls None zorder 1 **kwargs """ Parameters ---------- edgecolors mpltype `color` or list of colors default rc `patch edgecolor` Edge color for each patch making up the collection The special value face can be passed to make the edgecolor match the facecolor facecolors mpltype `color` or list of colors default rc `patch facecolor` Face color for each patch making up the collection linewidths float or list of floats default rc `patch linewidth` Line width for each patch making up the collection linestyles str or tuple or list thereof default solid Valid strings are [ solid dashed dashdot dotted - -- - ] Dash tuples should be of the form offset onoffseq where *onoffseq* is an even length tuple of on and off ink lengths in points For examples see doc ` gallery lines bars and markers linestyles` capstyle ` CapStyle`-like default butt Style to use for capping lines for all paths in the collection Allowed values are % CapStyle s joinstyle ` JoinStyle`-like default round Style to use for joining lines for all paths in the collection Allowed values are % JoinStyle s antialiaseds bool or list of bool default rc `patch antialiased` Whether each patch in the collection should be drawn with antialiasing offsets float float or list thereof default 0 0 A vector by which to translate each patch after rendering default is no translation The translation is performed in screen pixel coordinates i e after the Artist s transform is applied offset transform `~ Transform` default ` IdentityTransform` A single transform which will be applied to each *offsets* vector before it is used cmap norm Data normalization and colormapping parameters See ` ScalarMappable` for a detailed description hatch str optional Hatching pattern to use in filled paths if any Valid strings are [ \\ | - + x o O * ] See doc ` gallery shapes and collections hatch style reference` for the meaning of each hatch type pickradius float default 5 0 If ``pickradius < 0`` then ` Collection contains` will return ``True`` whenever the test point is inside of one of the polygons formed by the control points of a Path in the Collection On the other hand if it is greater than 0 then we instead check if the test point is contained in a stroke of width ``2*pickradius`` following any of the Paths in the Collection urls list of str default None A URL for each patch to link to once drawn Currently only works for the SVG backend See doc ` gallery misc hyperlinks sgskip` for examples zorder float default 1 The drawing order shared by all Patches in the Collection See doc ` gallery misc zorder demo` for all defaults and examples **kwargs Remaining keyword arguments will be used to set properties as ``Collection set {key} val `` for each key-value pair in *kwargs* """ super init self get colorizer cmap norm colorizer # list of un-scaled dash patterns # this is needed scaling the dash pattern by linewidth self us linestyles [ 0 None ] # list of dash patterns self linestyles [ 0 None ] # list of unbroadcast scaled linewidths self us lw [0] self linewidths [0] self gapcolor None # Currently only used by LineCollection # Flags set by set mappable flags are colors from mapping an array? self face is mapped None self edge is mapped None self mapped colors None # calculated in update scalarmappable # Temporary logic to set hatchcolor This eager resolution is temporary # and will be replaced by a proper mechanism in a follow-up PR hatch color mpl rcParams[ hatch color ] if hatch color edge hatch color mpl rcParams[ patch edgecolor ] self hatch color mcolors to rgba hatch color self hatch linewidth mpl rcParams[ hatch linewidth ] self set facecolor facecolors self set edgecolor edgecolors self set linewidth linewidths self set linestyle linestyles self set antialiased antialiaseds self set pickradius pickradius self set urls urls self set hatch hatch self set zorder zorder if capstyle self set capstyle capstyle else self capstyle None if joinstyle self set joinstyle joinstyle else self joinstyle None if offsets is not None offsets np asanyarray offsets float # Broadcast 2 -> 1 2 but nothing else if offsets shape 2 offsets offsets[None ] self offsets offsets self offset transform offset transform self path effects None self internal update kwargs self paths None
def get hatchcolor self """Return the hatch color """ if self hatch color edge if self edgecolor[3] 0 # fully transparent return colors to rgba mpl rcParams[ patch edgecolor ] return self get edgecolor return self hatch color
def embedding lookup params ids partition strategy "mod" name None validate indices True # pylint disable unused-argument max norm None """Looks up embeddings for the given `ids` from a list of tensors This function is used to perform parallel lookups on the list of tensors in `params` It is a generalization of `tf gather` where `params` is interpreted as a partitioning of a large embedding tensor `params` may be a `PartitionedVariable` as returned by using `tf compat v1 get variable ` with a partitioner If `len params > 1` each element `id` of `ids` is partitioned between the elements of `params` according to the `partition strategy` In all strategies if the id space does not evenly divide the number of partitions each of the first ` max id + 1 % len params ` partitions will be assigned one more id If `partition strategy` is `"mod"` we assign each id to partition `p id % len params ` For instance 13 ids are split across 5 partitions as `[[0 5 10] [1 6 11] [2 7 12] [3 8] [4 9]]` If `partition strategy` is `"div"` we assign ids to partitions in a contiguous manner In this case 13 ids are split across 5 partitions as `[[0 1 2] [3 4 5] [6 7 8] [9 10] [11 12]]` If the input ids are ragged tensors partition variables are not supported and the partition strategy and the max norm are ignored The results of the lookup are concatenated into a dense tensor The returned tensor has shape `shape ids + shape params [1 ]` Args params A single tensor representing the complete embedding tensor or a list of P tensors all of same shape except for the first dimension representing sharded embedding tensors Alternatively a `PartitionedVariable` created by partitioning along dimension 0 Each element must be appropriately sized for the given `partition strategy` ids A `Tensor` or a RaggedTensor with type `int32` or `int64` containing the ids to be looked up in `params` Caution On CPU if an out of bound id is found an error is raised On GPU if an out of bound id is found a 0 is stored in the corresponding output value partition strategy A string specifying the partitioning strategy relevant if `len params > 1` Currently `"div"` and `"mod"` are supported Default is `"mod"` name A name for the operation optional validate indices DEPRECATED If this operation is assigned to CPU values in `indices` are always validated to be within range If assigned to GPU out-of-bound indices result in safe but unspecified behavior which may include raising an error max norm If not `None` each embedding is clipped if its l2-norm is larger than this value Returns A `Tensor` or a RaggedTensor depending on the input with the same type as the tensors in `params` Raises ValueError If `params` is empty """ """ **Behavior Difference between CPU and GPU** Please note that when using `tf nn embedding lookup` on a GPU if an out-of-bound index is encountered a value of 0 will be stored in the corresponding output value On the other hand when using `tf nn embedding lookup` on a CPU an error will be returned if an out-of-bound index is found This behavior difference can impact the results of your computation especially when dealing with indices that may go beyond the bounds of the tensor Make sure to be mindful of this distinction when using the `tf nn embedding lookup` function in your computations **Usage Example** Here s an example demonstrating how to use `tf nn embedding lookup` ```python import tensorflow as tf # Example embedding matrix and indices embedding matrix tf constant [[0 1 0 2] [0 3 0 4] [0 5 0 6]] indices tf constant [1 0 2] # Perform embedding lookup embeddings tf nn embedding lookup embedding matrix indices # Print the result print "Embeddings " print embeddings numpy ``` """ return embedding lookup and transform params params ids ids partition strategy partition strategy name name max norm max norm transform fn None
def embedding lookup params ids partition strategy "mod" name None validate indices True # pylint disable unused-argument max norm None """Looks up embeddings for the given `ids` from a list of tensors This function is used to perform parallel lookups on the list of tensors in `params` It is a generalization of `tf gather` where `params` is interpreted as a partitioning of a large embedding tensor `params` may be a `PartitionedVariable` as returned by using `tf compat v1 get variable ` with a partitioner If `len params > 1` each element `id` of `ids` is partitioned between the elements of `params` according to the `partition strategy` In all strategies if the id space does not evenly divide the number of partitions each of the first ` max id + 1 % len params ` partitions will be assigned one more id If `partition strategy` is `"mod"` we assign each id to partition `p id % len params ` For instance 13 ids are split across 5 partitions as `[[0 5 10] [1 6 11] [2 7 12] [3 8] [4 9]]` If `partition strategy` is `"div"` we assign ids to partitions in a contiguous manner In this case 13 ids are split across 5 partitions as `[[0 1 2] [3 4 5] [6 7 8] [9 10] [11 12]]` If the input ids are ragged tensors partition variables are not supported and the partition strategy and the max norm are ignored The results of the lookup are concatenated into a dense tensor The returned tensor has shape `shape ids + shape params [1 ]` Args params A single tensor representing the complete embedding tensor or a list of P tensors all of same shape except for the first dimension representing sharded embedding tensors Alternatively a `PartitionedVariable` created by partitioning along dimension 0 Each element must be appropriately sized for the given `partition strategy` ids A `Tensor` or a RaggedTensor with type `int32` or `int64` containing the ids to be looked up in `params` Caution Without XLA on CPU if an out of bound id is found an error is raised On GPU if an out of bound id is found a 0 is stored in the corresponding output value With XLA for both CPU and GPU all out of bound ids are replaced with maximum of `ids` inferred from the input `params` and corresponding value will be taken into output partition strategy A string specifying the partitioning strategy relevant if `len params > 1` Currently `"div"` and `"mod"` are supported Default is `"mod"` name A name for the operation optional validate indices DEPRECATED If this operation is assigned to CPU values in `indices` are always validated to be within range If assigned to GPU out-of-bound indices result in safe but unspecified behavior which may include raising an error max norm If not `None` each embedding is clipped if its l2-norm is larger than this value Returns A `Tensor` or a RaggedTensor depending on the input with the same type as the tensors in `params` Raises ValueError If `params` is empty """ """ **Behavior Difference between CPU and GPU** Please note that when using `tf nn embedding lookup` on a GPU if an out-of-bound index is encountered a value of 0 will be stored in the corresponding output value On the other hand when using `tf nn embedding lookup` on a CPU an error will be returned if an out-of-bound index is found This behavior difference can impact the results of your computation especially when dealing with indices that may go beyond the bounds of the tensor Make sure to be mindful of this distinction when using the `tf nn embedding lookup` function in your computations **Usage Example** Here s an example demonstrating how to use `tf nn embedding lookup` ```python import tensorflow as tf # Example embedding matrix and indices embedding matrix tf constant [[0 1 0 2] [0 3 0 4] [0 5 0 6]] indices tf constant [1 0 2] # Perform embedding lookup embeddings tf nn embedding lookup embedding matrix indices # Print the result print "Embeddings " print embeddings numpy ``` """ return embedding lookup and transform params params ids ids partition strategy partition strategy name name max norm max norm transform fn None
void Compute OpKernelContext* ctx override { const unsigned int src idx 0; const Tensor& input ctx->input src idx ; const float input min range ctx->input 1 scalar<float> ; const float input max range ctx->input 2 scalar<float> ; float min range std min 0 0f input min range ; float max range; OP REQUIRES ctx input max range > input min range absl InvalidArgumentError "input max range must be larger than input min range " ; When the minimum and maximum ranges are too close together nudge them apart by a small value so that they are slightly different This helps us avoid creating ill-formed buffers where all quantized values map to the same float number These kinds of buffers cause problems for downstream ops when they need to do calculations on them We pick the value by making sure that zero is not more than 100x the overall range from the maximum so that the value can be easily represented when we promote the quantized value to a higher intermediate bit depth since that s a common requirement const float epsilon std max 1 0f std max fabsf input min range fabsf input max range * ensure minimum range ; max range std max input max range min range + epsilon ; Clamping the max range to zero since max range can also be negative max range std max 0 0f max range ; auto cpu engine engine engine kind cpu 0 ; const Tensor& src tensor MklGetInput ctx src idx ; MklDnnShape src mkl shape; GetMklShape ctx src idx &src mkl shape native format ; auto src tf shape src mkl shape IsMklTensor ? src mkl shape GetTfShape src tensor shape ; auto src dims src mkl shape IsMklTensor ? src mkl shape GetSizesAsMklDnnDims TFShapeToMklDnnDims src tensor shape ; auto output dims src dims; Set the dst layout to be the best mkl layout based on dims and type memory format tag dst layout type; switch src tf shape dims { case 0 ComputeScalar ctx min range max range ; return; case 1 dst layout type memory format tag x; break; case 2 dst layout type memory format tag nc; break; case 3 dst layout type memory format tag tnc; break; case 4 dst layout type memory format tag nhwc; break; case 5 dst layout type memory format tag ndhwc; break; default OP REQUIRES OK ctx absl AbortedError "Input dims must be < 5 and > 1" ; return; } Create reorder memory for src dst both are defined in mkl util h they are wrapper MklDnnData<S> src &cpu engine ; MklDnnData<T> dst &cpu engine ; #ifdef ENABLE ONEDNN V3 MklDnnData<float> scale &cpu engine ; #endif ENABLE ONEDNN V3 auto src md src mkl shape IsMklTensor ? src mkl shape GetMklLayout memory desc src dims MklDnnType<S> dst layout type ; src SetUsrMem src md &src tensor ; memory desc dst md memory desc src dims MklDnnType<T> dst layout type ; Standard shape assignments for layout pass MklDnnShape output mkl shape; TensorShape output tf shape; if src mkl shape IsMklTensor { output mkl shape SetMklTensor true ; output mkl shape SET MKL LAYOUT dst md ; output mkl shape SetElemType MklDnnType<T> ; output mkl shape SetTfLayout src mkl shape GetDimension src mkl shape GetSizesAsMklDnnDims src mkl shape GetTfDataFormat ; output tf shape AddDim dst md get size sizeof T ; } else { output mkl shape SetMklTensor false ; output tf shape MklDnnDimsToTFShape output dims ; } Tensor* output tensor nullptr; AllocateOutputSetMklShape ctx 0 &output tensor output tf shape output mkl shape native format ; dst SetUsrMem dst md output tensor ; TensorShape min tf shape {}; MklDnnShape min mkl shape; min mkl shape SetMklTensor false ; Tensor* output min tensor nullptr; AllocateOutputSetMklShape ctx 1 &output min tensor min tf shape min mkl shape native format ; TensorShape max tf shape {}; MklDnnShape max mkl shape; max mkl shape SetMklTensor false ; Tensor* output max tensor nullptr; AllocateOutputSetMklShape ctx 2 &output max tensor max tf shape max mkl shape native format ; Create the oneDNN wrapper over Eigen threadpool and set max threads in oneDNN Eigen ThreadPoolInterface* eigen interface EigenThreadPoolFromTfContext ctx ; tsl OneDnnThreadPool eigen tp eigen interface ThreadPoolUseCallerThread ; float scale factor 0; if mode QUANTIZE MODE SCALED { Estimating scales for quantization const int num bits sizeof T * 8; const float max abs std max std abs min range std abs max range ; const bool is signed std is same<T qint8> ; float target range; if is signed { max range max abs; min range -max abs; If it is signed we try to keep 0 0 being 0 and drop one bucket For example if it is 8 bits we have the range [-127 127] So for input range of [-x x] the scale should be 254 2*x target range static cast<float> uint64 t{1} << num bits - 1 2 ; } else { max range max abs; min range 0 0; If it is unsigned and num bits 8 the range with 8 bits is [0 255] If the input range is [0 x] then the scale is 255 x instead of 254 as in the case above target range static cast<float> uint64 t{1} << num bits - 1 ; } scale factor target range max abs; #ifdef ENABLE ONEDNN V3 auto scale md memory desc {1} MklDnnType<float> memory format tag x ; MklReorderWithScaleFwdParams fwdParams src dims src md dst md scale md ; Tensor scale tensor; OP REQUIRES OK ctx ctx->allocate temp DT FLOAT {1} &scale tensor ; scale tensor flat<float> 0 scale factor; scale SetUsrMem scale md &scale tensor ; #else MklReorderWithScaleFwdParams fwdParams src dims src md dst md ; #endif ENABLE ONEDNN V3 fwdParams dtypes append typeid S name ; fwdParams dtypes append typeid T name ; fwdParams post op params name "scale"; fwdParams post op params param push back scale factor ; MklReorderWithScalePrimitive* reorder prim MklReorderWithScalePrimitiveFactory<T> Get src GetUsrMem dst GetUsrMem fwdParams ; std shared ptr<stream> cpu stream; cpu stream reset CreateStream &eigen tp reorder prim->GetEngine ; reorder prim->Execute src GetUsrMemDataHandle dst GetUsrMemDataHandle #ifdef ENABLE ONEDNN V3 scale GetUsrMemDataHandle #endif ENABLE ONEDNN V3 cpu stream ; } else if mode QUANTIZE MODE MIN FIRST { using namespace dnnl; std shared ptr<stream> cpu stream; cpu stream reset CreateStream &eigen tp cpu engine ; auto shift static cast<S> -min range ; memory dims shift dims src tf shape dims 1 ; auto shift md memory desc shift dims MklDnnType<S> dst layout type ; memory shift mem shift md cpu engine void* &shift ; primitive attr attr; std vector<float> src 0 scale{255 0f max range - min range }; std vector<float> src 1 scale{255 0f max range - min range }; #ifdef ENABLE ONEDNN V3 attr set scales mask DNNL ARG SRC 0 0 ; attr set scales mask DNNL ARG SRC 1 0 ; auto binary pd binary primitive desc cpu engine algorithm binary add src md shift md dst md attr ; #else attr set scales DNNL ARG SRC 0 0 src 0 scale ; attr set scales DNNL ARG SRC 1 0 src 1 scale ; auto binary d binary desc algorithm binary add src md shift md dst md ; auto binary pd binary primitive desc binary d attr cpu engine ; #endif ENABLE ONEDNN V3 auto binary prim binary binary pd ; auto src 0 scale mem memory {{1} MklDnnType<float> memory format tag x} cpu engine src 0 scale data ; auto src 1 scale mem memory {{1} MklDnnType<float> memory format tag x} cpu engine src 1 scale data ; std unordered map<int memory> net args{ {DNNL ARG SRC 0 *src GetUsrMem } {DNNL ARG SRC 1 shift mem} {DNNL ARG DST *dst GetUsrMem } #ifdef ENABLE ONEDNN V3 {DNNL ARG ATTR SCALES | DNNL ARG SRC 0 src 0 scale mem} { DNNL ARG ATTR SCALES | DNNL ARG SRC 1 src 1 scale mem } #endif }; binary prim execute *cpu stream net args ; } else { OP REQUIRES ctx false absl UnimplementedError "Supported modes are MIN FIRST and SCALED only " ; } output min tensor->scalar<float> min range; output max tensor->scalar<float> max range; }
void DoCompute OpKernelContext* ctx const Tensor& logits t const Tensor& num samples t GuardedPhiloxRandom* generator { OP REQUIRES ctx TensorShapeUtils IsMatrix logits t shape absl InvalidArgumentError absl StrCat "logits should be a matrix got shape " logits t shape DebugString ; OP REQUIRES ctx TensorShapeUtils IsScalar num samples t shape absl InvalidArgumentError absl StrCat "num samples should be a scalar got shape " num samples t shape DebugString ; const int num samples num samples t scalar<int> ; OP REQUIRES ctx num samples > 0 absl InvalidArgumentError absl StrCat "num samples should be non-negative got " num samples ; const int batch size static cast<int> logits t dim size 0 ; const int num classes static cast<int> logits t dim size 1 ; OP REQUIRES ctx batch size logits t dim size 0 absl InvalidArgumentError "batch size cannot exceed max int" ; OP REQUIRES ctx num classes logits t dim size 1 absl InvalidArgumentError "num classes cannot exceed max int" ; OP REQUIRES ctx num classes > 0 absl InvalidArgumentError absl StrCat "num classes should be positive got " num classes ; Tensor* samples t; OP REQUIRES OK ctx ctx->allocate output 0 TensorShape {batch size num samples} &samples t ; Execute kernel only for nonempty output; otherwise Eigen crashes on GPU if samples t->NumElements > 0 { Tensor noises scores scratch; Scratch space only used for GPU if std is same<Device GPUDevice> value { OP REQUIRES OK ctx ctx->allocate temp DT FLOAT TensorShape {batch size num samples num classes} &noises ; OP REQUIRES OK ctx ctx->allocate temp DT FLOAT TensorShape {batch size num samples num classes} &scores ; OP REQUIRES OK ctx ctx->allocate temp DT FLOAT TensorShape {batch size num samples} &scratch ; } int num samples ceil 4 num samples + 3 4 * 4; CPU generates doubles 2 samples per number if std is same<Device CPUDevice> value num samples ceil 4 * 2; auto rng generator->ReserveRandomOutputs batch size * num samples ceil 4 256 ; functor MultinomialFunctor<Device T OutputType> ctx ctx->eigen device<Device> logits t matrix<T> noises flat<float> scores flat<float> scratch flat<float> batch size num classes num samples rng samples t->matrix<OutputType> ; } }
def truediv x y name None """Divides x y elementwise using Python 3 division operator semantics NOTE Prefer using the Tensor operator or tf divide which obey Python division operator semantics This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first This op is generated by normal `x y` division with `from future import division` If you want integer division that rounds down use `x y` or `tf math floordiv` `x` and `y` must have the same numeric type If the inputs are floating point the output will have the same type If the inputs are integral the inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32` and `int64` matching the behavior of Numpy Example >>> # Division with integer tensors returns float >>> x1 tf constant [10 20 30] dtype tf int32 >>> y1 tf constant [2 4 5] dtype tf int32 >>> result1 tf math truediv x1 y1 <tf Tensor shape 3 dtype float64 numpy array [5 5 6 ] > >>> # Division with different shaped tensors broadcasting >>> x2 tf constant [[10 20] [30 40]] dtype tf float64 >>> y2 tf constant [2 5] dtype tf float64 >>> result2 tf math truediv x2 y2 <tf Tensor shape 2 2 dtype float64 numpy array [[ 5 4 ] [15 8 ]] > # Handling potential division by zero returns inf >>> x3 tf constant 5 dtype tf float32 >>> y3 tf constant 0 dtype tf float32 >>> result3 tf math truediv x3 y3 <tf Tensor shape dtype float32 numpy inf> Args x `Tensor` numerator of numeric type y `Tensor` denominator of numeric type name A name for the operation optional Returns `x y` evaluated in floating point Raises TypeError If `x` and `y` have different dtypes """ return truediv python3 x y name
